{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdrUMMcxgt8E"
      },
      "outputs": [],
      "source": [
        "# ACUPUNCTURE CATE Analysis -- fixed epsilon\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import warnings\n",
        "import sys\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class TeeOutput:\n",
        "    \"\"\"Class to write output to both console and file simultaneously.\"\"\"\n",
        "    def __init__(self, filename):\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(filename, 'w')\n",
        "\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "        self.log.flush()  # Ensure immediate write to file\n",
        "\n",
        "    def flush(self):\n",
        "        self.terminal.flush()\n",
        "        self.log.flush()\n",
        "\n",
        "    def close(self):\n",
        "        self.log.close()\n",
        "\n",
        "class AcupunctureCATEAllocator:\n",
        "    \"\"\"Acupuncture CATE allocation algorithm with fixed gamma=0.5 and updated heavy interval threshold.\"\"\"\n",
        "\n",
        "    def __init__(self, epsilon=0.1, gamma=0.5, delta=0.05, heavy_multiplier=1.6, random_seed=42):\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "        self.rho = gamma * np.sqrt(epsilon)\n",
        "        self.delta = delta\n",
        "        self.heavy_multiplier = heavy_multiplier\n",
        "        self.random_seed = random_seed\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "        print(f\"Acupuncture CATE Allocation Algorithm\")\n",
        "        print(f\"ε = {epsilon}\")\n",
        "        print(f\"√ε = {np.sqrt(epsilon):.6f}\")\n",
        "        print(f\"γ = {gamma}\")\n",
        "        print(f\"ρ = γ√ε = {self.rho:.6f}\")\n",
        "        print(f\"Heavy multiplier = {heavy_multiplier}x\")\n",
        "        print(f\"δ = {delta}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    def process_acupuncture_data(self, df, outcome_col='pk5', treatment_col='group'):\n",
        "        \"\"\"Process acupuncture dataset for analysis.\"\"\"\n",
        "        print(f\"Processing acupuncture data with {len(df)} patients\")\n",
        "        print(f\"Available columns: {list(df.columns)}\")\n",
        "\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        if treatment_col not in df_processed.columns:\n",
        "            raise ValueError(f\"Missing required treatment column: {treatment_col}\")\n",
        "        if outcome_col not in df_processed.columns:\n",
        "            raise ValueError(f\"Missing required outcome column: {outcome_col}\")\n",
        "\n",
        "        df_processed['treatment'] = df_processed[treatment_col]\n",
        "        df_processed['outcome'] = df_processed[outcome_col]\n",
        "\n",
        "        if 'pk1' in df_processed.columns:\n",
        "            df_processed['baseline_headache'] = df_processed['pk1']\n",
        "        else:\n",
        "            df_processed['baseline_headache'] = 0\n",
        "\n",
        "        initial_size = len(df_processed)\n",
        "        df_processed = df_processed.dropna(subset=['outcome', 'treatment'])\n",
        "        final_size = len(df_processed)\n",
        "\n",
        "        if initial_size != final_size:\n",
        "            print(f\"Dropped {initial_size - final_size} rows due to missing outcome/treatment\")\n",
        "\n",
        "        print(f\"Final dataset: {final_size} patients\")\n",
        "        print(f\"Treatment distribution: {df_processed['treatment'].value_counts().to_dict()}\")\n",
        "        print(f\"Outcome (12-month headache score) statistics: mean={df_processed['outcome'].mean():.2f}, std={df_processed['outcome'].std():.2f}\")\n",
        "\n",
        "        if 'baseline_headache' in df_processed.columns:\n",
        "            print(f\"Baseline headache stats: mean={df_processed['baseline_headache'].mean():.2f}, std={df_processed['baseline_headache'].std():.2f}\")\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def create_age_chronicity_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on age-chronicity interaction.\"\"\"\n",
        "        print(f\"Creating age-chronicity interaction groups (target: {n_groups})\")\n",
        "\n",
        "        if 'age' not in df.columns or 'chronicity' not in df.columns:\n",
        "            print(\"No age or chronicity variables found\")\n",
        "            return []\n",
        "\n",
        "        age = df['age'].fillna(df['age'].median())\n",
        "        chronicity = df['chronicity'].fillna(df['chronicity'].median())\n",
        "\n",
        "        # Create interaction score (normalized age * chronicity)\n",
        "        age_norm = (age - age.min()) / (age.max() - age.min()) if age.max() > age.min() else age * 0\n",
        "        chron_norm = (chronicity - chronicity.min()) / (chronicity.max() - chronicity.min()) if chronicity.max() > chronicity.min() else chronicity * 0\n",
        "        interaction_score = age_norm * chron_norm\n",
        "\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(interaction_score, percentiles)\n",
        "        bins = np.digitize(interaction_score, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'age_chronicity_group_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'age_chronicity'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} age-chronicity interaction groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_age_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on age brackets.\"\"\"\n",
        "        print(f\"Creating age groups (target: {n_groups})\")\n",
        "\n",
        "        if 'age' not in df.columns:\n",
        "            print(\"No age variable found\")\n",
        "            return []\n",
        "\n",
        "        age = df['age'].fillna(df['age'].median())\n",
        "\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(age, percentiles)\n",
        "        bins = np.digitize(age, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'age_group_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'age'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} age groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_chronicity_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on headache chronicity.\"\"\"\n",
        "        print(f\"Creating chronicity groups (target: {n_groups})\")\n",
        "\n",
        "        if 'chronicity' not in df.columns:\n",
        "            print(\"No chronicity variable found\")\n",
        "            return []\n",
        "\n",
        "        chronicity = df['chronicity'].fillna(df['chronicity'].median())\n",
        "\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(chronicity, percentiles)\n",
        "        bins = np.digitize(chronicity, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'chronicity_group_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'chronicity'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} chronicity groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_multidimensional_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on composite score of all continuous variables.\"\"\"\n",
        "        print(f\"Creating multidimensional composite groups (target: {n_groups})\")\n",
        "\n",
        "        continuous_vars = ['age', 'chronicity', 'pk1']\n",
        "        available_vars = [col for col in continuous_vars if col in df.columns]\n",
        "\n",
        "        if len(available_vars) < 2:\n",
        "            print(\"Not enough continuous variables for multidimensional grouping\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Using continuous variables: {available_vars}\")\n",
        "\n",
        "        # Create normalized composite score\n",
        "        composite_score = pd.Series(0.0, index=df.index)\n",
        "\n",
        "        for var in available_vars:\n",
        "            values = df[var].fillna(df[var].median())\n",
        "            if values.max() > values.min():\n",
        "                normalized = (values - values.min()) / (values.max() - values.min())\n",
        "            else:\n",
        "                normalized = values * 0\n",
        "            composite_score += normalized\n",
        "\n",
        "        # Divide by number of variables to get mean\n",
        "        composite_score = composite_score / len(available_vars)\n",
        "\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(composite_score, percentiles)\n",
        "        bins = np.digitize(composite_score, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'multidim_group_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'multidimensional'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} multidimensional groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_baseline_headache_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on baseline headache scores.\"\"\"\n",
        "        print(f\"Creating baseline headache groups (target: {n_groups})\")\n",
        "\n",
        "        if 'pk1' not in df.columns:\n",
        "            print(\"No baseline headache data available\")\n",
        "            return []\n",
        "\n",
        "        baseline = df['pk1'].fillna(df['pk1'].median())\n",
        "\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(baseline, percentiles)\n",
        "        bins = np.digitize(baseline, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'baseline_headache_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'baseline_headache'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} baseline headache groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_baseline_age_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on baseline headache-age interaction.\"\"\"\n",
        "        print(f\"Creating baseline-age interaction groups (target: {n_groups})\")\n",
        "\n",
        "        if 'pk1' not in df.columns or 'age' not in df.columns:\n",
        "            print(\"No baseline headache or age variables found\")\n",
        "            return []\n",
        "\n",
        "        baseline = df['pk1'].fillna(df['pk1'].median())\n",
        "        age = df['age'].fillna(df['age'].median())\n",
        "\n",
        "        # Create interaction score (normalized baseline * age)\n",
        "        baseline_norm = (baseline - baseline.min()) / (baseline.max() - baseline.min()) if baseline.max() > baseline.min() else baseline * 0\n",
        "        age_norm = (age - age.min()) / (age.max() - age.min()) if age.max() > age.min() else age * 0\n",
        "        interaction_score = baseline_norm * age_norm\n",
        "\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(interaction_score, percentiles)\n",
        "        bins = np.digitize(interaction_score, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'baseline_age_group_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'baseline_age'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} baseline-age interaction groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_covariate_forest_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups using clustering on baseline covariates only.\"\"\"\n",
        "        print(f\"Creating covariate-based forest groups (target: {n_groups})\")\n",
        "\n",
        "        feature_cols = ['age', 'sex', 'migraine', 'chronicity', 'pk1']  # baseline covariates\n",
        "        available_features = [col for col in feature_cols if col in df.columns]\n",
        "\n",
        "        if not available_features:\n",
        "            print(\"No features available for covariate clustering\")\n",
        "            return []\n",
        "\n",
        "        X = df[available_features].copy()\n",
        "\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object':\n",
        "                le = LabelEncoder()\n",
        "                X[col] = X[col].fillna('missing')\n",
        "                X[col] = le.fit_transform(X[col])\n",
        "            else:\n",
        "                if X[col].isna().any():\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "        cluster_features = StandardScaler().fit_transform(X.values)\n",
        "        labels = KMeans(n_clusters=n_groups, random_state=self.random_seed).fit_predict(cluster_features)\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[labels == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'covariate_cluster_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'covariate_cluster'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} covariate-based groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_propensity_groups(self, df, n_groups=50, min_size=6):\n",
        "        \"\"\"Create groups based on propensity score strata.\"\"\"\n",
        "        print(f\"Creating propensity score groups (target: {n_groups})\")\n",
        "\n",
        "        feature_cols = ['age', 'sex', 'migraine', 'chronicity', 'pk1']  # baseline covariates\n",
        "        available_features = [col for col in feature_cols if col in df.columns]\n",
        "\n",
        "        if not available_features:\n",
        "            print(\"No features available for propensity scoring\")\n",
        "            return []\n",
        "\n",
        "        X = df[available_features].copy()\n",
        "\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object':\n",
        "                le = LabelEncoder()\n",
        "                X[col] = X[col].fillna('missing')\n",
        "                X[col] = le.fit_transform(X[col])\n",
        "            else:\n",
        "                if X[col].isna().any():\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "        try:\n",
        "            prop_scores = cross_val_predict(\n",
        "                LogisticRegression(random_state=self.random_seed, max_iter=1000),\n",
        "                X, df['treatment'], method='predict_proba', cv=5\n",
        "            )[:, 1]\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing propensity scores: {e}\")\n",
        "            return []\n",
        "\n",
        "        quantiles = np.linspace(0, 1, n_groups + 1)\n",
        "        bins = np.digitize(prop_scores, np.quantile(prop_scores, quantiles)) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'propensity_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'propensity'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} propensity groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def _ensure_balance_and_compute_cate(self, df, groups):\n",
        "        \"\"\"Ensure treatment balance and compute group CATE.\"\"\"\n",
        "        balanced_groups = []\n",
        "\n",
        "        for group in groups:\n",
        "            group_df = df.loc[group['indices']]\n",
        "\n",
        "            treatment_rate = group_df['treatment'].mean()\n",
        "            n_treated = group_df['treatment'].sum()\n",
        "            n_control = len(group_df) - n_treated\n",
        "\n",
        "            if not (0.15 <= treatment_rate <= 0.85 and n_treated >= 3 and n_control >= 3):\n",
        "                continue\n",
        "\n",
        "            treated_outcomes = group_df[group_df['treatment'] == 1]['outcome']\n",
        "            control_outcomes = group_df[group_df['treatment'] == 0]['outcome']\n",
        "            # Reverse sign: lower headache scores = better (treatment benefit)\n",
        "            cate = -(treated_outcomes.mean() - control_outcomes.mean())\n",
        "\n",
        "            balanced_groups.append({\n",
        "                'id': group['id'],\n",
        "                'indices': group['indices'],\n",
        "                'size': len(group_df),\n",
        "                'treatment_rate': treatment_rate,\n",
        "                'n_treated': int(n_treated),\n",
        "                'n_control': int(n_control),\n",
        "                'cate': cate,\n",
        "                'type': group['type']\n",
        "            })\n",
        "\n",
        "        return balanced_groups\n",
        "\n",
        "    def normalize_cates(self, groups):\n",
        "        \"\"\"Normalize CATE values to [0,1].\"\"\"\n",
        "        cates = [g['cate'] for g in groups]\n",
        "        min_cate, max_cate = min(cates), max(cates)\n",
        "\n",
        "        if max_cate > min_cate:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = (group['cate'] - min_cate) / (max_cate - min_cate)\n",
        "        else:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = 0.5\n",
        "\n",
        "        print(f\"CATE normalization: [{min_cate:.3f}, {max_cate:.3f}] → [0, 1]\")\n",
        "        return groups\n",
        "\n",
        "    def plot_cate_distribution(self, groups, title_suffix=\"\"):\n",
        "        \"\"\"Plot CATE distribution.\"\"\"\n",
        "        original_cates = [g['cate'] for g in groups]\n",
        "        normalized_cates = [g['normalized_cate'] for g in groups]\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "        ax1.hist(original_cates, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        ax1.set_xlabel('Original CATE (headache reduction effect)')\n",
        "        ax1.set_ylabel('Frequency')\n",
        "        ax1.set_title(f'Original CATE Distribution{title_suffix}')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        ax2.hist(normalized_cates, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "        ax2.set_xlabel('Normalized CATE (τ)')\n",
        "        ax2.set_ylabel('Frequency')\n",
        "        ax2.set_title(f'Normalized CATE Distribution{title_suffix}')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def estimate_tau(self, true_tau, accuracy):\n",
        "        \"\"\"Estimate tau using Hoeffding's inequality with Bernoulli samples.\"\"\"\n",
        "        sample_size = int(np.ceil(np.log(2/self.delta) / (2 * accuracy**2)))\n",
        "        samples = np.random.binomial(1, true_tau, sample_size)\n",
        "        return np.mean(samples), sample_size\n",
        "\n",
        "    def run_single_trial(self, groups, epsilon_val, trial_seed):\n",
        "        \"\"\"Run allocation algorithm for single trial with fixed gamma.\"\"\"\n",
        "        np.random.seed(self.random_seed + trial_seed)\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "        rho = self.gamma * np.sqrt(epsilon_val)\n",
        "\n",
        "        tau_estimates_rho = []\n",
        "        for tau in tau_true:\n",
        "            estimate, _ = self.estimate_tau(tau, rho)\n",
        "            tau_estimates_rho.append(estimate)\n",
        "        tau_estimates_rho = np.array(tau_estimates_rho)\n",
        "\n",
        "        tau_estimates_eps = []\n",
        "        for tau in tau_true:\n",
        "            estimate, _ = self.estimate_tau(tau, epsilon_val)\n",
        "            tau_estimates_eps.append(estimate)\n",
        "        tau_estimates_eps = np.array(tau_estimates_eps)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for K in range(1, n_groups):\n",
        "            optimal_indices = np.argsort(tau_true)[-K:]\n",
        "            optimal_value = np.sum(tau_true[optimal_indices])\n",
        "\n",
        "            rho_indices = np.argsort(tau_estimates_rho)[-K:]\n",
        "            rho_value = np.sum(tau_true[rho_indices])\n",
        "\n",
        "            eps_indices = np.argsort(tau_estimates_eps)[-K:]\n",
        "            eps_value = np.sum(tau_true[eps_indices])\n",
        "\n",
        "            rho_ratio = rho_value / optimal_value if optimal_value > 0 else 0\n",
        "            eps_ratio = eps_value / optimal_value if optimal_value > 0 else 0\n",
        "            rho_success = rho_ratio >= (1 - epsilon_val)\n",
        "            eps_success = eps_ratio >= (1 - epsilon_val)\n",
        "\n",
        "            tau_k_est = tau_estimates_rho[rho_indices[0]]\n",
        "            a2_lower = tau_k_est\n",
        "            a2_upper = tau_k_est + 2 * rho\n",
        "            units_in_a2 = np.sum((tau_estimates_rho >= a2_lower) & (tau_estimates_rho <= a2_upper))\n",
        "            expected_a2 = 2 * rho * n_groups\n",
        "            is_heavy = units_in_a2 > self.heavy_multiplier * expected_a2\n",
        "\n",
        "            results.append({\n",
        "                'K': K,\n",
        "                'optimal_value': optimal_value,\n",
        "                'rho_value': rho_value,\n",
        "                'eps_value': eps_value,\n",
        "                'rho_ratio': rho_ratio,\n",
        "                'eps_ratio': eps_ratio,\n",
        "                'rho_success': rho_success,\n",
        "                'eps_success': eps_success,\n",
        "                'is_heavy': is_heavy,\n",
        "                'tau_k_est': tau_k_est,\n",
        "                'units_in_a2': units_in_a2\n",
        "            })\n",
        "\n",
        "        return results, tau_estimates_rho\n",
        "\n",
        "    def find_recovery_units(self, K, tau_true, tau_estimates, epsilon_val):\n",
        "        \"\"\"Find minimum units needed to achieve 1-epsilon performance.\"\"\"\n",
        "        n_groups = len(tau_true)\n",
        "\n",
        "        rho_indices = np.argsort(tau_estimates)[-K:]\n",
        "        optimal_value = np.sum(tau_true[np.argsort(tau_true)[-K:]])\n",
        "\n",
        "        remaining_indices = np.argsort(tau_estimates)[:-K][::-1]\n",
        "\n",
        "        for extra in range(1, 11):\n",
        "            if extra > len(remaining_indices):\n",
        "                break\n",
        "\n",
        "            expanded_indices = np.concatenate([rho_indices, remaining_indices[:extra]])\n",
        "            expanded_value = np.sum(tau_true[expanded_indices])\n",
        "\n",
        "            if expanded_value / optimal_value >= (1 - epsilon_val):\n",
        "                return extra\n",
        "\n",
        "        return None\n",
        "\n",
        "    def find_closest_working_budget(self, failed_K, trial_results):\n",
        "        \"\"\"Find closest budget that works for a failed budget.\"\"\"\n",
        "        working_budgets = [r['K'] for r in trial_results if r['rho_success']]\n",
        "\n",
        "        if not working_budgets:\n",
        "            return None, None\n",
        "\n",
        "        distances_any = [abs(K - failed_K) for K in working_budgets]\n",
        "        min_distance_any = min(distances_any)\n",
        "\n",
        "        smaller_working = [K for K in working_budgets if K < failed_K]\n",
        "        if smaller_working:\n",
        "            min_distance_smaller = failed_K - max(smaller_working)\n",
        "        else:\n",
        "            min_distance_smaller = None\n",
        "\n",
        "        return min_distance_any, min_distance_smaller\n",
        "\n",
        "    def analyze_method(self, groups, epsilon_val, n_trials=30):\n",
        "        \"\"\"Analyze single method with fixed gamma and updated heavy threshold.\"\"\"\n",
        "        print(f\"\\nAnalyzing {len(groups)} groups with ε={epsilon_val}, γ={self.gamma}\")\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "\n",
        "        trial_data = []\n",
        "\n",
        "        for trial in range(n_trials):\n",
        "            print(f\"Trial {trial + 1}/{n_trials}...\")\n",
        "\n",
        "            trial_results, tau_estimates = self.run_single_trial(groups, epsilon_val, trial)\n",
        "\n",
        "            failed_results = [r for r in trial_results if not r['rho_success']]\n",
        "            failed_budgets = [r['K'] for r in failed_results]\n",
        "\n",
        "            failed_heavy_estimated = []\n",
        "            failed_heavy_true = []\n",
        "            rho = self.gamma * np.sqrt(epsilon_val)\n",
        "\n",
        "            for failed_result in failed_results:\n",
        "                K = failed_result['K']\n",
        "                failed_heavy_estimated.append(failed_result['is_heavy'])\n",
        "\n",
        "                tau_k_true = tau_true[np.argsort(tau_true)[-K:]][0]\n",
        "                a2_lower_true = tau_k_true\n",
        "                a2_upper_true = tau_k_true + 2 * rho\n",
        "                units_in_a2_true = np.sum((tau_true >= a2_lower_true) & (tau_true <= a2_upper_true))\n",
        "                expected_a2_true = 2 * rho * n_groups\n",
        "                is_heavy_true = units_in_a2_true > self.heavy_multiplier * expected_a2_true\n",
        "                failed_heavy_true.append(is_heavy_true)\n",
        "\n",
        "            print(f\"  Failed budgets: {failed_budgets}\")\n",
        "\n",
        "            if len(failed_budgets) > 0:\n",
        "                estimated_clean = [bool(x) for x in failed_heavy_estimated]\n",
        "                true_clean = [bool(x) for x in failed_heavy_true]\n",
        "                print(f\"  HEAVY INTERVALS - Estimated: {estimated_clean}\")\n",
        "                print(f\"  HEAVY INTERVALS - True τ_K:   {true_clean}\")\n",
        "\n",
        "            total_heavy = sum(r['is_heavy'] for r in trial_results)\n",
        "            failed_heavy = sum(r['is_heavy'] for r in failed_results)\n",
        "\n",
        "            recovery_units = []\n",
        "            distances_to_working_any = []\n",
        "            distances_to_working_smaller = []\n",
        "\n",
        "            for failed_result in failed_results:\n",
        "                K = failed_result['K']\n",
        "\n",
        "                recovery = self.find_recovery_units(K, tau_true, tau_estimates, epsilon_val)\n",
        "                if recovery is not None:\n",
        "                    recovery_units.append(recovery)\n",
        "\n",
        "                distance_any, distance_smaller = self.find_closest_working_budget(K, trial_results)\n",
        "                if distance_any is not None:\n",
        "                    distances_to_working_any.append(distance_any)\n",
        "                if distance_smaller is not None:\n",
        "                    distances_to_working_smaller.append(distance_smaller)\n",
        "\n",
        "            trial_info = {\n",
        "                'trial': trial,\n",
        "                'failed_budgets': failed_budgets,\n",
        "                'num_failures': len(failed_results),\n",
        "                'total_heavy': total_heavy,\n",
        "                'failed_heavy': failed_heavy,\n",
        "                'failed_heavy_estimated': failed_heavy_estimated,\n",
        "                'failed_heavy_true': failed_heavy_true,\n",
        "                'recovery_units': recovery_units,\n",
        "                'distances_to_working_any': distances_to_working_any,\n",
        "                'distances_to_working_smaller': distances_to_working_smaller\n",
        "            }\n",
        "\n",
        "            trial_data.append(trial_info)\n",
        "\n",
        "            print(f\"  Failures: {len(failed_results)}, Total heavy: {total_heavy}, Failed heavy: {failed_heavy}\")\n",
        "            if recovery_units:\n",
        "                print(f\"  Recovery units: μ={np.mean(recovery_units):.1f}, med={np.median(recovery_units):.0f}, max={np.max(recovery_units)}\")\n",
        "            if distances_to_working_any:\n",
        "                print(f\"  Distance any: μ={np.mean(distances_to_working_any):.1f}, med={np.median(distances_to_working_any):.0f}, max={np.max(distances_to_working_any)}\")\n",
        "            if distances_to_working_smaller:\n",
        "                print(f\"  Distance smaller: μ={np.mean(distances_to_working_smaller):.1f}, med={np.median(distances_to_working_smaller):.0f}, max={np.max(distances_to_working_smaller)}\")\n",
        "            else:\n",
        "                print(f\"  Distance smaller: No smaller working budgets found\")\n",
        "\n",
        "        return trial_data\n",
        "\n",
        "    def print_method_summary(self, method_name, trial_data, n_groups, epsilon_val):\n",
        "        \"\"\"Print summary statistics for a method.\"\"\"\n",
        "        budget_10pct_threshold = max(1, int(0.1 * n_groups))\n",
        "\n",
        "        print(f\"\\n{'='*100}\")\n",
        "        print(f\"SUMMARY - {method_name} - ε={epsilon_val} - {n_groups} GROUPS\")\n",
        "        print(\"=\"*100)\n",
        "        print(f\"{'Fail μ':<7} {'Fail σ':<7} {'FailR% μ':<9} {'FailR% σ':<9} {'TotHvy':<8} {'FailHvy':<9} {'Rec μ':<7} {'Rec med':<8} {'Rec max':<8} {'DAny μ':<8} {'DAny σ':<10} {'DAny max':<10} {'DSmall μ':<10} {'DSmall σ':<12} {'DSmall max':<12}\")\n",
        "        print(\"-\"*120)\n",
        "\n",
        "        all_failures = [t['num_failures'] for t in trial_data]\n",
        "        all_total_heavy = [t['total_heavy'] for t in trial_data]\n",
        "        all_failed_heavy = [t['failed_heavy'] for t in trial_data]\n",
        "        all_recovery = []\n",
        "        all_distances_any = []\n",
        "        all_distances_smaller = []\n",
        "\n",
        "        for t in trial_data:\n",
        "            all_recovery.extend(t['recovery_units'])\n",
        "            all_distances_any.extend(t['distances_to_working_any'])\n",
        "            all_distances_smaller.extend(t['distances_to_working_smaller'])\n",
        "\n",
        "        avg_failures = np.mean(all_failures)\n",
        "        std_failures = np.std(all_failures)\n",
        "        avg_failure_rate = avg_failures / (n_groups - 1) * 100\n",
        "        std_failure_rate = std_failures / (n_groups - 1) * 100\n",
        "        avg_total_heavy = np.mean(all_total_heavy)\n",
        "        avg_failed_heavy = np.mean(all_failed_heavy)\n",
        "\n",
        "        if all_recovery:\n",
        "            recovery_mean = np.mean(all_recovery)\n",
        "            recovery_med = np.median(all_recovery)\n",
        "            recovery_max = np.max(all_recovery)\n",
        "        else:\n",
        "            recovery_mean = recovery_med = recovery_max = np.nan\n",
        "\n",
        "        if all_distances_any:\n",
        "            distance_any_mean = np.mean(all_distances_any)\n",
        "            distance_any_std = np.std(all_distances_any)\n",
        "            distance_any_max = np.max(all_distances_any)\n",
        "        else:\n",
        "            distance_any_mean = distance_any_std = distance_any_max = np.nan\n",
        "\n",
        "        if all_distances_smaller:\n",
        "            distance_smaller_mean = np.mean(all_distances_smaller)\n",
        "            distance_smaller_std = np.std(all_distances_smaller)\n",
        "            distance_smaller_max = np.max(all_distances_smaller)\n",
        "        else:\n",
        "            distance_smaller_mean = distance_smaller_std = distance_smaller_max = np.nan\n",
        "\n",
        "        print(f\"{avg_failures:<7.1f} {std_failures:<7.1f} {avg_failure_rate:<9.1f} {std_failure_rate:<9.1f} {avg_total_heavy:<8.1f} {avg_failed_heavy:<9.1f} \"\n",
        "              f\"{recovery_mean:<7.1f} {recovery_med:<8.0f} {recovery_max:<8.0f} \"\n",
        "              f\"{distance_any_mean:<8.1f} {distance_any_std:<10.1f} {distance_any_max:<10.0f} \"\n",
        "              f\"{distance_smaller_mean:<10.1f} {distance_smaller_std:<12.1f} {distance_smaller_max:<12.0f}\")\n",
        "\n",
        "        return {\n",
        "            'avg_failures': avg_failures,\n",
        "            'failure_rate_pct': avg_failure_rate,\n",
        "            'avg_recovery': recovery_mean,\n",
        "            'n_groups': n_groups\n",
        "        }\n",
        "\n",
        "\n",
        "def run_comprehensive_acupuncture_analysis(df_acupuncture, epsilon_values=None, n_trials=30, log_file=None):\n",
        "    \"\"\"Run comprehensive acupuncture analysis with all methods, fixed gamma=0.5, and 1.6x heavy threshold.\"\"\"\n",
        "\n",
        "    if epsilon_values is None:\n",
        "        epsilon_values = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2]\n",
        "\n",
        "    if log_file is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        log_file = f\"acupuncture_comprehensive_analysis_gamma05_{timestamp}.txt\"\n",
        "\n",
        "    original_stdout = sys.stdout\n",
        "    tee = TeeOutput(log_file)\n",
        "    sys.stdout = tee\n",
        "\n",
        "    try:\n",
        "        print(\"COMPREHENSIVE ACUPUNCTURE ANALYSIS - ALL METHODS, FIXED γ=0.5, HEAVY THRESHOLD=1.6x\")\n",
        "        print(f\"Log file: {log_file}\")\n",
        "        print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        methods = [\n",
        "            ('Age-Chronicity Interaction', lambda allocator, df: allocator.create_age_chronicity_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Baseline-Age Interaction', lambda allocator, df: allocator.create_baseline_age_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Age Groups', lambda allocator, df: allocator.create_age_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Chronicity Groups', lambda allocator, df: allocator.create_chronicity_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Multidimensional Composite', lambda allocator, df: allocator.create_multidimensional_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Baseline Headache', lambda allocator, df: allocator.create_baseline_headache_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Covariate Forest 30', lambda allocator, df: allocator.create_covariate_forest_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Covariate Forest 50', lambda allocator, df: allocator.create_covariate_forest_groups(df, n_groups=50, min_size=6)),\n",
        "            ('Propensity Score', lambda allocator, df: allocator.create_propensity_groups(df, n_groups=50, min_size=6))\n",
        "        ]\n",
        "\n",
        "        all_results = {}\n",
        "\n",
        "        for method_name, method_func in methods:\n",
        "            print(f\"\\n{'='*120}\")\n",
        "            print(f\"ANALYZING ACUPUNCTURE METHOD: {method_name}\")\n",
        "            print(\"=\"*120)\n",
        "\n",
        "            method_results = []\n",
        "\n",
        "            for eps in epsilon_values:\n",
        "                print(f\"\\n{'='*100}\")\n",
        "                print(f\"METHOD: {method_name} | EPSILON = {eps}\")\n",
        "                print(\"=\"*100)\n",
        "\n",
        "                allocator = AcupunctureCATEAllocator(epsilon=eps, gamma=0.5, heavy_multiplier=1.6)\n",
        "                df_processed = allocator.process_acupuncture_data(df_acupuncture)\n",
        "\n",
        "                try:\n",
        "                    groups = method_func(allocator, df_processed)\n",
        "\n",
        "                    if len(groups) < 3:\n",
        "                        print(f\"Too few groups ({len(groups)}) for {method_name} with ε = {eps} - skipping\")\n",
        "                        continue\n",
        "\n",
        "                    groups = allocator.normalize_cates(groups)\n",
        "\n",
        "                    allocator.plot_cate_distribution(groups, f\" ({method_name}, ε={eps})\")\n",
        "\n",
        "                    trial_data = allocator.analyze_method(groups, eps, n_trials)\n",
        "\n",
        "                    stats = allocator.print_method_summary(method_name, trial_data, len(groups), eps)\n",
        "\n",
        "                    epsilon_result = {\n",
        "                        'method': method_name,\n",
        "                        'epsilon': eps,\n",
        "                        'sqrt_epsilon': np.sqrt(eps),\n",
        "                        'gamma': 0.5,\n",
        "                        'rho': 0.5 * np.sqrt(eps),\n",
        "                        'groups': groups,\n",
        "                        'trial_data': trial_data,\n",
        "                        'stats': stats\n",
        "                    }\n",
        "\n",
        "                    method_results.append(epsilon_result)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error with {method_name} at ε = {eps}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            all_results[method_name] = method_results\n",
        "\n",
        "            if method_results:\n",
        "                print(f\"\\n{'='*120}\")\n",
        "                print(f\"METHOD SUMMARY - {method_name} - ALL EPSILON VALUES\")\n",
        "                print(\"=\"*120)\n",
        "                print(f\"{'ε':<8} {'√ε':<10} {'γ':<6} {'ρ':<10} {'Groups':<8} {'Fail μ':<8} {'FailR%':<8} {'Rec μ':<8}\")\n",
        "                print(\"-\" * 80)\n",
        "\n",
        "                for eps_result in method_results:\n",
        "                    eps = eps_result['epsilon']\n",
        "                    sqrt_eps = eps_result['sqrt_epsilon']\n",
        "                    gamma = eps_result['gamma']\n",
        "                    rho = eps_result['rho']\n",
        "                    n_groups = len(eps_result['groups'])\n",
        "                    stats = eps_result['stats']\n",
        "\n",
        "                    print(f\"{eps:<8} {sqrt_eps:<10.6f} {gamma:<6} {rho:<10.6f} \"\n",
        "                          f\"{n_groups:<8} {stats['avg_failures']:<8.1f} {stats['failure_rate_pct']:<8.1f} \"\n",
        "                          f\"{stats['avg_recovery']:<8.1f}\")\n",
        "                print(\"=\"*120)\n",
        "\n",
        "        print(f\"\\n{'='*200}\")\n",
        "        print(\"COMPREHENSIVE SUMMARY - ALL ACUPUNCTURE METHODS AND EPSILON VALUES\")\n",
        "        print(\"=\"*200)\n",
        "\n",
        "        summary_data = []\n",
        "\n",
        "        for method_name, method_results in all_results.items():\n",
        "            if not method_results:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n{'-'*100}\")\n",
        "            print(f\"ACUPUNCTURE METHOD: {method_name}\")\n",
        "            print(\"-\"*100)\n",
        "\n",
        "            for eps_result in method_results:\n",
        "                eps = eps_result['epsilon']\n",
        "                sqrt_eps = eps_result['sqrt_epsilon']\n",
        "                gamma = eps_result['gamma']\n",
        "                rho = eps_result['rho']\n",
        "                n_groups = len(eps_result['groups'])\n",
        "                stats = eps_result['stats']\n",
        "\n",
        "                summary_data.append({\n",
        "                    'method': method_name,\n",
        "                    'epsilon': eps,\n",
        "                    'sqrt_eps': sqrt_eps,\n",
        "                    'gamma': gamma,\n",
        "                    'rho': rho,\n",
        "                    'avg_failures': stats['avg_failures'],\n",
        "                    'failure_rate_pct': stats['failure_rate_pct'],\n",
        "                    'avg_recovery': stats['avg_recovery'],\n",
        "                    'n_groups': stats['n_groups']\n",
        "                })\n",
        "\n",
        "            method_data = [d for d in summary_data if d['method'] == method_name]\n",
        "            if method_data:\n",
        "                print(f\"{'ε':<8} {'√ε':<10} {'γ':<6} {'ρ':<10} {'Groups':<8} {'Fail μ':<8} {'FailR%':<8} {'Rec μ':<8}\")\n",
        "                print(\"-\" * 80)\n",
        "\n",
        "                for data in method_data:\n",
        "                    print(f\"{data['epsilon']:<8} {data['sqrt_eps']:<10.6f} {data['gamma']:<6} {data['rho']:<10.6f} \"\n",
        "                          f\"{data['n_groups']:<8} {data['avg_failures']:<8.1f} {data['failure_rate_pct']:<8.1f} \"\n",
        "                          f\"{data['avg_recovery']:<8.1f}\")\n",
        "\n",
        "        print(f\"\\n{'='*200}\")\n",
        "        print(\"OVERALL SUMMARY TABLE - ALL ACUPUNCTURE METHODS COMBINED\")\n",
        "        print(\"=\"*200)\n",
        "        print(f\"{'Method':<18} {'ε':<8} {'√ε':<10} {'γ':<6} {'ρ':<10} {'Groups':<8} {'Fail μ':<8} {'FailR%':<8} {'Rec μ':<8}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        for data in summary_data:\n",
        "            print(f\"{data['method']:<18} {data['epsilon']:<8} {data['sqrt_eps']:<10.6f} {data['gamma']:<6} {data['rho']:<10.6f} \"\n",
        "                  f\"{data['n_groups']:<8} {data['avg_failures']:<8.1f} {data['failure_rate_pct']:<8.1f} \"\n",
        "                  f\"{data['avg_recovery']:<8.1f}\")\n",
        "\n",
        "        print(f\"\\n{'='*100}\")\n",
        "        print(\"KEY INSIGHTS FOR ACUPUNCTURE DATASET\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        if summary_data:\n",
        "            method_performance = {}\n",
        "            for method_name in all_results.keys():\n",
        "                method_data = [d for d in summary_data if d['method'] == method_name]\n",
        "                if method_data:\n",
        "                    avg_failure_rate = np.mean([d['failure_rate_pct'] for d in method_data])\n",
        "                    method_performance[method_name] = avg_failure_rate\n",
        "\n",
        "            if method_performance:\n",
        "                best_method = min(method_performance, key=method_performance.get)\n",
        "                worst_method = max(method_performance, key=method_performance.get)\n",
        "\n",
        "                print(f\"BEST PERFORMING ACUPUNCTURE METHOD: {best_method}\")\n",
        "                print(f\"  Average failure rate: {method_performance[best_method]:.1f}%\")\n",
        "\n",
        "                print(f\"\\nWORST PERFORMING ACUPUNCTURE METHOD: {worst_method}\")\n",
        "                print(f\"  Average failure rate: {method_performance[worst_method]:.1f}%\")\n",
        "\n",
        "                print(f\"\\nACUPUNCTURE METHOD RANKING (by average failure rate):\")\n",
        "                sorted_methods = sorted(method_performance.items(), key=lambda x: x[1])\n",
        "                for i, (method, rate) in enumerate(sorted_methods, 1):\n",
        "                    print(f\"  {i}. {method}: {rate:.1f}%\")\n",
        "\n",
        "        print(f\"\\nEFFECT OF EPSILON ON ACUPUNCTURE DATA:\")\n",
        "        epsilon_performance = {}\n",
        "        for eps in epsilon_values:\n",
        "            eps_data = [d for d in summary_data if d['epsilon'] == eps]\n",
        "            if eps_data:\n",
        "                avg_failure_rate = np.mean([d['failure_rate_pct'] for d in eps_data])\n",
        "                epsilon_performance[eps] = avg_failure_rate\n",
        "\n",
        "        if epsilon_performance:\n",
        "            print(f\"{'Epsilon':<10} {'Avg Failure Rate':<15} {'ρ = 0.5√ε':<12}\")\n",
        "            print(\"-\" * 40)\n",
        "            for eps in sorted(epsilon_performance.keys()):\n",
        "                rho = 0.5 * np.sqrt(eps)\n",
        "                print(f\"{eps:<10} {epsilon_performance[eps]:<15.1f} {rho:<12.6f}\")\n",
        "\n",
        "        return all_results, summary_data\n",
        "\n",
        "    finally:\n",
        "        sys.stdout = original_stdout\n",
        "        tee.close()\n",
        "        print(f\"Comprehensive acupuncture analysis completed. Results saved to: {log_file}\")\n",
        "\n",
        "\n",
        "# Example usage for acupuncture dataset\n",
        "if __name__ == \"__main__\":\n",
        "    # Load acupuncture dataset and run the analysis\n",
        "    df_acupuncture = pd.read_stata('acupuncture.dta')\n",
        "\n",
        "    # Run comprehensive acupuncture analysis with same parameters as NSW and medical\n",
        "    epsilon_values = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2]\n",
        "\n",
        "    results, summary = run_comprehensive_acupuncture_analysis(\n",
        "        df_acupuncture,\n",
        "        epsilon_values=epsilon_values,\n",
        "        n_trials=30,\n",
        "        log_file=\"acupuncture_comprehensive_analysis_gamma05_heavy16.txt\"\n",
        "    )"
      ]
    }
  ]
}