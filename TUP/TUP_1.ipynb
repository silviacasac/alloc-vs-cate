{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrjSYjb7S9A7"
      },
      "outputs": [],
      "source": [
        "# TUP CATE ANALYSIS - COMPLETE SAMPLE SIZE ANALYSIS WITH ALL GROUPING METHODS\n",
        "# Updated to include all grouping methods from previous TUP code\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import warnings\n",
        "import sys\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def theoretical_bound(m, beta, N, delta, OPT):\n",
        "    \"\"\"Compute theoretical bound (1 - (N*ln(2N/delta)/m)^beta) * OPT\"\"\"\n",
        "    term = N * np.log(2 * N / delta) / m\n",
        "    if term >= 1:\n",
        "        return 0  # Bound becomes meaningless\n",
        "    return (1 - term**beta) * OPT\n",
        "\n",
        "class CompleteTUPSampleSizeAnalyzer:\n",
        "    \"\"\"Complete TUP CATE allocation with all grouping methods.\"\"\"\n",
        "\n",
        "    def __init__(self, random_seed=42):\n",
        "        self.random_seed = random_seed\n",
        "        np.random.seed(random_seed)\n",
        "        print(f\"Complete TUP Sample Size Analyzer initialized with seed {random_seed}\")\n",
        "\n",
        "    def process_tup_data(self, df, outcome_col=None):\n",
        "        \"\"\"Process TUP dataset - SAME AS ORIGINAL.\"\"\"\n",
        "        print(f\"Processing TUP data with {len(df)} observations\")\n",
        "        print(f\"Available columns: {len(df.columns)} columns\")\n",
        "\n",
        "        # DEBUG: Check what columns we actually have\n",
        "        target_cols = [col for col in df.columns if 'target' in col.lower()]\n",
        "        consumption_cols = [col for col in df.columns if 'consumption' in col.lower()]\n",
        "        outcome_cols = [col for col in df.columns if 'outcome' in col.lower()]\n",
        "\n",
        "        print(f\"Columns with 'target': {target_cols}\")\n",
        "        print(f\"Columns with 'consumption': {consumption_cols}\")\n",
        "        print(f\"Columns with 'outcome': {outcome_cols}\")\n",
        "\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Check for required columns\n",
        "        if 'treatment' not in df_processed.columns:\n",
        "            raise ValueError(\"Missing required 'treatment' column\")\n",
        "\n",
        "        # First option: If outcome column already exists, use it directly\n",
        "        if 'outcome' in df_processed.columns:\n",
        "            print(\"Found existing 'outcome' column - using directly\")\n",
        "            # Look for baseline consumption\n",
        "            baseline_cols = [col for col in df.columns if 'pc_exp_month_bl' in col or ('bl' in col and 'exp' in col)]\n",
        "            if baseline_cols:\n",
        "                df_processed['baseline_consumption'] = df_processed[baseline_cols[0]]\n",
        "                print(f\"Using {baseline_cols[0]} as baseline consumption\")\n",
        "            else:\n",
        "                print(\"Warning: No baseline consumption found, but proceeding with existing outcome\")\n",
        "                df_processed['baseline_consumption'] = 0\n",
        "\n",
        "        # Second option: If target_column_consumption already exists\n",
        "        elif 'target_column_consumption' in df_processed.columns:\n",
        "            print(\"Found existing target_column_consumption - using as outcome\")\n",
        "            df_processed['outcome'] = df_processed['target_column_consumption']\n",
        "            baseline_cols = [col for col in df.columns if 'pc_exp_month_bl' in col or ('bl' in col and 'exp' in col)]\n",
        "            if baseline_cols:\n",
        "                df_processed['baseline_consumption'] = df_processed[baseline_cols[0]]\n",
        "                print(f\"Using {baseline_cols[0]} as baseline consumption\")\n",
        "            else:\n",
        "                print(\"Warning: No baseline consumption found, but proceeding with existing outcome\")\n",
        "                df_processed['baseline_consumption'] = 0\n",
        "\n",
        "        # Third option: If outcome column is explicitly specified\n",
        "        elif outcome_col and outcome_col in df_processed.columns:\n",
        "            print(f\"Using provided outcome column: {outcome_col}\")\n",
        "            df_processed['outcome'] = df_processed[outcome_col]\n",
        "            baseline_consumption_cols = [col for col in df.columns if 'bl' in col and any(x in col.lower() for x in ['exp', 'consumption', 'income'])]\n",
        "            if baseline_consumption_cols:\n",
        "                df_processed['baseline_consumption'] = df_processed[baseline_consumption_cols[0]]\n",
        "            else:\n",
        "                df_processed['baseline_consumption'] = 0\n",
        "\n",
        "        # Final cleaning\n",
        "        initial_size = len(df_processed)\n",
        "        df_processed = df_processed.dropna(subset=['outcome', 'treatment'])\n",
        "        final_size = len(df_processed)\n",
        "\n",
        "        if initial_size != final_size:\n",
        "            print(f\"Dropped {initial_size - final_size} additional rows due to missing outcome/treatment\")\n",
        "\n",
        "        print(f\"Final dataset: {final_size} households\")\n",
        "        print(f\"Treatment distribution: {df_processed['treatment'].value_counts().to_dict()}\")\n",
        "        print(f\"Outcome statistics: mean={df_processed['outcome'].mean():.3f}, std={df_processed['outcome'].std():.3f}\")\n",
        "\n",
        "        if 'baseline_consumption' in df_processed.columns:\n",
        "            print(f\"Baseline consumption stats: mean={df_processed['baseline_consumption'].mean():.3f}, std={df_processed['baseline_consumption'].std():.3f}\")\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def create_baseline_poverty_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups by baseline consumption.\"\"\"\n",
        "        print(f\"Creating baseline poverty groups (target: {n_groups})\")\n",
        "\n",
        "        if 'baseline_consumption' not in df.columns:\n",
        "            print(\"No baseline consumption found, using first available consumption measure\")\n",
        "            consumption_cols = [col for col in df.columns if 'pc_exp' in col and 'bl' in col]\n",
        "            if consumption_cols:\n",
        "                baseline_col = consumption_cols[0]\n",
        "                df['baseline_consumption'] = df[baseline_col]\n",
        "            else:\n",
        "                print(\"No baseline consumption measures found\")\n",
        "                return []\n",
        "\n",
        "        consumption = df['baseline_consumption'].fillna(df['baseline_consumption'].median())\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(consumption, percentiles)\n",
        "        bins = np.digitize(consumption, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'poverty_level_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'baseline_poverty'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} baseline poverty groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_demographics_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups by demographics.\"\"\"\n",
        "        print(f\"Creating TUP demographics groups\")\n",
        "\n",
        "        potential_features = []\n",
        "        demo_patterns = ['female', 'gender', 'head', 'age', 'education', 'literate',\n",
        "                        'married', 'widow', 'household_size', 'children', 'caste', 'religion']\n",
        "\n",
        "        for pattern in demo_patterns:\n",
        "            matching_cols = [col for col in df.columns if pattern in col.lower() and not col.startswith('gkt')]\n",
        "            potential_features.extend(matching_cols)\n",
        "\n",
        "        potential_features = list(set(potential_features))\n",
        "\n",
        "        available_features = []\n",
        "        for col in potential_features:\n",
        "            if col in df.columns and df[col].notna().sum() > 0:\n",
        "                unique_vals = df[col].nunique()\n",
        "                if 2 <= unique_vals <= 10:\n",
        "                    available_features.append(col)\n",
        "\n",
        "        if len(available_features) == 0:\n",
        "            print(\"No suitable demographic variables found, creating simple binary splits\")\n",
        "            if 'baseline_consumption' in df.columns:\n",
        "                median_consumption = df['baseline_consumption'].median()\n",
        "                df['consumption_above_median'] = (df['baseline_consumption'] > median_consumption).astype(int)\n",
        "                available_features = ['consumption_above_median']\n",
        "            else:\n",
        "                return []\n",
        "\n",
        "        print(f\"Using demographic features: {available_features}\")\n",
        "        if len(available_features) > 3:\n",
        "            available_features = available_features[:3]\n",
        "\n",
        "        df_clean = df.dropna(subset=available_features)\n",
        "        print(f\"After removing missing values: {len(df_clean)}/{len(df)} households\")\n",
        "\n",
        "        if len(df_clean) == 0:\n",
        "            return []\n",
        "\n",
        "        groups = []\n",
        "        unique_combinations = df_clean[available_features].drop_duplicates()\n",
        "        print(f\"Found {len(unique_combinations)} unique demographic combinations\")\n",
        "\n",
        "        for combo_idx, (idx, combo) in enumerate(unique_combinations.iterrows()):\n",
        "            mask = pd.Series(True, index=df.index)\n",
        "            combo_description = []\n",
        "\n",
        "            for feature in available_features:\n",
        "                mask = mask & (df[feature] == combo[feature])\n",
        "                combo_description.append(f\"{feature}={combo[feature]}\")\n",
        "\n",
        "            indices = df[mask].index.tolist()\n",
        "            combo_id = \"_\".join(combo_description)\n",
        "\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': combo_id,\n",
        "                    'indices': indices,\n",
        "                    'type': 'demographics'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} demographic groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_village_groups(self, df, min_size=6):\n",
        "        \"\"\"Create village groups.\"\"\"\n",
        "        print(f\"Creating village-based groups from survey indicators (min_size={min_size})\")\n",
        "\n",
        "        location_patterns = ['district', 'rural', 'urban', 'capital', 'area', 'upazila', 'thana']\n",
        "        location_cols = []\n",
        "\n",
        "        for pattern in location_patterns:\n",
        "            matching_cols = [col for col in df.columns\n",
        "                           if pattern in col.lower() and 'bl_' in col\n",
        "                           and not any(x in col.lower() for x in ['gram flour', 'food', 'loan'])]\n",
        "            location_cols.extend(matching_cols)\n",
        "\n",
        "        location_cols = list(set(location_cols))\n",
        "\n",
        "        if location_cols:\n",
        "            print(f\"Found location indicator columns: {location_cols[:3]}...\")\n",
        "            location_col = location_cols[0]\n",
        "            print(f\"Using location indicator: {location_col}\")\n",
        "\n",
        "            groups = []\n",
        "            for location_value in df[location_col].unique():\n",
        "                if pd.isna(location_value):\n",
        "                    continue\n",
        "\n",
        "                indices = df[df[location_col] == location_value].index.tolist()\n",
        "                if len(indices) >= min_size:\n",
        "                    groups.append({\n",
        "                        'id': f'location_{location_col}_{location_value}',\n",
        "                        'indices': indices,\n",
        "                        'type': 'village'\n",
        "                    })\n",
        "        else:\n",
        "            print(\"No location indicators found\")\n",
        "\n",
        "        print(f\"Raw geographic groups created: {len(groups)}\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        print(f\"Balanced geographic groups after filtering: {len(balanced_groups)}\")\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_causal_forest_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create causal forest groups.\"\"\"\n",
        "        print(f\"Creating causal forest groups (target: {n_groups})\")\n",
        "\n",
        "        exclude_patterns = ['outcome', 'treatment', 'el1', 'el2', 'el3', 'el4', 'total_score']\n",
        "        feature_cols = [col for col in df.columns\n",
        "                       if not any(pattern in col for pattern in exclude_patterns)]\n",
        "\n",
        "        X = df[feature_cols].copy()\n",
        "\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
        "                try:\n",
        "                    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "                except:\n",
        "                    X[col] = 0\n",
        "            elif X[col].dtype == 'bool':\n",
        "                X[col] = X[col].astype(int)\n",
        "\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(0)\n",
        "\n",
        "        treated_mask = df['treatment'] == 1\n",
        "        control_mask = df['treatment'] == 0\n",
        "\n",
        "        if treated_mask.sum() < 5 or control_mask.sum() < 5:\n",
        "            print(\"Not enough treated or control observations for causal forest\")\n",
        "            return []\n",
        "\n",
        "        rf_treated = RandomForestRegressor(n_estimators=100, random_state=self.random_seed)\n",
        "        rf_control = RandomForestRegressor(n_estimators=100, random_state=self.random_seed)\n",
        "\n",
        "        rf_treated.fit(X[treated_mask], df.loc[treated_mask, 'outcome'])\n",
        "        rf_control.fit(X[control_mask], df.loc[control_mask, 'outcome'])\n",
        "\n",
        "        pred_cate = rf_treated.predict(X) - rf_control.predict(X)\n",
        "        cluster_features = np.column_stack([X.values, pred_cate.reshape(-1, 1)])\n",
        "        cluster_features = StandardScaler().fit_transform(cluster_features)\n",
        "\n",
        "        labels = KMeans(n_clusters=n_groups, random_state=self.random_seed).fit_predict(cluster_features)\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[labels == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'causal_forest_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'causal_forest'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} causal forest groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_propensity_groups(self, df, n_groups=50, min_size=6):\n",
        "        \"\"\"Create groups based on propensity score strata.\"\"\"\n",
        "        print(f\"Creating propensity score groups (target: {n_groups})\")\n",
        "\n",
        "        feature_cols = [col for col in df.columns\n",
        "                       if col not in ['treatment', 'outcome', 'target_column_consumption']]\n",
        "\n",
        "        X = df[feature_cols].copy()\n",
        "\n",
        "        # Handle different data types properly\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
        "                X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "            elif X[col].dtype == 'bool':\n",
        "                X[col] = X[col].astype(int)\n",
        "\n",
        "        # Fill missing values properly\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 0)\n",
        "\n",
        "        # Get propensity scores\n",
        "        prop_scores = cross_val_predict(\n",
        "            LogisticRegression(random_state=self.random_seed),\n",
        "            X, df['treatment'], method='predict_proba', cv=5\n",
        "        )[:, 1]\n",
        "\n",
        "        # Create strata\n",
        "        quantiles = np.linspace(0, 1, n_groups + 1)\n",
        "        bins = np.digitize(prop_scores, np.quantile(prop_scores, quantiles)) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'propensity_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'propensity'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} propensity groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_asset_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on asset ownership patterns.\"\"\"\n",
        "        print(f\"Creating asset groups (target: {n_groups})\")\n",
        "\n",
        "        # Look for asset-related columns\n",
        "        asset_patterns = ['asset', 'own', 'land', 'livestock', 'house', 'roof', 'wall', 'floor', 'toilet']\n",
        "        asset_cols = []\n",
        "\n",
        "        for pattern in asset_patterns:\n",
        "            matching_cols = [col for col in df.columns if pattern in col.lower() and 'bl' in col.lower()]\n",
        "            asset_cols.extend(matching_cols)\n",
        "\n",
        "        asset_cols = list(set(asset_cols))\n",
        "\n",
        "        if not asset_cols:\n",
        "            print(\"No asset columns found, using baseline consumption as proxy\")\n",
        "            return self.create_baseline_poverty_groups(df, n_groups, min_size)\n",
        "\n",
        "        print(f\"Found {len(asset_cols)} asset-related columns\")\n",
        "\n",
        "        # Take subset of asset columns if too many\n",
        "        if len(asset_cols) > 10:\n",
        "            asset_cols = asset_cols[:10]\n",
        "\n",
        "        X = df[asset_cols].copy()\n",
        "\n",
        "        # Handle missing values and encode\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
        "                X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "            elif X[col].dtype == 'bool':\n",
        "                X[col] = X[col].astype(int)\n",
        "\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(0)\n",
        "\n",
        "        # Cluster based on asset patterns\n",
        "        X_scaled = StandardScaler().fit_transform(X)\n",
        "        labels = KMeans(n_clusters=n_groups, random_state=self.random_seed).fit_predict(X_scaled)\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[labels == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'asset_cluster_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'asset'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} asset groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_household_composition_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups based on household composition.\"\"\"\n",
        "        print(f\"Creating household composition groups\")\n",
        "\n",
        "        # Look for household composition variables\n",
        "        hh_patterns = ['household_size', 'children', 'adult', 'elderly', 'dependent', 'member']\n",
        "        hh_cols = []\n",
        "\n",
        "        for pattern in hh_patterns:\n",
        "            matching_cols = [col for col in df.columns if pattern in col.lower() and 'bl' in col.lower()]\n",
        "            hh_cols.extend(matching_cols)\n",
        "\n",
        "        hh_cols = list(set(hh_cols))\n",
        "\n",
        "        if not hh_cols:\n",
        "            print(\"No household composition columns found\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Found household composition columns: {hh_cols}\")\n",
        "\n",
        "        # Create categorical splits based on household size and composition\n",
        "        groups = []\n",
        "\n",
        "        # Try household size first\n",
        "        size_cols = [col for col in hh_cols if 'size' in col.lower()]\n",
        "        if size_cols:\n",
        "            size_col = size_cols[0]\n",
        "            size_values = df[size_col].fillna(df[size_col].median())\n",
        "\n",
        "            # Create size categories\n",
        "            size_categories = pd.cut(size_values, bins=3, labels=['Small', 'Medium', 'Large'])\n",
        "\n",
        "            for category in size_categories.categories:\n",
        "                indices = df.index[size_categories == category].tolist()\n",
        "                if len(indices) >= min_size:\n",
        "                    groups.append({\n",
        "                        'id': f'household_size_{category}',\n",
        "                        'indices': indices,\n",
        "                        'type': 'household_composition'\n",
        "                    })\n",
        "\n",
        "        # Try children categories\n",
        "        children_cols = [col for col in hh_cols if 'children' in col.lower()]\n",
        "        if children_cols and len(groups) < 5:\n",
        "            children_col = children_cols[0]\n",
        "            children_values = df[children_col].fillna(0)\n",
        "\n",
        "            # Create children categories\n",
        "            for threshold in [0, 2]:\n",
        "                if threshold == 0:\n",
        "                    mask = children_values == 0\n",
        "                    label = 'no_children'\n",
        "                else:\n",
        "                    mask = children_values >= threshold\n",
        "                    label = f'children_gte_{threshold}'\n",
        "\n",
        "                indices = df.index[mask].tolist()\n",
        "                if len(indices) >= min_size:\n",
        "                    groups.append({\n",
        "                        'id': f'household_{label}',\n",
        "                        'indices': indices,\n",
        "                        'type': 'household_composition'\n",
        "                    })\n",
        "\n",
        "        print(f\"Created {len(groups)} household composition groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def _ensure_balance_and_compute_cate(self, df, groups):\n",
        "        \"\"\"Ensure balance and compute CATE.\"\"\"\n",
        "        balanced_groups = []\n",
        "\n",
        "        for group in groups:\n",
        "            group_df = df.loc[group['indices']]\n",
        "\n",
        "            treatment_rate = group_df['treatment'].mean()\n",
        "            n_treated = group_df['treatment'].sum()\n",
        "            n_control = len(group_df) - n_treated\n",
        "\n",
        "            if not (0.15 <= treatment_rate <= 0.85 and n_treated >= 3 and n_control >= 3):\n",
        "                continue\n",
        "\n",
        "            treated_outcomes = group_df[group_df['treatment'] == 1]['outcome']\n",
        "            control_outcomes = group_df[group_df['treatment'] == 0]['outcome']\n",
        "            cate = treated_outcomes.mean() - control_outcomes.mean()\n",
        "\n",
        "            balanced_groups.append({\n",
        "                'id': group['id'],\n",
        "                'indices': group['indices'],\n",
        "                'size': len(group_df),\n",
        "                'treatment_rate': treatment_rate,\n",
        "                'n_treated': int(n_treated),\n",
        "                'n_control': int(n_control),\n",
        "                'cate': cate,\n",
        "                'type': group['type']\n",
        "            })\n",
        "\n",
        "        return balanced_groups\n",
        "\n",
        "    def normalize_cates(self, groups):\n",
        "        \"\"\"Normalize CATE values to [0,1].\"\"\"\n",
        "        cates = [g['cate'] for g in groups]\n",
        "        min_cate, max_cate = min(cates), max(cates)\n",
        "\n",
        "        if max_cate > min_cate:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = (group['cate'] - min_cate) / (max_cate - min_cate)\n",
        "        else:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = 0.5\n",
        "\n",
        "        print(f\"CATE normalization: [{min_cate:.3f}, {max_cate:.3f}] → [0, 1]\")\n",
        "        return groups\n",
        "\n",
        "    def simulate_sampling_trial(self, groups, sample_size, trial_seed):\n",
        "        \"\"\"Simulate sampling trial\"\"\"\n",
        "        np.random.seed(self.random_seed + trial_seed)\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "\n",
        "        # Initialize tau estimates\n",
        "        tau_estimates = np.zeros(n_groups)\n",
        "        sample_counts = np.zeros(n_groups)\n",
        "\n",
        "        # Perform sampling: choose group uniformly, sample Bernoulli(tau(u))\n",
        "        for _ in range(sample_size):\n",
        "            group_idx = np.random.randint(n_groups)\n",
        "            sample = np.random.binomial(1, tau_true[group_idx])\n",
        "\n",
        "            sample_counts[group_idx] += 1\n",
        "            if sample_counts[group_idx] == 1:\n",
        "                tau_estimates[group_idx] = sample\n",
        "            else:\n",
        "                tau_estimates[group_idx] = ((sample_counts[group_idx] - 1) * tau_estimates[group_idx] + sample) / sample_counts[group_idx]\n",
        "\n",
        "        # Groups with no samples get estimate 0\n",
        "        tau_estimates[sample_counts == 0] = 0\n",
        "\n",
        "        return tau_estimates, sample_counts\n",
        "\n",
        "    def analyze_sample_size_performance(self, groups, sample_sizes, budget_percentages, n_trials=50):\n",
        "        \"\"\"Analyze performance vs sample size.\"\"\"\n",
        "        print(f\"Analyzing sample size performance with {len(groups)} groups\")\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "\n",
        "        # Calculate budgets\n",
        "        budgets = [max(1, int(p * n_groups)) for p in budget_percentages]\n",
        "        print(f\"Budget percentages {budget_percentages} → K values {budgets}\")\n",
        "\n",
        "        # Calculate optimal values\n",
        "        optimal_values = {}\n",
        "        for i, K in enumerate(budgets):\n",
        "            optimal_indices = np.argsort(tau_true)[-K:]\n",
        "            optimal_values[budget_percentages[i]] = np.sum(tau_true[optimal_indices])\n",
        "\n",
        "        # Run trials\n",
        "        results = {bp: {'sample_sizes': [], 'values': [], 'stds': []} for bp in budget_percentages}\n",
        "\n",
        "        for sample_size in sample_sizes:\n",
        "            print(f\"  Sample size {sample_size}...\")\n",
        "\n",
        "            budget_trial_values = {bp: [] for bp in budget_percentages}\n",
        "\n",
        "            for trial in range(n_trials):\n",
        "                tau_estimates, sample_counts = self.simulate_sampling_trial(groups, sample_size, trial)\n",
        "\n",
        "                for i, K in enumerate(budgets):\n",
        "                    bp = budget_percentages[i]\n",
        "\n",
        "                    # Select top K based on estimates\n",
        "                    selected_indices = np.argsort(tau_estimates)[-K:]\n",
        "\n",
        "                    # Compute realized value with true tau\n",
        "                    realized_value = np.sum(tau_true[selected_indices])\n",
        "                    budget_trial_values[bp].append(realized_value)\n",
        "\n",
        "            # Store results\n",
        "            for bp in budget_percentages:\n",
        "                results[bp]['sample_sizes'].append(sample_size)\n",
        "                results[bp]['values'].append(np.mean(budget_trial_values[bp]))\n",
        "                results[bp]['stds'].append(np.std(budget_trial_values[bp]))\n",
        "\n",
        "        return results, optimal_values\n",
        "\n",
        "    def plot_sample_size_analysis(self, results, optimal_values, method_name, budget_percentages, n_groups):\n",
        "        \"\"\"Create 6 plots (one per budget) for sample size analysis with theoretical bounds\"\"\"\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        # Calculate parameters for theoretical bounds\n",
        "        delta = 0.05\n",
        "\n",
        "        print(f\"\\nPlotting {method_name} (M={n_groups})\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for i, bp in enumerate([0.2, 0.3, 0.5, 0.7]):\n",
        "            ax = axes[i]\n",
        "\n",
        "            # Get data for this budget\n",
        "            sample_sizes = results[bp]['sample_sizes']\n",
        "            values = results[bp]['values']\n",
        "            stds = results[bp]['stds']\n",
        "            optimal_val = optimal_values[bp]\n",
        "\n",
        "            # Normalize all values by optimal value\n",
        "            values_norm = np.array(values) / optimal_val\n",
        "            stds_norm = np.array(stds) / optimal_val\n",
        "\n",
        "            # Plot empirical performance curve\n",
        "            ax.errorbar(sample_sizes, values_norm, yerr=stds_norm,\n",
        "                      marker='o', capsize=5, capthick=3, linewidth=6, markersize=8,\n",
        "                      label='Empirical data', color='blue', alpha=0.8)\n",
        "\n",
        "            # Plot optimal value (normalized to 1)\n",
        "            ax.axhline(y=1.0, color='black', linestyle=':', linewidth=2,\n",
        "                      label='Optimal (1.0)', alpha=0.8)\n",
        "\n",
        "            # Create smooth curves for plotting theoretical bounds\n",
        "            m_smooth = np.linspace(min(sample_sizes), max(sample_sizes), 200)\n",
        "\n",
        "            # Plot reference curves with MUCH thicker lines and normalize\n",
        "            ref_curve_05 = [theoretical_bound(m, 0.5, n_groups, delta, optimal_val) / optimal_val for m in m_smooth]\n",
        "            ref_curve_10 = [theoretical_bound(m, 1.0, n_groups, delta, optimal_val) / optimal_val for m in m_smooth]\n",
        "\n",
        "            ax.plot(m_smooth, ref_curve_05, 'red', linestyle=(0, (3, 2)), linewidth=6,\n",
        "                  label='FullCATE', alpha=0.8)\n",
        "            ax.plot(m_smooth, ref_curve_10, 'green', linestyle=(0, (3, 1, 1, 1)), linewidth=6,\n",
        "                  label='ALLOC', alpha=0.8)\n",
        "\n",
        "            # Set labels\n",
        "            ax.set_xlabel('Sample size', fontsize=23)\n",
        "            ax.set_ylabel('Normalized allocation value', fontsize=23)\n",
        "            ax.set_title(f'Budget = {bp*100:.0f}% (K={max(1, int(bp * n_groups))})', fontsize=24, fontweight='bold')\n",
        "\n",
        "            ax.legend(fontsize=21, framealpha=0.9)\n",
        "            ax.grid(True, alpha=0.4, linewidth=1)\n",
        "\n",
        "            ax.tick_params(axis='both', which='major', labelsize=16, width=1.5, length=5)\n",
        "\n",
        "            y_min = 0.2\n",
        "            y_max = 1.05  # Slightly above optimal\n",
        "            ax.set_ylim(y_min, y_max)\n",
        "\n",
        "            for spine in ax.spines.values():\n",
        "                spine.set_linewidth(1.5)\n",
        "\n",
        "        plt.suptitle(f'{method_name} (M={n_groups})', fontsize=24, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        clean_name = method_name.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')\n",
        "        pdf_filename = f\"{clean_name}_M{n_groups}_sample_size_analysis.pdf\"\n",
        "        plt.savefig(pdf_filename, format='pdf', dpi=300, bbox_inches='tight')\n",
        "        print(f\"Saved plot as: {pdf_filename}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Plot complete for {method_name}\")\n",
        "\n",
        "\n",
        "def run_complete_tup_sample_size_analysis(df_tup, sample_size_range=None, budget_percentages=None, n_trials=50, outcome_col=None):\n",
        "    \"\"\"Run comprehensive sample size analysis on TUP dataset with all grouping methods.\"\"\"\n",
        "\n",
        "    if sample_size_range is None:\n",
        "        sample_size_range = [100, 250, 500, 750, 1000, 1200, 1500, 2000, 5000, 10000, 20000]\n",
        "\n",
        "    if budget_percentages is None:\n",
        "        budget_percentages = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "    print(\"COMPLETE TUP SAMPLE SIZE ANALYSIS - ALL GROUPING METHODS\")\n",
        "    print(f\"Sample sizes: {sample_size_range}\")\n",
        "    print(f\"Budget percentages: {budget_percentages}\")\n",
        "    print(f\"Trials per sample size: {n_trials}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Define ALL TUP grouping methods\n",
        "    methods = [\n",
        "        ('Village Groups', lambda analyzer, df: analyzer.create_village_groups(df, min_size=6)),\n",
        "        ('Baseline Poverty (30)', lambda analyzer, df: analyzer.create_baseline_poverty_groups(df, n_groups=30, min_size=6)),\n",
        "        ('Baseline Poverty (50)', lambda analyzer, df: analyzer.create_baseline_poverty_groups(df, n_groups=50, min_size=6)),\n",
        "        ('Demographics', lambda analyzer, df: analyzer.create_demographics_groups(df, min_size=6)),\n",
        "        ('Causal Forest (30)', lambda analyzer, df: analyzer.create_causal_forest_groups(df, n_groups=30, min_size=6)),\n",
        "        ('Causal Forest (50)', lambda analyzer, df: analyzer.create_causal_forest_groups(df, n_groups=50, min_size=6)),\n",
        "        ('Propensity Score (30)', lambda analyzer, df: analyzer.create_propensity_groups(df, n_groups=30, min_size=6)),\n",
        "        ('Propensity Score (50)', lambda analyzer, df: analyzer.create_propensity_groups(df, n_groups=50, min_size=6)),\n",
        "        ('Asset Groups (30)', lambda analyzer, df: analyzer.create_asset_groups(df, n_groups=30, min_size=6)),\n",
        "        ('Asset Groups (50)', lambda analyzer, df: analyzer.create_asset_groups(df, n_groups=50, min_size=6)),\n",
        "        ('Household Composition', lambda analyzer, df: analyzer.create_household_composition_groups(df, min_size=6))\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    for method_name, method_func in methods:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ANALYZING TUP METHOD: {method_name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        try:\n",
        "            analyzer = CompleteTUPSampleSizeAnalyzer()\n",
        "            df_processed = analyzer.process_tup_data(df_tup, outcome_col=outcome_col)\n",
        "\n",
        "            groups = method_func(analyzer, df_processed)\n",
        "\n",
        "            if len(groups) < 10:\n",
        "                print(f\"Too few groups ({len(groups)}) for {method_name} - skipping\")\n",
        "                continue\n",
        "\n",
        "            groups = analyzer.normalize_cates(groups)\n",
        "\n",
        "            # Run sample size analysis\n",
        "            results, optimal_values = analyzer.analyze_sample_size_performance(\n",
        "                groups, sample_size_range, budget_percentages, n_trials\n",
        "            )\n",
        "\n",
        "            all_results[method_name] = {\n",
        "                'results': results,\n",
        "                'optimal_values': optimal_values,\n",
        "                'n_groups': len(groups)\n",
        "            }\n",
        "\n",
        "            # Create plots with theoretical bounds\n",
        "            print(f\"Creating plots for {method_name}...\")\n",
        "            analyzer.plot_sample_size_analysis(\n",
        "                results, optimal_values, method_name, budget_percentages, len(groups)\n",
        "            )\n",
        "\n",
        "            # Print summary\n",
        "            print(f\"\\nSummary for {method_name}:\")\n",
        "            print(f\"Number of groups: {len(groups)}\")\n",
        "            print(\"Optimal values by budget:\")\n",
        "            for bp in budget_percentages:\n",
        "                print(f\"  {bp*100:.0f}%: {optimal_values[bp]:.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {method_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def preprocess_tup_data(filepath):\n",
        "    \"\"\"Preprocess TUP data using the same approach as the original code.\"\"\"\n",
        "    # Load the data\n",
        "    df1 = pd.read_stata(filepath)\n",
        "    print(f\"Loaded TUP data: {df1.shape}\")\n",
        "\n",
        "    # Follow the original preprocessing steps\n",
        "    columns_to_drop_original = df1.columns[df1.columns.str.endswith('el1') |\n",
        "                                  df1.columns.str.endswith('el2') |\n",
        "                                  df1.columns.str.endswith('el3') |\n",
        "                                  df1.columns.str.endswith('el4') | df1.columns.str.startswith('el')]\n",
        "\n",
        "    categorical_columns = df1.select_dtypes(include=['object']).columns\n",
        "    unique_value_counts = df1[categorical_columns].nunique()\n",
        "\n",
        "    leave_categories = []\n",
        "    keep_categories = []\n",
        "    for i in categorical_columns:\n",
        "        if unique_value_counts[i] > 2 * unique_value_counts.mean():\n",
        "            leave_categories.append(i)\n",
        "        else:\n",
        "            keep_categories.append(i)\n",
        "\n",
        "    # Keep only relevant columns\n",
        "    object_columns = df1.select_dtypes(include=['object'])\n",
        "    cols_keep = []\n",
        "    for col in df1.columns:\n",
        "        if col not in object_columns and col not in leave_categories and col not in columns_to_drop_original:\n",
        "            cols_keep.append(col)\n",
        "\n",
        "    df1_filtered = df1[cols_keep]\n",
        "    df1_encoded = pd.get_dummies(df1_filtered)\n",
        "\n",
        "    # Create target variable\n",
        "    target_col_consumption = df1['pc_exp_month_el3'] - df1['pc_exp_month_bl']\n",
        "    df1_encoded['target_column_consumption'] = target_col_consumption\n",
        "\n",
        "    # Clean data\n",
        "    df1_encoded = df1_encoded[df1_encoded['target_column_consumption'].notna()]\n",
        "    df1_encoded = df1_encoded.dropna(axis=1, how='all')\n",
        "    df1_encoded = df1_encoded.fillna(df1_encoded.mean())\n",
        "\n",
        "    # Feature selection using Random Forest\n",
        "    X = df1_encoded.drop(columns='target_column_consumption')\n",
        "    y = df1_encoded['target_column_consumption']\n",
        "\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf.fit(X, y)\n",
        "\n",
        "    feature_importances = rf.feature_importances_\n",
        "    importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "    top_1000_features = importance_df.sort_values(by='Importance', ascending=False).head(1000)['Feature'].tolist()\n",
        "\n",
        "    # Ensure treatment and baseline consumption are included\n",
        "    if 'treatment' not in top_1000_features:\n",
        "        top_1000_features.append('treatment')\n",
        "    if 'pc_exp_month_bl' not in top_1000_features:\n",
        "        top_1000_features.append('pc_exp_month_bl')\n",
        "\n",
        "    df_final = df1_encoded[top_1000_features].copy()\n",
        "\n",
        "    df_final['target_column_consumption'] = df1_encoded['target_column_consumption']\n",
        "    df_final['outcome'] = df1_encoded['target_column_consumption']  # Also create 'outcome' column\n",
        "\n",
        "    return df_final\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess TUP data\n",
        "    df_tup = preprocess_tup_data('TUP_HH_Constructed.dta')\n",
        "\n",
        "    # Run comprehensive sample size analysis with all grouping methods\n",
        "    sample_sizes = [100, 250, 500, 750, 1000, 1200, 1500, 2000, 5000, 10000, 20000]\n",
        "    budget_percentages = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "    results = run_complete_tup_sample_size_analysis(\n",
        "        df_tup,\n",
        "        sample_size_range=sample_sizes,\n",
        "        budget_percentages=budget_percentages,\n",
        "        n_trials=50\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPLETE TUP ANALYSIS WITH ALL GROUPING METHODS FINISHED\")\n",
        "    print(\"=\"*80)"
      ]
    }
  ]
}
