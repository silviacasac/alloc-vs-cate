{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8NWNZqZT7H-"
      },
      "outputs": [],
      "source": [
        "# Comprehensive TUP CATE Analysis with Fixed Gamma and Heavy Interval Threshold\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import warnings\n",
        "import sys\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class TeeOutput:\n",
        "    \"\"\"Class to write output to both console and file simultaneously.\"\"\"\n",
        "    def __init__(self, filename):\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(filename, 'w')\n",
        "\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "        self.log.flush()  # Ensure immediate write to file\n",
        "\n",
        "    def flush(self):\n",
        "        self.terminal.flush()\n",
        "        self.log.flush()\n",
        "\n",
        "    def close(self):\n",
        "        self.log.close()\n",
        "\n",
        "class TUPCATEAllocator:\n",
        "    \"\"\"TUP CATE allocation algorithm with fixed gamma=0.5 and updated heavy interval threshold.\"\"\"\n",
        "\n",
        "    def __init__(self, epsilon=0.1, gamma=0.5, delta=0.05, heavy_multiplier=1.6, random_seed=42):\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "        self.rho = gamma * np.sqrt(epsilon)\n",
        "        self.delta = delta\n",
        "        self.heavy_multiplier = heavy_multiplier  # New parameter for heavy interval threshold\n",
        "        self.random_seed = random_seed\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "        print(f\"TUP CATE Allocation Algorithm\")\n",
        "        print(f\"ε = {epsilon}\")\n",
        "        print(f\"√ε = {np.sqrt(epsilon):.6f}\")\n",
        "        print(f\"γ = {gamma}\")\n",
        "        print(f\"ρ = γ√ε = {self.rho:.6f}\")\n",
        "        print(f\"Heavy multiplier = {heavy_multiplier}x\")\n",
        "        print(f\"δ = {delta}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    def process_tup_data(self, df, outcome_col=None):\n",
        "        \"\"\"Process TUP dataset for analysis\"\"\"\n",
        "        print(f\"Processing TUP data with {len(df)} observations\")\n",
        "        print(f\"Available columns: {len(df.columns)} columns\")\n",
        "\n",
        "        # DEBUG: Check what columns we actually have\n",
        "        target_cols = [col for col in df.columns if 'target' in col.lower()]\n",
        "        consumption_cols = [col for col in df.columns if 'consumption' in col.lower()]\n",
        "        outcome_cols = [col for col in df.columns if 'outcome' in col.lower()]\n",
        "\n",
        "        print(f\"Columns with 'target': {target_cols}\")\n",
        "        print(f\"Columns with 'consumption': {consumption_cols}\")\n",
        "        print(f\"Columns with 'outcome': {outcome_cols}\")\n",
        "\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Check for required columns\n",
        "        if 'treatment' not in df_processed.columns:\n",
        "            raise ValueError(\"Missing required 'treatment' column\")\n",
        "\n",
        "        # First option: If outcome column already exists, use it directly\n",
        "        if 'outcome' in df_processed.columns:\n",
        "            print(\"Found existing 'outcome' column - using directly\")\n",
        "            # Keep the existing outcome column\n",
        "\n",
        "            # Look for baseline consumption\n",
        "            baseline_cols = [col for col in df.columns if 'pc_exp_month_bl' in col or ('bl' in col and 'exp' in col)]\n",
        "            if baseline_cols:\n",
        "                df_processed['baseline_consumption'] = df_processed[baseline_cols[0]]\n",
        "                print(f\"Using {baseline_cols[0]} as baseline consumption\")\n",
        "            else:\n",
        "                print(\"Warning: No baseline consumption found, but proceeding with existing outcome\")\n",
        "                df_processed['baseline_consumption'] = 0  # Placeholder\n",
        "\n",
        "        # Second option: If target_column_consumption already exists (from preprocessing), use it\n",
        "        elif 'target_column_consumption' in df_processed.columns:\n",
        "            print(\"Found existing target_column_consumption - using as outcome\")\n",
        "            df_processed['outcome'] = df_processed['target_column_consumption']\n",
        "\n",
        "            # Look for baseline consumption\n",
        "            baseline_cols = [col for col in df.columns if 'pc_exp_month_bl' in col or ('bl' in col and 'exp' in col)]\n",
        "            if baseline_cols:\n",
        "                df_processed['baseline_consumption'] = df_processed[baseline_cols[0]]\n",
        "                print(f\"Using {baseline_cols[0]} as baseline consumption\")\n",
        "            else:\n",
        "                print(\"Warning: No baseline consumption found, but proceeding with existing outcome\")\n",
        "                df_processed['baseline_consumption'] = 0  # Placeholder\n",
        "\n",
        "        # Third option: If outcome column is explicitly specified, use it\n",
        "        elif outcome_col and outcome_col in df_processed.columns:\n",
        "            print(f\"Using provided outcome column: {outcome_col}\")\n",
        "            df_processed['outcome'] = df_processed[outcome_col]\n",
        "            baseline_consumption_cols = [col for col in df.columns if 'bl' in col and any(x in col.lower() for x in ['exp', 'consumption', 'income'])]\n",
        "            if baseline_consumption_cols:\n",
        "                df_processed['baseline_consumption'] = df_processed[baseline_consumption_cols[0]]\n",
        "            else:\n",
        "                df_processed['baseline_consumption'] = 0  # Default if no baseline found\n",
        "\n",
        "        # Final cleaning\n",
        "        initial_size = len(df_processed)\n",
        "        df_processed = df_processed.dropna(subset=['outcome', 'treatment'])\n",
        "        final_size = len(df_processed)\n",
        "\n",
        "        if initial_size != final_size:\n",
        "            print(f\"Dropped {initial_size - final_size} additional rows due to missing outcome/treatment\")\n",
        "\n",
        "        print(f\"Final dataset: {final_size} households\")\n",
        "        print(f\"Treatment distribution: {df_processed['treatment'].value_counts().to_dict()}\")\n",
        "        print(f\"Outcome statistics: mean={df_processed['outcome'].mean():.3f}, std={df_processed['outcome'].std():.3f}\")\n",
        "\n",
        "        if 'baseline_consumption' in df_processed.columns:\n",
        "            print(f\"Baseline consumption stats: mean={df_processed['baseline_consumption'].mean():.3f}, std={df_processed['baseline_consumption'].std():.3f}\")\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def create_baseline_poverty_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups by baseline consumption quintiles/deciles.\"\"\"\n",
        "        print(f\"Creating baseline poverty groups (target: {n_groups})\")\n",
        "\n",
        "        if 'baseline_consumption' not in df.columns:\n",
        "            print(\"No baseline consumption found, using first available consumption measure\")\n",
        "            consumption_cols = [col for col in df.columns if 'pc_exp' in col and 'bl' in col]\n",
        "            if consumption_cols:\n",
        "                baseline_col = consumption_cols[0]\n",
        "                df['baseline_consumption'] = df[baseline_col]\n",
        "            else:\n",
        "                print(\"No baseline consumption measures found\")\n",
        "                return []\n",
        "\n",
        "        # Create consumption-based groups\n",
        "        consumption = df['baseline_consumption'].fillna(df['baseline_consumption'].median())\n",
        "\n",
        "        # Create percentile groups\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(consumption, percentiles)\n",
        "        bins = np.digitize(consumption, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'poverty_level_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'baseline_poverty'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} baseline poverty groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_demographics_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups by demographic characteristics relevant to TUP.\"\"\"\n",
        "        print(f\"Creating TUP demographics groups\")\n",
        "\n",
        "        # Look for demographic variables commonly in TUP datasets\n",
        "        potential_features = []\n",
        "\n",
        "        # Look for household head characteristics\n",
        "        demo_patterns = ['female', 'gender', 'head', 'age', 'education', 'literate',\n",
        "                        'married', 'widow', 'household_size', 'children', 'caste', 'religion']\n",
        "\n",
        "        for pattern in demo_patterns:\n",
        "            matching_cols = [col for col in df.columns if pattern in col.lower() and not col.startswith('gkt')]\n",
        "            potential_features.extend(matching_cols)\n",
        "\n",
        "        # Remove duplicates and check which ones have reasonable variation\n",
        "        potential_features = list(set(potential_features))\n",
        "\n",
        "        available_features = []\n",
        "        for col in potential_features:\n",
        "            if col in df.columns and df[col].notna().sum() > 0:\n",
        "                # Check if it's not too sparse or too uniform\n",
        "                unique_vals = df[col].nunique()\n",
        "                if 2 <= unique_vals <= 10:  # Reasonable number of categories\n",
        "                    available_features.append(col)\n",
        "\n",
        "        if len(available_features) == 0:\n",
        "            print(\"No suitable demographic variables found, creating simple binary splits\")\n",
        "            # Create simple splits based on baseline consumption\n",
        "            if 'baseline_consumption' in df.columns:\n",
        "                median_consumption = df['baseline_consumption'].median()\n",
        "                df['consumption_above_median'] = (df['baseline_consumption'] > median_consumption).astype(int)\n",
        "                available_features = ['consumption_above_median']\n",
        "            else:\n",
        "                return []\n",
        "\n",
        "        print(f\"Using demographic features: {available_features}\")\n",
        "\n",
        "        # Limit to top 3 features to avoid too many combinations\n",
        "        if len(available_features) > 3:\n",
        "            available_features = available_features[:3]\n",
        "\n",
        "        # Remove rows with missing values in these features\n",
        "        df_clean = df.dropna(subset=available_features)\n",
        "        print(f\"After removing missing values: {len(df_clean)}/{len(df)} households\")\n",
        "\n",
        "        if len(df_clean) == 0:\n",
        "            return []\n",
        "\n",
        "        # Get unique combinations\n",
        "        groups = []\n",
        "        unique_combinations = df_clean[available_features].drop_duplicates()\n",
        "        print(f\"Found {len(unique_combinations)} unique demographic combinations\")\n",
        "\n",
        "        for combo_idx, (idx, combo) in enumerate(unique_combinations.iterrows()):\n",
        "            mask = pd.Series(True, index=df.index)\n",
        "            combo_description = []\n",
        "\n",
        "            for feature in available_features:\n",
        "                mask = mask & (df[feature] == combo[feature])\n",
        "                combo_description.append(f\"{feature}={combo[feature]}\")\n",
        "\n",
        "            indices = df[mask].index.tolist()\n",
        "            combo_id = \"_\".join(combo_description)\n",
        "\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': combo_id,\n",
        "                    'indices': indices,\n",
        "                    'type': 'demographics'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} demographic groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_village_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups by village-level characteristics from TUP survey data.\"\"\"\n",
        "        print(f\"Creating village-based groups from survey indicators (min_size={min_size})\")\n",
        "\n",
        "        # Look for location-type indicators that can proxy for villages\n",
        "        location_patterns = ['district', 'rural', 'urban', 'capital', 'area', 'upazila', 'thana']\n",
        "        location_cols = []\n",
        "\n",
        "        for pattern in location_patterns:\n",
        "            matching_cols = [col for col in df.columns\n",
        "                           if pattern in col.lower() and 'bl_' in col\n",
        "                           and not any(x in col.lower() for x in ['gram flour', 'food', 'loan'])]\n",
        "            location_cols.extend(matching_cols)\n",
        "\n",
        "        location_cols = list(set(location_cols))  # Remove duplicates\n",
        "\n",
        "        if location_cols:\n",
        "            print(f\"Found location indicator columns: {location_cols[:3]}...\")  # Show first 3\n",
        "\n",
        "            # Use the first location column to create groups\n",
        "            location_col = location_cols[0]\n",
        "            print(f\"Using location indicator: {location_col}\")\n",
        "\n",
        "            # For binary indicator columns, group by the indicator value\n",
        "            groups = []\n",
        "            for location_value in df[location_col].unique():\n",
        "                if pd.isna(location_value):\n",
        "                    continue\n",
        "\n",
        "                indices = df[df[location_col] == location_value].index.tolist()\n",
        "                if len(indices) >= min_size:\n",
        "                    groups.append({\n",
        "                        'id': f'location_{location_col}_{location_value}',\n",
        "                        'indices': indices,\n",
        "                        'type': 'village'\n",
        "                    })\n",
        "        else:\n",
        "            print(\"No location indicators found\")\n",
        "\n",
        "        print(f\"Raw geographic groups created: {len(groups)}\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        print(f\"Balanced geographic groups after filtering: {len(balanced_groups)}\")\n",
        "\n",
        "        return balanced_groups\n",
        "        \"\"\"Create groups by geographic/village characteristics.\"\"\"\n",
        "        print(f\"Creating geographic groups\")\n",
        "\n",
        "        # Look for geographic identifiers\n",
        "        geo_patterns = ['village', 'block', 'district', 'area', 'region', 'location', 'gram']\n",
        "        geo_cols = []\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            matching_cols = [col for col in df.columns if pattern in col.lower()]\n",
        "            geo_cols.extend(matching_cols)\n",
        "\n",
        "        geo_cols = list(set(geo_cols))  # Remove duplicates\n",
        "\n",
        "        # Use the first available geographic variable\n",
        "        geo_col = geo_cols[0]\n",
        "        print(f\"Using geographic variable: {geo_col}\")\n",
        "\n",
        "        groups = []\n",
        "        for geo_id in df[geo_col].unique():\n",
        "            if pd.isna(geo_id):\n",
        "                continue\n",
        "\n",
        "            indices = df[df[geo_col] == geo_id].index.tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'geo_{geo_col}_{geo_id}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'geographic'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} geographic groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_causal_forest_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups using Random Forest to predict treatment effects.\"\"\"\n",
        "        print(f\"Creating causal forest groups (target: {n_groups})\")\n",
        "\n",
        "        # Exclude outcome, treatment, and consumption change columns\n",
        "        exclude_patterns = ['outcome', 'treatment', 'el1', 'el2', 'el3', 'el4', 'total_score']\n",
        "        feature_cols = [col for col in df.columns\n",
        "                       if not any(pattern in col for pattern in exclude_patterns)]\n",
        "\n",
        "        X = df[feature_cols].copy()\n",
        "\n",
        "        # Handle different data types properly\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
        "                try:\n",
        "                    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "                except:\n",
        "                    X[col] = 0  # Default for problematic columns\n",
        "            elif X[col].dtype == 'bool':\n",
        "                X[col] = X[col].astype(int)\n",
        "\n",
        "        # Fill missing values\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(0)\n",
        "\n",
        "        # Train separate models\n",
        "        treated_mask = df['treatment'] == 1\n",
        "        control_mask = df['treatment'] == 0\n",
        "\n",
        "        if treated_mask.sum() < 5 or control_mask.sum() < 5:\n",
        "            print(\"Not enough treated or control observations for causal forest\")\n",
        "            return []\n",
        "\n",
        "        rf_treated = RandomForestRegressor(n_estimators=100, random_state=self.random_seed)\n",
        "        rf_control = RandomForestRegressor(n_estimators=100, random_state=self.random_seed)\n",
        "\n",
        "        rf_treated.fit(X[treated_mask], df.loc[treated_mask, 'outcome'])\n",
        "        rf_control.fit(X[control_mask], df.loc[control_mask, 'outcome'])\n",
        "\n",
        "        # Predict CATE and cluster\n",
        "        pred_cate = rf_treated.predict(X) - rf_control.predict(X)\n",
        "        cluster_features = np.column_stack([X.values, pred_cate.reshape(-1, 1)])\n",
        "        cluster_features = StandardScaler().fit_transform(cluster_features)\n",
        "\n",
        "        labels = KMeans(n_clusters=n_groups, random_state=self.random_seed).fit_predict(cluster_features)\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[labels == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'causal_forest_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'causal_forest'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} causal forest groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_propensity_groups(self, df, n_groups=50, min_size=6):\n",
        "        \"\"\"Create groups based on propensity score strata.\"\"\"\n",
        "        print(f\"Creating propensity score groups (target: {n_groups})\")\n",
        "\n",
        "        # Exclude outcome and treatment columns\n",
        "        feature_cols = [col for col in df.columns\n",
        "                       if col not in ['treatment', 'outcome']]\n",
        "\n",
        "        X = df[feature_cols].copy()\n",
        "\n",
        "        # Handle different data types properly\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
        "                try:\n",
        "                    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "                except:\n",
        "                    X[col] = 0\n",
        "            elif X[col].dtype == 'bool':\n",
        "                X[col] = X[col].astype(int)\n",
        "\n",
        "        # Fill missing values\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(0)\n",
        "\n",
        "        # Get propensity scores\n",
        "        try:\n",
        "            prop_scores = cross_val_predict(\n",
        "                LogisticRegression(random_state=self.random_seed, max_iter=1000),\n",
        "                X, df['treatment'], method='predict_proba', cv=5\n",
        "            )[:, 1]\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing propensity scores: {e}\")\n",
        "            return []\n",
        "\n",
        "        # Create strata\n",
        "        quantiles = np.linspace(0, 1, n_groups + 1)\n",
        "        bins = np.digitize(prop_scores, np.quantile(prop_scores, quantiles)) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'propensity_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'propensity'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} propensity groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_asset_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on baseline asset ownership.\"\"\"\n",
        "        print(f\"Creating asset-based groups (target: {n_groups})\")\n",
        "\n",
        "        # Look for asset-related variables\n",
        "        asset_patterns = ['asset', 'livestock', 'land', 'house', 'own']\n",
        "        asset_cols = []\n",
        "\n",
        "        for pattern in asset_patterns:\n",
        "            matching_cols = [col for col in df.columns\n",
        "                           if pattern in col.lower() and 'bl' in col]  # Baseline assets\n",
        "            asset_cols.extend(matching_cols)\n",
        "\n",
        "        asset_cols = list(set(asset_cols))\n",
        "\n",
        "        if not asset_cols:\n",
        "            print(\"No asset variables found, using baseline consumption as proxy\")\n",
        "            if 'baseline_consumption' in df.columns:\n",
        "                asset_score = df['baseline_consumption']\n",
        "            else:\n",
        "                print(\"No suitable asset or consumption variables found\")\n",
        "                return []\n",
        "        else:\n",
        "            print(f\"Using asset variables: {asset_cols[:5]}\")  # Show first 5\n",
        "\n",
        "            # Create asset score from available variables - handle different data types\n",
        "            asset_df = df[asset_cols].copy()\n",
        "            processed_assets = []\n",
        "\n",
        "            for col in asset_cols:\n",
        "                col_data = asset_df[col]\n",
        "\n",
        "                # Handle different data types\n",
        "                if col_data.dtype == 'object' or col_data.dtype.name == 'category':\n",
        "                    # For categorical/string columns, convert to binary (has asset vs doesn't)\n",
        "                    # Assume non-null, non-empty, non-zero values indicate asset ownership\n",
        "                    try:\n",
        "                        # Try to convert to numeric first\n",
        "                        numeric_version = pd.to_numeric(col_data, errors='coerce')\n",
        "                        if not numeric_version.isna().all():\n",
        "                            processed_assets.append(numeric_version.fillna(0))\n",
        "                        else:\n",
        "                            # Convert categorical to binary\n",
        "                            binary_version = (~col_data.isna() &\n",
        "                                            (col_data != '') &\n",
        "                                            (col_data != '0') &\n",
        "                                            (col_data.astype(str).str.upper() != 'NO')).astype(int)\n",
        "                            processed_assets.append(binary_version)\n",
        "                    except:\n",
        "                        # Fallback: just check if not null\n",
        "                        binary_version = (~col_data.isna()).astype(int)\n",
        "                        processed_assets.append(binary_version)\n",
        "\n",
        "                elif col_data.dtype == 'bool':\n",
        "                    processed_assets.append(col_data.astype(int))\n",
        "\n",
        "                else:\n",
        "                    # Numeric columns - use as is, fill NaN with 0\n",
        "                    processed_assets.append(col_data.fillna(0))\n",
        "\n",
        "            if processed_assets:\n",
        "                # Create asset score as sum of processed asset indicators\n",
        "                asset_score = pd.concat(processed_assets, axis=1).sum(axis=1)\n",
        "            else:\n",
        "                print(\"Could not process any asset variables\")\n",
        "                return []\n",
        "\n",
        "        # Create asset-based groups\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(asset_score, percentiles)\n",
        "        bins = np.digitize(asset_score, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'asset_level_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'assets'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} asset groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_livelihood_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups based on baseline livelihood strategies.\"\"\"\n",
        "        print(f\"Creating livelihood strategy groups\")\n",
        "\n",
        "        # Look for occupation/livelihood variables\n",
        "        livelihood_patterns = ['occup', 'work', 'job', 'labor', 'employ', 'income_source']\n",
        "        livelihood_cols = []\n",
        "\n",
        "        for pattern in livelihood_patterns:\n",
        "            matching_cols = [col for col in df.columns\n",
        "                           if pattern in col.lower() and 'bl' in col]\n",
        "            livelihood_cols.extend(matching_cols)\n",
        "\n",
        "        livelihood_cols = list(set(livelihood_cols))\n",
        "\n",
        "        groups = []\n",
        "        for livelihood_type in df[livelihood_col].unique():\n",
        "            if pd.isna(livelihood_type):\n",
        "                continue\n",
        "\n",
        "            indices = df[df[livelihood_col] == livelihood_type].index.tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'livelihood_{livelihood_type}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'livelihood'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} livelihood groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def _ensure_balance_and_compute_cate(self, df, groups):\n",
        "        \"\"\"Ensure treatment balance and compute group CATE.\"\"\"\n",
        "        balanced_groups = []\n",
        "\n",
        "        for group in groups:\n",
        "            group_df = df.loc[group['indices']]\n",
        "\n",
        "            treatment_rate = group_df['treatment'].mean()\n",
        "            n_treated = group_df['treatment'].sum()\n",
        "            n_control = len(group_df) - n_treated\n",
        "\n",
        "            if not (0.15 <= treatment_rate <= 0.85 and n_treated >= 3 and n_control >= 3):\n",
        "                continue\n",
        "\n",
        "            treated_outcomes = group_df[group_df['treatment'] == 1]['outcome']\n",
        "            control_outcomes = group_df[group_df['treatment'] == 0]['outcome']\n",
        "            cate = treated_outcomes.mean() - control_outcomes.mean()\n",
        "\n",
        "            balanced_groups.append({\n",
        "                'id': group['id'],\n",
        "                'indices': group['indices'],\n",
        "                'size': len(group_df),\n",
        "                'treatment_rate': treatment_rate,\n",
        "                'n_treated': int(n_treated),\n",
        "                'n_control': int(n_control),\n",
        "                'cate': cate,\n",
        "                'type': group['type']\n",
        "            })\n",
        "\n",
        "        return balanced_groups\n",
        "\n",
        "    def normalize_cates(self, groups):\n",
        "        \"\"\"Normalize CATE values to [0,1].\"\"\"\n",
        "        cates = [g['cate'] for g in groups]\n",
        "        min_cate, max_cate = min(cates), max(cates)\n",
        "\n",
        "        if max_cate > min_cate:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = (group['cate'] - min_cate) / (max_cate - min_cate)\n",
        "        else:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = 0.5\n",
        "\n",
        "        print(f\"CATE normalization: [{min_cate:.3f}, {max_cate:.3f}] → [0, 1]\")\n",
        "        return groups\n",
        "\n",
        "    def plot_cate_distribution(self, groups, title_suffix=\"\"):\n",
        "        \"\"\"Plot CATE distribution.\"\"\"\n",
        "        original_cates = [g['cate'] for g in groups]\n",
        "        normalized_cates = [g['normalized_cate'] for g in groups]\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "        ax1.hist(original_cates, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        ax1.set_xlabel('Original CATE')\n",
        "        ax1.set_ylabel('Frequency')\n",
        "        ax1.set_title(f'Original CATE Distribution{title_suffix}')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        ax2.hist(normalized_cates, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "        ax2.set_xlabel('Normalized CATE (τ)')\n",
        "        ax2.set_ylabel('Frequency')\n",
        "        ax2.set_title(f'Normalized CATE Distribution{title_suffix}')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def estimate_tau(self, true_tau, accuracy):\n",
        "        \"\"\"Estimate tau using Hoeffding's inequality with Bernoulli samples.\"\"\"\n",
        "        sample_size = int(np.ceil(np.log(2/self.delta) / (2 * accuracy**2)))\n",
        "        samples = np.random.binomial(1, true_tau, sample_size)\n",
        "        return np.mean(samples), sample_size\n",
        "\n",
        "    def run_single_trial(self, groups, epsilon_val, trial_seed):\n",
        "        \"\"\"Run allocation algorithm for single trial with fixed gamma.\"\"\"\n",
        "        np.random.seed(self.random_seed + trial_seed)\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "        rho = self.gamma * np.sqrt(epsilon_val)  # Use fixed gamma\n",
        "\n",
        "        # Estimate all tau values using rho accuracy\n",
        "        tau_estimates_rho = []\n",
        "        for tau in tau_true:\n",
        "            estimate, _ = self.estimate_tau(tau, rho)\n",
        "            tau_estimates_rho.append(estimate)\n",
        "        tau_estimates_rho = np.array(tau_estimates_rho)\n",
        "\n",
        "        # Also estimate using epsilon accuracy for comparison\n",
        "        tau_estimates_eps = []\n",
        "        for tau in tau_true:\n",
        "            estimate, _ = self.estimate_tau(tau, epsilon_val)\n",
        "            tau_estimates_eps.append(estimate)\n",
        "        tau_estimates_eps = np.array(tau_estimates_eps)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for K in range(1, n_groups):\n",
        "            optimal_indices = np.argsort(tau_true)[-K:]\n",
        "            optimal_value = np.sum(tau_true[optimal_indices])\n",
        "\n",
        "            rho_indices = np.argsort(tau_estimates_rho)[-K:]\n",
        "            rho_value = np.sum(tau_true[rho_indices])\n",
        "\n",
        "            eps_indices = np.argsort(tau_estimates_eps)[-K:]\n",
        "            eps_value = np.sum(tau_true[eps_indices])\n",
        "\n",
        "            rho_ratio = rho_value / optimal_value if optimal_value > 0 else 0\n",
        "            eps_ratio = eps_value / optimal_value if optimal_value > 0 else 0\n",
        "            rho_success = rho_ratio >= (1 - epsilon_val)\n",
        "            eps_success = eps_ratio >= (1 - epsilon_val)\n",
        "\n",
        "            tau_k_est = tau_estimates_rho[rho_indices[0]]\n",
        "            a2_lower = tau_k_est\n",
        "            a2_upper = tau_k_est + 2 * rho\n",
        "            units_in_a2 = np.sum((tau_estimates_rho >= a2_lower) & (tau_estimates_rho <= a2_upper))\n",
        "            expected_a2 = 2 * rho * n_groups\n",
        "            # Updated heavy interval detection with 1.6x multiplier\n",
        "            is_heavy = units_in_a2 > self.heavy_multiplier * expected_a2\n",
        "\n",
        "            results.append({\n",
        "                'K': K,\n",
        "                'optimal_value': optimal_value,\n",
        "                'rho_value': rho_value,\n",
        "                'eps_value': eps_value,\n",
        "                'rho_ratio': rho_ratio,\n",
        "                'eps_ratio': eps_ratio,\n",
        "                'rho_success': rho_success,\n",
        "                'eps_success': eps_success,\n",
        "                'is_heavy': is_heavy,\n",
        "                'tau_k_est': tau_k_est,\n",
        "                'units_in_a2': units_in_a2\n",
        "            })\n",
        "\n",
        "        return results, tau_estimates_rho\n",
        "\n",
        "    def find_recovery_units(self, K, tau_true, tau_estimates, epsilon_val):\n",
        "        \"\"\"Find minimum units needed to achieve 1-epsilon performance.\"\"\"\n",
        "        n_groups = len(tau_true)\n",
        "\n",
        "        # Original allocation (using rho estimates)\n",
        "        rho_indices = np.argsort(tau_estimates)[-K:]\n",
        "        optimal_value = np.sum(tau_true[np.argsort(tau_true)[-K:]])\n",
        "\n",
        "        # Remaining candidates (sorted by estimate, best first)\n",
        "        remaining_indices = np.argsort(tau_estimates)[:-K][::-1]\n",
        "\n",
        "        # Test adding 1 to 10 additional units\n",
        "        for extra in range(1, 11):\n",
        "            if extra > len(remaining_indices):\n",
        "                break\n",
        "\n",
        "            expanded_indices = np.concatenate([rho_indices, remaining_indices[:extra]])\n",
        "            expanded_value = np.sum(tau_true[expanded_indices])\n",
        "\n",
        "            if expanded_value / optimal_value >= (1 - epsilon_val):\n",
        "                return extra\n",
        "\n",
        "        return None  # Need more than 10 units\n",
        "\n",
        "    def find_closest_working_budget(self, failed_K, trial_results):\n",
        "        \"\"\"Find closest budget that works for a failed budget.\"\"\"\n",
        "        working_budgets = [r['K'] for r in trial_results if r['rho_success']]\n",
        "\n",
        "        if not working_budgets:\n",
        "            return None, None\n",
        "\n",
        "        # Distance to any working budget (either direction)\n",
        "        distances_any = [abs(K - failed_K) for K in working_budgets]\n",
        "        min_distance_any = min(distances_any)\n",
        "\n",
        "        # Distance to smaller working budget (underspending)\n",
        "        smaller_working = [K for K in working_budgets if K < failed_K]\n",
        "        if smaller_working:\n",
        "            min_distance_smaller = failed_K - max(smaller_working)\n",
        "        else:\n",
        "            min_distance_smaller = None\n",
        "\n",
        "        return min_distance_any, min_distance_smaller\n",
        "\n",
        "    def analyze_method(self, groups, epsilon_val, n_trials=30):\n",
        "        \"\"\"Analyze single method with fixed gamma and updated heavy threshold.\"\"\"\n",
        "        print(f\"\\nAnalyzing {len(groups)} groups with ε={epsilon_val}, γ={self.gamma}\")\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "\n",
        "        trial_data = []\n",
        "\n",
        "        for trial in range(n_trials):\n",
        "            print(f\"Trial {trial + 1}/{n_trials}...\")\n",
        "\n",
        "            # Run single trial\n",
        "            trial_results, tau_estimates = self.run_single_trial(groups, epsilon_val, trial)\n",
        "\n",
        "            # Analyze failures\n",
        "            failed_results = [r for r in trial_results if not r['rho_success']]\n",
        "            failed_budgets = [r['K'] for r in failed_results]\n",
        "\n",
        "            # Check which failed budgets are heavy with true tau values\n",
        "            failed_heavy_estimated = []\n",
        "            failed_heavy_true = []\n",
        "            rho = self.gamma * np.sqrt(epsilon_val)\n",
        "\n",
        "            for failed_result in failed_results:\n",
        "                K = failed_result['K']\n",
        "                # Heavy with estimated values (already computed)\n",
        "                failed_heavy_estimated.append(failed_result['is_heavy'])\n",
        "\n",
        "                # Check heavy with true tau values\n",
        "                tau_k_true = tau_true[np.argsort(tau_true)[-K:]][0]  # True smallest in top-K\n",
        "                a2_lower_true = tau_k_true\n",
        "                a2_upper_true = tau_k_true + 2 * rho\n",
        "                units_in_a2_true = np.sum((tau_true >= a2_lower_true) & (tau_true <= a2_upper_true))\n",
        "                expected_a2_true = 2 * rho * n_groups\n",
        "                is_heavy_true = units_in_a2_true > self.heavy_multiplier * expected_a2_true\n",
        "                failed_heavy_true.append(is_heavy_true)\n",
        "\n",
        "            # Print trial summary\n",
        "            print(f\"  Failed budgets: {failed_budgets}\")\n",
        "\n",
        "            # Print heavy vectors\n",
        "            if len(failed_budgets) > 0:\n",
        "                estimated_clean = [bool(x) for x in failed_heavy_estimated]\n",
        "                true_clean = [bool(x) for x in failed_heavy_true]\n",
        "                print(f\"  HEAVY INTERVALS - Estimated: {estimated_clean}\")\n",
        "                print(f\"  HEAVY INTERVALS - True τ_K:   {true_clean}\")\n",
        "\n",
        "            # Count total heavy intervals and failed budgets in heavy intervals\n",
        "            total_heavy = sum(r['is_heavy'] for r in trial_results)\n",
        "            failed_heavy = sum(r['is_heavy'] for r in failed_results)\n",
        "\n",
        "            # Recovery analysis\n",
        "            recovery_units = []\n",
        "            distances_to_working_any = []\n",
        "            distances_to_working_smaller = []\n",
        "\n",
        "            for failed_result in failed_results:\n",
        "                K = failed_result['K']\n",
        "\n",
        "                # Find recovery units needed\n",
        "                recovery = self.find_recovery_units(K, tau_true, tau_estimates, epsilon_val)\n",
        "                if recovery is not None:\n",
        "                    recovery_units.append(recovery)\n",
        "\n",
        "                # Find distances to closest working budgets\n",
        "                distance_any, distance_smaller = self.find_closest_working_budget(K, trial_results)\n",
        "                if distance_any is not None:\n",
        "                    distances_to_working_any.append(distance_any)\n",
        "                if distance_smaller is not None:\n",
        "                    distances_to_working_smaller.append(distance_smaller)\n",
        "\n",
        "            trial_info = {\n",
        "                'trial': trial,\n",
        "                'failed_budgets': failed_budgets,\n",
        "                'num_failures': len(failed_results),\n",
        "                'total_heavy': total_heavy,\n",
        "                'failed_heavy': failed_heavy,\n",
        "                'failed_heavy_estimated': failed_heavy_estimated,\n",
        "                'failed_heavy_true': failed_heavy_true,\n",
        "                'recovery_units': recovery_units,\n",
        "                'distances_to_working_any': distances_to_working_any,\n",
        "                'distances_to_working_smaller': distances_to_working_smaller\n",
        "            }\n",
        "\n",
        "            trial_data.append(trial_info)\n",
        "\n",
        "            print(f\"  Failures: {len(failed_results)}, Total heavy: {total_heavy}, Failed heavy: {failed_heavy}\")\n",
        "            if recovery_units:\n",
        "                print(f\"  Recovery units: μ={np.mean(recovery_units):.1f}, med={np.median(recovery_units):.0f}, max={np.max(recovery_units)}\")\n",
        "            if distances_to_working_any:\n",
        "                print(f\"  Distance any: μ={np.mean(distances_to_working_any):.1f}, med={np.median(distances_to_working_any):.0f}, max={np.max(distances_to_working_any)}\")\n",
        "            if distances_to_working_smaller:\n",
        "                print(f\"  Distance smaller: μ={np.mean(distances_to_working_smaller):.1f}, med={np.median(distances_to_working_smaller):.0f}, max={np.max(distances_to_working_smaller)}\")\n",
        "            else:\n",
        "                print(f\"  Distance smaller: No smaller working budgets found\")\n",
        "\n",
        "        return trial_data\n",
        "\n",
        "    def print_method_summary(self, method_name, trial_data, n_groups, epsilon_val):\n",
        "        \"\"\"Print summary statistics for a method.\"\"\"\n",
        "        budget_10pct_threshold = max(1, int(0.1 * n_groups))\n",
        "\n",
        "        print(f\"\\n{'='*100}\")\n",
        "        print(f\"SUMMARY - {method_name} - ε={epsilon_val} - {n_groups} GROUPS\")\n",
        "        print(\"=\"*100)\n",
        "        print(f\"{'Fail μ':<7} {'Fail σ':<7} {'FailR% μ':<9} {'FailR% σ':<9} {'TotHvy':<8} {'FailHvy':<9} {'Rec μ':<7} {'Rec med':<8} {'Rec max':<8} {'DAny μ':<8} {'DAny σ':<10} {'DAny max':<10} {'DSmall μ':<10} {'DSmall σ':<12} {'DSmall max':<12}\")\n",
        "        print(\"-\"*120)\n",
        "\n",
        "        # Aggregate statistics across all trials - ALL BUDGETS\n",
        "        all_failures = [t['num_failures'] for t in trial_data]\n",
        "        all_total_heavy = [t['total_heavy'] for t in trial_data]\n",
        "        all_failed_heavy = [t['failed_heavy'] for t in trial_data]\n",
        "        all_recovery = []\n",
        "        all_distances_any = []\n",
        "        all_distances_smaller = []\n",
        "\n",
        "        for t in trial_data:\n",
        "            all_recovery.extend(t['recovery_units'])\n",
        "            all_distances_any.extend(t['distances_to_working_any'])\n",
        "            all_distances_smaller.extend(t['distances_to_working_smaller'])\n",
        "\n",
        "        avg_failures = np.mean(all_failures)\n",
        "        std_failures = np.std(all_failures)\n",
        "        avg_failure_rate = avg_failures / (n_groups - 1) * 100\n",
        "        std_failure_rate = std_failures / (n_groups - 1) * 100\n",
        "        avg_total_heavy = np.mean(all_total_heavy)\n",
        "        avg_failed_heavy = np.mean(all_failed_heavy)\n",
        "\n",
        "        # Recovery statistics\n",
        "        if all_recovery:\n",
        "            recovery_mean = np.mean(all_recovery)\n",
        "            recovery_med = np.median(all_recovery)\n",
        "            recovery_max = np.max(all_recovery)\n",
        "        else:\n",
        "            recovery_mean = recovery_med = recovery_max = np.nan\n",
        "\n",
        "        # Distance statistics - any direction\n",
        "        if all_distances_any:\n",
        "            distance_any_mean = np.mean(all_distances_any)\n",
        "            distance_any_std = np.std(all_distances_any)\n",
        "            distance_any_max = np.max(all_distances_any)\n",
        "        else:\n",
        "            distance_any_mean = distance_any_std = distance_any_max = np.nan\n",
        "\n",
        "        # Distance statistics - smaller only\n",
        "        if all_distances_smaller:\n",
        "            distance_smaller_mean = np.mean(all_distances_smaller)\n",
        "            distance_smaller_std = np.std(all_distances_smaller)\n",
        "            distance_smaller_max = np.max(all_distances_smaller)\n",
        "        else:\n",
        "            distance_smaller_mean = distance_smaller_std = distance_smaller_max = np.nan\n",
        "\n",
        "        print(f\"{avg_failures:<7.1f} {std_failures:<7.1f} {avg_failure_rate:<9.1f} {std_failure_rate:<9.1f} {avg_total_heavy:<8.1f} {avg_failed_heavy:<9.1f} \"\n",
        "              f\"{recovery_mean:<7.1f} {recovery_med:<8.0f} {recovery_max:<8.0f} \"\n",
        "              f\"{distance_any_mean:<8.1f} {distance_any_std:<10.1f} {distance_any_max:<10.0f} \"\n",
        "              f\"{distance_smaller_mean:<10.1f} {distance_smaller_std:<12.1f} {distance_smaller_max:<12.0f}\")\n",
        "\n",
        "        return {\n",
        "            'avg_failures': avg_failures,\n",
        "            'failure_rate_pct': avg_failure_rate,\n",
        "            'avg_recovery': recovery_mean,\n",
        "            'n_groups': n_groups\n",
        "        }\n",
        "\n",
        "\n",
        "def run_comprehensive_tup_analysis(df_tup, epsilon_values=None, n_trials=30, log_file=None, outcome_col=None):\n",
        "    \"\"\"Run comprehensive TUP analysis with all methods, fixed gamma=0.5, and 1.6x heavy threshold.\"\"\"\n",
        "\n",
        "    if epsilon_values is None:\n",
        "        epsilon_values = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2]\n",
        "\n",
        "    # Set up logging if requested\n",
        "    if log_file is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        log_file = f\"tup_comprehensive_analysis_gamma05_{timestamp}.txt\"\n",
        "\n",
        "    # Redirect output to both console and file\n",
        "    original_stdout = sys.stdout\n",
        "    tee = TeeOutput(log_file)\n",
        "    sys.stdout = tee\n",
        "\n",
        "    try:\n",
        "        print(\"COMPREHENSIVE TUP ANALYSIS - ALL METHODS, FIXED γ=0.5, HEAVY THRESHOLD=1.6x\")\n",
        "        print(f\"Log file: {log_file}\")\n",
        "        print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        # Define all TUP-specific grouping methods\n",
        "        methods = [\n",
        "            ('Village', lambda allocator, df: allocator.create_village_groups(df, min_size=6)),\n",
        "            ('Baseline Poverty', lambda allocator, df: allocator.create_baseline_poverty_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Demographics', lambda allocator, df: allocator.create_demographics_groups(df, min_size=6)),\n",
        "            ('Geographic', lambda allocator, df: allocator.create_geographic_groups(df, min_size=6)),\n",
        "            ('Assets', lambda allocator, df: allocator.create_asset_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Livelihood', lambda allocator, df: allocator.create_livelihood_groups(df, min_size=6)),\n",
        "            ('Causal Forest 30', lambda allocator, df: allocator.create_causal_forest_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Causal Forest 50', lambda allocator, df: allocator.create_causal_forest_groups(df, n_groups=50, min_size=6)),\n",
        "            ('Propensity Score', lambda allocator, df: allocator.create_propensity_groups(df, n_groups=50, min_size=6))\n",
        "        ]\n",
        "\n",
        "        all_results = {}\n",
        "\n",
        "        for method_name, method_func in methods:\n",
        "            print(f\"\\n{'='*120}\")\n",
        "            print(f\"ANALYZING TUP METHOD: {method_name}\")\n",
        "            print(\"=\"*120)\n",
        "\n",
        "            method_results = []\n",
        "\n",
        "            for eps in epsilon_values:\n",
        "                print(f\"\\n{'='*100}\")\n",
        "                print(f\"METHOD: {method_name} | EPSILON = {eps}\")\n",
        "                print(\"=\"*100)\n",
        "\n",
        "                # Initialize allocator with fixed gamma=0.5 and 1.6x heavy threshold\n",
        "                allocator = TUPCATEAllocator(epsilon=eps, gamma=0.5, heavy_multiplier=1.6)\n",
        "                df_processed = allocator.process_tup_data(df_tup, outcome_col=outcome_col)\n",
        "\n",
        "                try:\n",
        "                    # Create groups using this method\n",
        "                    groups = method_func(allocator, df_processed)\n",
        "\n",
        "                    if len(groups) < 3:\n",
        "                        print(f\"Too few groups ({len(groups)}) for {method_name} with ε = {eps} - skipping\")\n",
        "                        continue\n",
        "\n",
        "                    groups = allocator.normalize_cates(groups)\n",
        "\n",
        "                    # Show CATE distribution\n",
        "                    allocator.plot_cate_distribution(groups, f\" ({method_name}, ε={eps})\")\n",
        "\n",
        "                    # Run analysis for this epsilon and method\n",
        "                    trial_data = allocator.analyze_method(groups, eps, n_trials)\n",
        "\n",
        "                    # Print method summary\n",
        "                    stats = allocator.print_method_summary(method_name, trial_data, len(groups), eps)\n",
        "\n",
        "                    epsilon_result = {\n",
        "                        'method': method_name,\n",
        "                        'epsilon': eps,\n",
        "                        'sqrt_epsilon': np.sqrt(eps),\n",
        "                        'gamma': 0.5,\n",
        "                        'rho': 0.5 * np.sqrt(eps),\n",
        "                        'groups': groups,\n",
        "                        'trial_data': trial_data,\n",
        "                        'stats': stats\n",
        "                    }\n",
        "\n",
        "                    method_results.append(epsilon_result)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error with {method_name} at ε = {eps}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            all_results[method_name] = method_results\n",
        "\n",
        "            # Add method-specific summary table after all epsilons for this method\n",
        "            if method_results:\n",
        "                print(f\"\\n{'='*120}\")\n",
        "                print(f\"METHOD SUMMARY - {method_name} - ALL EPSILON VALUES\")\n",
        "                print(\"=\"*120)\n",
        "                print(f\"{'ε':<8} {'√ε':<10} {'γ':<6} {'ρ':<10} {'Groups':<8} {'Fail μ':<8} {'FailR%':<8} {'Rec μ':<8}\")\n",
        "                print(\"-\" * 80)\n",
        "\n",
        "                for eps_result in method_results:\n",
        "                    eps = eps_result['epsilon']\n",
        "                    sqrt_eps = eps_result['sqrt_epsilon']\n",
        "                    gamma = eps_result['gamma']\n",
        "                    rho = eps_result['rho']\n",
        "                    n_groups = len(eps_result['groups'])\n",
        "                    stats = eps_result['stats']\n",
        "\n",
        "                    print(f\"{eps:<8} {sqrt_eps:<10.6f} {gamma:<6} {rho:<10.6f} \"\n",
        "                          f\"{n_groups:<8} {stats['avg_failures']:<8.1f} {stats['failure_rate_pct']:<8.1f} \"\n",
        "                          f\"{stats['avg_recovery']:<8.1f}\")\n",
        "                print(\"=\"*120)\n",
        "\n",
        "        # Create comprehensive summary across all methods and epsilon values\n",
        "        print(f\"\\n{'='*200}\")\n",
        "        print(\"COMPREHENSIVE SUMMARY - ALL TUP METHODS AND EPSILON VALUES\")\n",
        "        print(\"=\"*200)\n",
        "\n",
        "        # Create summary table\n",
        "        summary_data = []\n",
        "\n",
        "        for method_name, method_results in all_results.items():\n",
        "            if not method_results:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n{'-'*100}\")\n",
        "            print(f\"TUP METHOD: {method_name}\")\n",
        "            print(\"-\"*100)\n",
        "\n",
        "            for eps_result in method_results:\n",
        "                eps = eps_result['epsilon']\n",
        "                sqrt_eps = eps_result['sqrt_epsilon']\n",
        "                gamma = eps_result['gamma']\n",
        "                rho = eps_result['rho']\n",
        "                n_groups = len(eps_result['groups'])\n",
        "                stats = eps_result['stats']\n",
        "\n",
        "                summary_data.append({\n",
        "                    'method': method_name,\n",
        "                    'epsilon': eps,\n",
        "                    'sqrt_eps': sqrt_eps,\n",
        "                    'gamma': gamma,\n",
        "                    'rho': rho,\n",
        "                    'avg_failures': stats['avg_failures'],\n",
        "                    'failure_rate_pct': stats['failure_rate_pct'],\n",
        "                    'avg_recovery': stats['avg_recovery'],\n",
        "                    'n_groups': stats['n_groups']\n",
        "                })\n",
        "\n",
        "            # Print method-specific table\n",
        "            method_data = [d for d in summary_data if d['method'] == method_name]\n",
        "            if method_data:\n",
        "                print(f\"{'ε':<8} {'√ε':<10} {'γ':<6} {'ρ':<10} {'Groups':<8} {'Fail μ':<8} {'FailR%':<8} {'Rec μ':<8}\")\n",
        "                print(\"-\" * 80)\n",
        "\n",
        "                for data in method_data:\n",
        "                    print(f\"{data['epsilon']:<8} {data['sqrt_eps']:<10.6f} {data['gamma']:<6} {data['rho']:<10.6f} \"\n",
        "                          f\"{data['n_groups']:<8} {data['avg_failures']:<8.1f} {data['failure_rate_pct']:<8.1f} \"\n",
        "                          f\"{data['avg_recovery']:<8.1f}\")\n",
        "\n",
        "        # Overall summary table\n",
        "        print(f\"\\n{'='*200}\")\n",
        "        print(\"OVERALL SUMMARY TABLE - ALL TUP METHODS COMBINED\")\n",
        "        print(\"=\"*200)\n",
        "        print(f\"{'Method':<18} {'ε':<8} {'√ε':<10} {'γ':<6} {'ρ':<10} {'Groups':<8} {'Fail μ':<8} {'FailR%':<8} {'Rec μ':<8}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        for data in summary_data:\n",
        "            print(f\"{data['method']:<18} {data['epsilon']:<8} {data['sqrt_eps']:<10.6f} {data['gamma']:<6} {data['rho']:<10.6f} \"\n",
        "                  f\"{data['n_groups']:<8} {data['avg_failures']:<8.1f} {data['failure_rate_pct']:<8.1f} \"\n",
        "                  f\"{data['avg_recovery']:<8.1f}\")\n",
        "\n",
        "        # TUP-specific analysis insights\n",
        "        print(f\"\\n{'='*100}\")\n",
        "        print(\"KEY INSIGHTS FOR TUP DATASET\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        # Find best and worst performing methods for TUP\n",
        "        if summary_data:\n",
        "            # Average performance across all epsilon values per method\n",
        "            method_performance = {}\n",
        "            for method_name in all_results.keys():\n",
        "                method_data = [d for d in summary_data if d['method'] == method_name]\n",
        "                if method_data:\n",
        "                    avg_failure_rate = np.mean([d['failure_rate_pct'] for d in method_data])\n",
        "                    method_performance[method_name] = avg_failure_rate\n",
        "\n",
        "            if method_performance:\n",
        "                best_method = min(method_performance, key=method_performance.get)\n",
        "                worst_method = max(method_performance, key=method_performance.get)\n",
        "\n",
        "                print(f\"BEST PERFORMING TUP METHOD: {best_method}\")\n",
        "                print(f\"  Average failure rate: {method_performance[best_method]:.1f}%\")\n",
        "\n",
        "                print(f\"\\nWORST PERFORMING TUP METHOD: {worst_method}\")\n",
        "                print(f\"  Average failure rate: {method_performance[worst_method]:.1f}%\")\n",
        "\n",
        "                print(f\"\\nTUP METHOD RANKING (by average failure rate):\")\n",
        "                sorted_methods = sorted(method_performance.items(), key=lambda x: x[1])\n",
        "                for i, (method, rate) in enumerate(sorted_methods, 1):\n",
        "                    print(f\"  {i}. {method}: {rate:.1f}%\")\n",
        "\n",
        "        # Effect of epsilon on TUP\n",
        "        print(f\"\\nEFFECT OF EPSILON ON TUP:\")\n",
        "        epsilon_performance = {}\n",
        "        for eps in epsilon_values:\n",
        "            eps_data = [d for d in summary_data if d['epsilon'] == eps]\n",
        "            if eps_data:\n",
        "                avg_failure_rate = np.mean([d['failure_rate_pct'] for d in eps_data])\n",
        "                epsilon_performance[eps] = avg_failure_rate\n",
        "\n",
        "        if epsilon_performance:\n",
        "            print(f\"{'Epsilon':<10} {'Avg Failure Rate':<15} {'ρ = 0.5√ε':<12}\")\n",
        "            print(\"-\" * 40)\n",
        "            for eps in sorted(epsilon_performance.keys()):\n",
        "                rho = 0.5 * np.sqrt(eps)\n",
        "                print(f\"{eps:<10} {epsilon_performance[eps]:<15.1f} {rho:<12.6f}\")\n",
        "\n",
        "        return all_results, summary_data\n",
        "\n",
        "    finally:\n",
        "        # Restore original stdout and close log file\n",
        "        sys.stdout = original_stdout\n",
        "        tee.close()\n",
        "        print(f\"Comprehensive TUP analysis completed. Results saved to: {log_file}\")\n",
        "\n",
        "\n",
        "# Example usage for TUP dataset\n",
        "if __name__ == \"__main__\":\n",
        "    # Load TUP dataset (using the same preprocessing as in the original code)\n",
        "    import pandas as pd\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "    def preprocess_tup_data(filepath):\n",
        "        \"\"\"Preprocess TUP data using the same approach as the original code.\"\"\"\n",
        "        # Load the data\n",
        "        df1 = pd.read_stata(filepath)\n",
        "        print(f\"Loaded TUP data: {df1.shape}\")\n",
        "\n",
        "        # Follow the original preprocessing steps\n",
        "        columns_to_drop_original = df1.columns[df1.columns.str.endswith('el1') |\n",
        "                                      df1.columns.str.endswith('el2') |\n",
        "                                      df1.columns.str.endswith('el3') |\n",
        "                                      df1.columns.str.endswith('el4') | df1.columns.str.startswith('el')]\n",
        "\n",
        "        categorical_columns = df1.select_dtypes(include=['object']).columns\n",
        "        unique_value_counts = df1[categorical_columns].nunique()\n",
        "\n",
        "        leave_categories = []\n",
        "        keep_categories = []\n",
        "        for i in categorical_columns:\n",
        "            if unique_value_counts[i] > 2 * unique_value_counts.mean():\n",
        "                leave_categories.append(i)\n",
        "            else:\n",
        "                keep_categories.append(i)\n",
        "\n",
        "        # Keep only relevant columns\n",
        "        object_columns = df1.select_dtypes(include=['object'])\n",
        "        cols_keep = []\n",
        "        for col in df1.columns:\n",
        "            if col not in object_columns and col not in leave_categories and col not in columns_to_drop_original:\n",
        "                cols_keep.append(col)\n",
        "\n",
        "        df1_filtered = df1[cols_keep]\n",
        "        df1_encoded = pd.get_dummies(df1_filtered)\n",
        "\n",
        "        # Create target variable\n",
        "        target_col_consumption = df1['pc_exp_month_el3'] - df1['pc_exp_month_bl']\n",
        "        df1_encoded['target_column_consumption'] = target_col_consumption\n",
        "\n",
        "        # Clean data\n",
        "        df1_encoded = df1_encoded[df1_encoded['target_column_consumption'].notna()]\n",
        "        df1_encoded = df1_encoded.dropna(axis=1, how='all')\n",
        "        df1_encoded = df1_encoded.fillna(df1_encoded.mean())\n",
        "\n",
        "        # Feature selection using Random Forest\n",
        "        X = df1_encoded.drop(columns='target_column_consumption')\n",
        "        y = df1_encoded['target_column_consumption']\n",
        "\n",
        "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        rf.fit(X, y)\n",
        "\n",
        "        feature_importances = rf.feature_importances_\n",
        "        importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "        top_1000_features = importance_df.sort_values(by='Importance', ascending=False).head(1000)['Feature'].tolist()\n",
        "\n",
        "        # Ensure treatment and baseline consumption are included\n",
        "        if 'treatment' not in top_1000_features:\n",
        "            top_1000_features.append('treatment')\n",
        "        if 'pc_exp_month_bl' not in top_1000_features:\n",
        "            top_1000_features.append('pc_exp_month_bl')\n",
        "\n",
        "        df_final = df1_encoded[top_1000_features].copy()\n",
        "\n",
        "        df_final['target_column_consumption'] = df1_encoded['target_column_consumption']\n",
        "        df_final['outcome'] = df1_encoded['target_column_consumption']  # Also create 'outcome' column\n",
        "\n",
        "        return df_final\n",
        "\n",
        "    epsilon_values = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2]\n",
        "\n",
        "    results, summary = run_comprehensive_tup_analysis(\n",
        "        df_tup,\n",
        "        epsilon_values=epsilon_values,\n",
        "        n_trials=30,\n",
        "        log_file=\"tup_comprehensive_analysis_gamma05_heavy16.txt\"\n",
        "    )"
      ]
    }
  ]
}
