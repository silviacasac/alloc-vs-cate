{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGJoKkKIWvLm"
      },
      "outputs": [],
      "source": [
        "# NSW CATE ANALYSIS - SAMPLE SIZE ANALYSIS WITH THEORETICAL BOUNDS\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import warnings\n",
        "import sys\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def theoretical_bound(m, beta, N, delta, OPT):\n",
        "    \"\"\"Compute theoretical bound (1 - (N*ln(2N/delta)/m)^beta) * OPT\"\"\"\n",
        "    term = N * np.log(2 * N / delta) / m\n",
        "    if term >= 1:\n",
        "        return 0  # Bound becomes meaningless\n",
        "    return (1 - term**beta) * OPT\n",
        "\n",
        "class TeeOutput:\n",
        "    \"\"\"Class to write output to both console and file simultaneously.\"\"\"\n",
        "    def __init__(self, filename):\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(filename, 'w')\n",
        "\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "        self.log.flush()\n",
        "\n",
        "    def flush(self):\n",
        "        self.terminal.flush()\n",
        "        self.log.flush()\n",
        "\n",
        "    def close(self):\n",
        "        self.log.close()\n",
        "\n",
        "class NSWSampleSizeAnalyzer:\n",
        "    \"\"\"NSW CATE allocation with sample size analysis and theoretical bounds.\"\"\"\n",
        "\n",
        "    def __init__(self, random_seed=42):\n",
        "        self.random_seed = random_seed\n",
        "        np.random.seed(random_seed)\n",
        "        print(f\"NSW Sample Size Analyzer initialized with seed {random_seed}\")\n",
        "\n",
        "    def process_nsw_data(self, df, outcome_col='re78', treatment_col='treat'):\n",
        "        \"\"\"Process NSW dataset for analysis.\"\"\"\n",
        "        print(f\"Processing NSW data with {len(df)} observations\")\n",
        "        print(f\"Available columns: {list(df.columns)}\")\n",
        "\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Check for required columns\n",
        "        if treatment_col not in df_processed.columns:\n",
        "            raise ValueError(f\"Missing required treatment column: {treatment_col}\")\n",
        "        if outcome_col not in df_processed.columns:\n",
        "            raise ValueError(f\"Missing required outcome column: {outcome_col}\")\n",
        "\n",
        "        # Set up treatment and outcome\n",
        "        df_processed['treatment'] = df_processed[treatment_col]\n",
        "        df_processed['outcome'] = df_processed[outcome_col]\n",
        "\n",
        "        # Set up baseline earnings\n",
        "        if 're75' in df_processed.columns:\n",
        "            df_processed['baseline_earnings'] = df_processed['re75']\n",
        "        else:\n",
        "            df_processed['baseline_earnings'] = 0  # Default if no baseline\n",
        "\n",
        "        # Clean data\n",
        "        initial_size = len(df_processed)\n",
        "        df_processed = df_processed.dropna(subset=['outcome', 'treatment'])\n",
        "        final_size = len(df_processed)\n",
        "\n",
        "        if initial_size != final_size:\n",
        "            print(f\"Dropped {initial_size - final_size} rows due to missing outcome/treatment\")\n",
        "\n",
        "        print(f\"Final dataset: {final_size} individuals\")\n",
        "        print(f\"Treatment distribution: {df_processed['treatment'].value_counts().to_dict()}\")\n",
        "        print(f\"Outcome (1978 earnings) statistics: mean=${df_processed['outcome'].mean():.0f}, std=${df_processed['outcome'].std():.0f}\")\n",
        "\n",
        "        if 'baseline_earnings' in df_processed.columns:\n",
        "            print(f\"Baseline (1975 earnings) stats: mean=${df_processed['baseline_earnings'].mean():.0f}, std=${df_processed['baseline_earnings'].std():.0f}\")\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def create_demographics_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups by key demographic characteristics in NSW data.\"\"\"\n",
        "        print(f\"Creating NSW demographics groups\")\n",
        "\n",
        "        # Key NSW demographic variables\n",
        "        demo_features = ['black', 'hispanic', 'married', 'nodegree']\n",
        "\n",
        "        # Check which features are available\n",
        "        available_features = [col for col in demo_features if col in df.columns]\n",
        "\n",
        "        if not available_features:\n",
        "            print(\"No demographic variables found\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Using demographic features: {available_features}\")\n",
        "\n",
        "        # Limit to top 3 features to avoid too many combinations\n",
        "        if len(available_features) > 3:\n",
        "            available_features = available_features[:3]\n",
        "\n",
        "        # Remove rows with missing values in these features\n",
        "        df_clean = df.dropna(subset=available_features)\n",
        "        print(f\"After removing missing values: {len(df_clean)}/{len(df)} individuals\")\n",
        "\n",
        "        if len(df_clean) == 0:\n",
        "            return []\n",
        "\n",
        "        # Get unique combinations\n",
        "        groups = []\n",
        "        unique_combinations = df_clean[available_features].drop_duplicates()\n",
        "        print(f\"Found {len(unique_combinations)} unique demographic combinations\")\n",
        "\n",
        "        for combo_idx, (idx, combo) in enumerate(unique_combinations.iterrows()):\n",
        "            mask = pd.Series(True, index=df.index)\n",
        "            combo_description = []\n",
        "\n",
        "            for feature in available_features:\n",
        "                mask = mask & (df[feature] == combo[feature])\n",
        "                combo_description.append(f\"{feature}={combo[feature]}\")\n",
        "\n",
        "            indices = df[mask].index.tolist()\n",
        "            combo_id = \"_\".join(combo_description)\n",
        "\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': combo_id,\n",
        "                    'indices': indices,\n",
        "                    'type': 'demographics'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} demographic groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_age_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on age brackets.\"\"\"\n",
        "        print(f\"Creating age groups (target: {n_groups})\")\n",
        "\n",
        "        if 'age' not in df.columns:\n",
        "            print(\"No age variable found\")\n",
        "            return []\n",
        "\n",
        "        # Create age-based groups\n",
        "        age = df['age'].fillna(df['age'].median())\n",
        "\n",
        "        # Create age brackets\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(age, percentiles)\n",
        "        bins = np.digitize(age, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'age_group_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'age'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} age groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_education_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups based on education levels.\"\"\"\n",
        "        print(f\"Creating education groups\")\n",
        "\n",
        "        if 'education' not in df.columns:\n",
        "            print(\"No education variable found\")\n",
        "            return []\n",
        "\n",
        "        groups = []\n",
        "        for education_level in df['education'].unique():\n",
        "            if pd.isna(education_level):\n",
        "                continue\n",
        "\n",
        "            indices = df[df['education'] == education_level].index.tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'education_{education_level}_years',\n",
        "                    'indices': indices,\n",
        "                    'type': 'education'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} education groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_baseline_earnings_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on 1975 baseline earnings.\"\"\"\n",
        "        print(f\"Creating baseline earnings groups (target: {n_groups})\")\n",
        "\n",
        "        if 'baseline_earnings' not in df.columns or df['baseline_earnings'].sum() == 0:\n",
        "            print(\"No baseline earnings data available\")\n",
        "            return []\n",
        "\n",
        "        # Create earnings-based groups\n",
        "        earnings = df['baseline_earnings'].fillna(0)  # Fill NaN with 0 for unemployed\n",
        "\n",
        "        # Create earnings brackets including zero earners\n",
        "        if (earnings == 0).mean() > 0.3:  # If >30% have zero earnings, create separate zero group\n",
        "            # Create one group for zero earners\n",
        "            zero_earners = df.index[earnings == 0].tolist()\n",
        "            groups = []\n",
        "            if len(zero_earners) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': 'zero_earnings_1975',\n",
        "                    'indices': zero_earners,\n",
        "                    'type': 'baseline_earnings'\n",
        "                })\n",
        "\n",
        "            # Create groups for positive earners\n",
        "            positive_earnings = earnings[earnings > 0]\n",
        "            if len(positive_earnings) > 0:\n",
        "                percentiles = np.linspace(0, 100, n_groups)\n",
        "                cuts = np.percentile(positive_earnings, percentiles)\n",
        "\n",
        "                for i in range(len(cuts) - 1):\n",
        "                    mask = (earnings > cuts[i]) & (earnings <= cuts[i + 1])\n",
        "                    indices = df.index[mask].tolist()\n",
        "                    if len(indices) >= min_size:\n",
        "                        groups.append({\n",
        "                            'id': f'earnings_1975_bracket_{i}',\n",
        "                            'indices': indices,\n",
        "                            'type': 'baseline_earnings'\n",
        "                        })\n",
        "        else:\n",
        "            # Standard percentile groups\n",
        "            percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "            cuts = np.percentile(earnings, percentiles)\n",
        "            bins = np.digitize(earnings, cuts) - 1\n",
        "\n",
        "            groups = []\n",
        "            for i in range(n_groups):\n",
        "                indices = df.index[bins == i].tolist()\n",
        "                if len(indices) >= min_size:\n",
        "                    groups.append({\n",
        "                        'id': f'earnings_1975_bracket_{i}',\n",
        "                        'indices': indices,\n",
        "                        'type': 'baseline_earnings'\n",
        "                    })\n",
        "\n",
        "        print(f\"Created {len(groups)} baseline earnings groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_race_ethnicity_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups based on race/ethnicity combinations.\"\"\"\n",
        "        print(f\"Creating race/ethnicity groups\")\n",
        "\n",
        "        # Create race/ethnicity categories\n",
        "        def get_race_ethnicity(row):\n",
        "            if row.get('black', 0) == 1:\n",
        "                return 'black'\n",
        "            elif row.get('hispanic', 0) == 1:\n",
        "                return 'hispanic'\n",
        "            else:\n",
        "                return 'white_other'\n",
        "\n",
        "        if 'black' in df.columns or 'hispanic' in df.columns:\n",
        "            df['race_ethnicity'] = df.apply(get_race_ethnicity, axis=1)\n",
        "\n",
        "            groups = []\n",
        "            for race_eth in df['race_ethnicity'].unique():\n",
        "                indices = df[df['race_ethnicity'] == race_eth].index.tolist()\n",
        "                if len(indices) >= min_size:\n",
        "                    groups.append({\n",
        "                        'id': f'race_{race_eth}',\n",
        "                        'indices': indices,\n",
        "                        'type': 'race_ethnicity'\n",
        "                    })\n",
        "\n",
        "            print(f\"Created {len(groups)} race/ethnicity groups\")\n",
        "            balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "            return balanced_groups\n",
        "        else:\n",
        "            print(\"No race/ethnicity variables found\")\n",
        "            return []\n",
        "\n",
        "    def create_employment_status_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups based on 1975 employment status.\"\"\"\n",
        "        print(f\"Creating employment status groups\")\n",
        "\n",
        "        if 'baseline_earnings' not in df.columns:\n",
        "            print(\"No baseline earnings data for employment status\")\n",
        "            return []\n",
        "\n",
        "        # Define employment status based on 1975 earnings\n",
        "        def get_employment_status(earnings):\n",
        "            if pd.isna(earnings) or earnings == 0:\n",
        "                return 'unemployed_1975'\n",
        "            elif earnings < 5000:  # Low earnings threshold for 1975\n",
        "                return 'low_earnings_1975'\n",
        "            else:\n",
        "                return 'higher_earnings_1975'\n",
        "\n",
        "        df['employment_status'] = df['baseline_earnings'].apply(get_employment_status)\n",
        "\n",
        "        groups = []\n",
        "        for status in df['employment_status'].unique():\n",
        "            indices = df[df['employment_status'] == status].index.tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'employment_{status}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'employment_status'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} employment status groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_causal_forest_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups using Random Forest to predict treatment effects.\"\"\"\n",
        "        print(f\"Creating causal forest groups (target: {n_groups})\")\n",
        "\n",
        "        # Use NSW covariates\n",
        "        feature_cols = ['age', 'education', 'black', 'hispanic', 'married', 'nodegree', 're75']\n",
        "        available_features = [col for col in feature_cols if col in df.columns]\n",
        "\n",
        "        if not available_features:\n",
        "            print(\"No features available for causal forest\")\n",
        "            return []\n",
        "\n",
        "        X = df[available_features].copy()\n",
        "\n",
        "        # Handle missing values\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 0)\n",
        "\n",
        "        # Train separate models\n",
        "        treated_mask = df['treatment'] == 1\n",
        "        control_mask = df['treatment'] == 0\n",
        "\n",
        "        if treated_mask.sum() < 5 or control_mask.sum() < 5:\n",
        "            print(\"Not enough treated or control observations for causal forest\")\n",
        "            return []\n",
        "\n",
        "        rf_treated = RandomForestRegressor(n_estimators=100, random_state=self.random_seed)\n",
        "        rf_control = RandomForestRegressor(n_estimators=100, random_state=self.random_seed)\n",
        "\n",
        "        rf_treated.fit(X[treated_mask], df.loc[treated_mask, 'outcome'])\n",
        "        rf_control.fit(X[control_mask], df.loc[control_mask, 'outcome'])\n",
        "\n",
        "        # Predict CATE and cluster\n",
        "        pred_cate = rf_treated.predict(X) - rf_control.predict(X)\n",
        "        cluster_features = np.column_stack([X.values, pred_cate.reshape(-1, 1)])\n",
        "        cluster_features = StandardScaler().fit_transform(cluster_features)\n",
        "\n",
        "        labels = KMeans(n_clusters=n_groups, random_state=self.random_seed).fit_predict(cluster_features)\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[labels == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'causal_forest_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'causal_forest'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} causal forest groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_propensity_groups(self, df, n_groups=50, min_size=6):\n",
        "        \"\"\"Create groups based on propensity score strata.\"\"\"\n",
        "        print(f\"Creating propensity score groups (target: {n_groups})\")\n",
        "\n",
        "        # Use NSW covariates\n",
        "        feature_cols = ['age', 'education', 'black', 'hispanic', 'married', 'nodegree', 're75']\n",
        "        available_features = [col for col in feature_cols if col in df.columns]\n",
        "\n",
        "        if not available_features:\n",
        "            print(\"No features available for propensity scoring\")\n",
        "            return []\n",
        "\n",
        "        X = df[available_features].copy()\n",
        "\n",
        "        # Handle missing values\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 0)\n",
        "\n",
        "        # Get propensity scores\n",
        "        try:\n",
        "            prop_scores = cross_val_predict(\n",
        "                LogisticRegression(random_state=self.random_seed, max_iter=1000),\n",
        "                X, df['treatment'], method='predict_proba', cv=5\n",
        "            )[:, 1]\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing propensity scores: {e}\")\n",
        "            return []\n",
        "\n",
        "        # Create strata\n",
        "        quantiles = np.linspace(0, 1, n_groups + 1)\n",
        "        bins = np.digitize(prop_scores, np.quantile(prop_scores, quantiles)) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'propensity_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'propensity'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} propensity groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def _ensure_balance_and_compute_cate(self, df, groups):\n",
        "        \"\"\"Ensure treatment balance and compute group CATE.\"\"\"\n",
        "        balanced_groups = []\n",
        "\n",
        "        for group in groups:\n",
        "            group_df = df.loc[group['indices']]\n",
        "\n",
        "            treatment_rate = group_df['treatment'].mean()\n",
        "            n_treated = group_df['treatment'].sum()\n",
        "            n_control = len(group_df) - n_treated\n",
        "\n",
        "            if not (0.15 <= treatment_rate <= 0.85 and n_treated >= 3 and n_control >= 3):\n",
        "                continue\n",
        "\n",
        "            treated_outcomes = group_df[group_df['treatment'] == 1]['outcome']\n",
        "            control_outcomes = group_df[group_df['treatment'] == 0]['outcome']\n",
        "            cate = treated_outcomes.mean() - control_outcomes.mean()\n",
        "\n",
        "            balanced_groups.append({\n",
        "                'id': group['id'],\n",
        "                'indices': group['indices'],\n",
        "                'size': len(group_df),\n",
        "                'treatment_rate': treatment_rate,\n",
        "                'n_treated': int(n_treated),\n",
        "                'n_control': int(n_control),\n",
        "                'cate': cate,\n",
        "                'type': group['type']\n",
        "            })\n",
        "\n",
        "        return balanced_groups\n",
        "\n",
        "    def normalize_cates(self, groups):\n",
        "        \"\"\"Normalize CATE values to [0,1].\"\"\"\n",
        "        cates = [g['cate'] for g in groups]\n",
        "        min_cate, max_cate = min(cates), max(cates)\n",
        "\n",
        "        if max_cate > min_cate:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = (group['cate'] - min_cate) / (max_cate - min_cate)\n",
        "        else:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = 0.5\n",
        "\n",
        "        print(f\"CATE normalization: [{min_cate:.0f}, {max_cate:.0f}] → [0, 1]\")\n",
        "        return groups\n",
        "\n",
        "    def simulate_sampling_trial(self, groups, sample_size, trial_seed):\n",
        "        \"\"\"Simulate sampling trial\"\"\"\n",
        "        np.random.seed(self.random_seed + trial_seed)\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "\n",
        "        # Initialize tau estimates\n",
        "        tau_estimates = np.zeros(n_groups)\n",
        "        sample_counts = np.zeros(n_groups)\n",
        "\n",
        "        # Perform sampling: choose group uniformly, sample Bernoulli(tau(u))\n",
        "        for _ in range(sample_size):\n",
        "            group_idx = np.random.randint(n_groups)\n",
        "            sample = np.random.binomial(1, tau_true[group_idx])\n",
        "\n",
        "            sample_counts[group_idx] += 1\n",
        "            if sample_counts[group_idx] == 1:\n",
        "                tau_estimates[group_idx] = sample\n",
        "            else:\n",
        "                tau_estimates[group_idx] = ((sample_counts[group_idx] - 1) * tau_estimates[group_idx] + sample) / sample_counts[group_idx]\n",
        "\n",
        "        # Groups with no samples get estimate 0\n",
        "        tau_estimates[sample_counts == 0] = 0\n",
        "\n",
        "        return tau_estimates, sample_counts\n",
        "\n",
        "    def analyze_sample_size_performance(self, groups, sample_sizes, budget_percentages, n_trials=50):\n",
        "        \"\"\"Analyze performance vs sample size.\"\"\"\n",
        "        print(f\"Analyzing sample size performance with {len(groups)} groups\")\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "\n",
        "        # Calculate budgets\n",
        "        budgets = [max(1, int(p * n_groups)) for p in budget_percentages]\n",
        "        print(f\"Budget percentages {budget_percentages} → K values {budgets}\")\n",
        "\n",
        "        # Calculate optimal values\n",
        "        optimal_values = {}\n",
        "        for i, K in enumerate(budgets):\n",
        "            optimal_indices = np.argsort(tau_true)[-K:]\n",
        "            optimal_values[budget_percentages[i]] = np.sum(tau_true[optimal_indices])\n",
        "\n",
        "        # Run trials\n",
        "        results = {bp: {'sample_sizes': [], 'values': [], 'stds': []} for bp in budget_percentages}\n",
        "\n",
        "        for sample_size in sample_sizes:\n",
        "            print(f\"  Sample size {sample_size}...\")\n",
        "\n",
        "            budget_trial_values = {bp: [] for bp in budget_percentages}\n",
        "\n",
        "            for trial in range(n_trials):\n",
        "                tau_estimates, sample_counts = self.simulate_sampling_trial(groups, sample_size, trial)\n",
        "\n",
        "                for i, K in enumerate(budgets):\n",
        "                    bp = budget_percentages[i]\n",
        "\n",
        "                    # Select top K based on estimates\n",
        "                    selected_indices = np.argsort(tau_estimates)[-K:]\n",
        "\n",
        "                    # Compute realized value with true tau\n",
        "                    realized_value = np.sum(tau_true[selected_indices])\n",
        "                    budget_trial_values[bp].append(realized_value)\n",
        "\n",
        "            # Store results\n",
        "            for bp in budget_percentages:\n",
        "                results[bp]['sample_sizes'].append(sample_size)\n",
        "                results[bp]['values'].append(np.mean(budget_trial_values[bp]))\n",
        "                results[bp]['stds'].append(np.std(budget_trial_values[bp]))\n",
        "\n",
        "        return results, optimal_values\n",
        "\n",
        "    def plot_sample_size_analysis(self, results, optimal_values, method_name, budget_percentages, n_groups):\n",
        "        \"\"\"Create 6 plots (one per budget) for sample size analysis with theoretical bounds - ENHANCED STYLING.\"\"\"\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        # Calculate parameters for theoretical bounds\n",
        "        delta = 0.05\n",
        "\n",
        "        print(f\"\\nPlotting {method_name} (M={n_groups})\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for i, bp in enumerate([0.2, 0.3, 0.5, 0.7]):\n",
        "            ax = axes[i]\n",
        "\n",
        "            # Get data for this budget\n",
        "            sample_sizes = results[bp]['sample_sizes']\n",
        "            values = results[bp]['values']\n",
        "            stds = results[bp]['stds']\n",
        "            optimal_val = optimal_values[bp]\n",
        "\n",
        "            # Normalize all values by optimal value\n",
        "            values_norm = np.array(values) / optimal_val\n",
        "            stds_norm = np.array(stds) / optimal_val\n",
        "\n",
        "            # Plot empirical performance curve\n",
        "            ax.errorbar(sample_sizes, values_norm, yerr=stds_norm,\n",
        "                      marker='o', capsize=5, capthick=3, linewidth=6, markersize=8,\n",
        "                      label='Empirical data', color='blue', alpha=0.8)\n",
        "\n",
        "            # Plot optimal value (normalized to 1)\n",
        "            ax.axhline(y=1.0, color='black', linestyle=':', linewidth=2,\n",
        "                      label='Optimal (1.0)', alpha=0.8)\n",
        "\n",
        "            m_smooth = np.linspace(min(sample_sizes), max(sample_sizes), 200)\n",
        "\n",
        "            # Plot reference curves\n",
        "            ref_curve_05 = [theoretical_bound(m, 0.5, n_groups, delta, optimal_val) / optimal_val for m in m_smooth]\n",
        "            ref_curve_10 = [theoretical_bound(m, 1.0, n_groups, delta, optimal_val) / optimal_val for m in m_smooth]\n",
        "\n",
        "            ax.plot(m_smooth, ref_curve_05, 'red', linestyle=(0, (3, 2)), linewidth=6,\n",
        "                  label='FullCATE', alpha=0.8)\n",
        "            ax.plot(m_smooth, ref_curve_10, 'green', linestyle=(0, (3, 1, 1, 1)), linewidth=6,\n",
        "                  label='ALLOC', alpha=0.8)\n",
        "\n",
        "            # Set labels\n",
        "            ax.set_xlabel('Sample size', fontsize=23)\n",
        "            ax.set_ylabel('Normalized allocation value', fontsize=23)\n",
        "            ax.set_title(f'Budget = {bp*100:.0f}% (K={max(1, int(bp * n_groups))})', fontsize=24, fontweight='bold')\n",
        "\n",
        "            ax.legend(fontsize=21, framealpha=0.9)\n",
        "            ax.grid(True, alpha=0.4, linewidth=1)\n",
        "\n",
        "            # Make tick labels larger\n",
        "            ax.tick_params(axis='both', which='major', labelsize=16, width=1.5, length=5)\n",
        "\n",
        "            y_min = 0.2\n",
        "            y_max = 1.05  # Slightly above optimal\n",
        "            ax.set_ylim(y_min, y_max)\n",
        "\n",
        "            # Keep axes normal weight\n",
        "            for spine in ax.spines.values():\n",
        "                spine.set_linewidth(1.5)\n",
        "\n",
        "        plt.suptitle(f'{method_name} (M={n_groups})', fontsize=24, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        clean_name = method_name.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')\n",
        "        pdf_filename = f\"{clean_name}_M{n_groups}_sample_size_analysis.pdf\"\n",
        "        plt.savefig(pdf_filename, format='pdf', dpi=300, bbox_inches='tight')\n",
        "        print(f\"Saved plot as: {pdf_filename}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Plot complete for {method_name}\")\n",
        "\n",
        "\n",
        "def run_nsw_sample_size_analysis(df_nsw, sample_size_range=None, budget_percentages=None, n_trials=50,\n",
        "                                outcome_col='re78', treatment_col='treat'):\n",
        "    \"\"\"Run sample size analysis on NSW dataset with theoretical bounds.\"\"\"\n",
        "\n",
        "    if sample_size_range is None:\n",
        "        sample_size_range = [100, 250, 500, 750, 1000, 1200, 1500, 2000, 5000, 10000, 20000]\n",
        "\n",
        "    if budget_percentages is None:\n",
        "        budget_percentages = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "    print(\"NSW SAMPLE SIZE ANALYSIS - EMPIRICAL VS THEORETICAL BOUNDS\")\n",
        "    print(f\"Sample sizes: {sample_size_range}\")\n",
        "    print(f\"Budget percentages: {budget_percentages}\")\n",
        "    print(f\"Trials per sample size: {n_trials}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Define NSW-specific grouping methods\n",
        "    methods = [\n",
        "        ('Demographics', lambda analyzer, df: analyzer.create_demographics_groups(df, min_size=6)),\n",
        "        ('Race/Ethnicity', lambda analyzer, df: analyzer.create_race_ethnicity_groups(df, min_size=6)),\n",
        "        ('Age Groups', lambda analyzer, df: analyzer.create_age_groups(df, n_groups=30, min_size=6)),\n",
        "        ('Education', lambda analyzer, df: analyzer.create_education_groups(df, min_size=6)),\n",
        "        ('Baseline Earnings', lambda analyzer, df: analyzer.create_baseline_earnings_groups(df, n_groups=30, min_size=6)),\n",
        "        ('Employment Status', lambda analyzer, df: analyzer.create_employment_status_groups(df, min_size=6)),\n",
        "        ('Causal Forest', lambda analyzer, df: analyzer.create_causal_forest_groups(df, n_groups=30, min_size=6)),\n",
        "        ('Propensity Score', lambda analyzer, df: analyzer.create_propensity_groups(df, n_groups=50, min_size=6))\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    for method_name, method_func in methods:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ANALYZING NSW METHOD: {method_name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        try:\n",
        "            analyzer = NSWSampleSizeAnalyzer()\n",
        "            df_processed = analyzer.process_nsw_data(df_nsw, outcome_col=outcome_col, treatment_col=treatment_col)\n",
        "\n",
        "            groups = method_func(analyzer, df_processed)\n",
        "\n",
        "            if len(groups) < 10:\n",
        "                print(f\"Too few groups ({len(groups)}) for {method_name} - skipping\")\n",
        "                continue\n",
        "\n",
        "            groups = analyzer.normalize_cates(groups)\n",
        "\n",
        "            # Run sample size analysis\n",
        "            results, optimal_values = analyzer.analyze_sample_size_performance(\n",
        "                groups, sample_size_range, budget_percentages, n_trials\n",
        "            )\n",
        "\n",
        "            all_results[method_name] = {\n",
        "                'results': results,\n",
        "                'optimal_values': optimal_values,\n",
        "                'n_groups': len(groups)\n",
        "            }\n",
        "\n",
        "            # Create plots with theoretical bounds\n",
        "            print(f\"Creating plots for {method_name}...\")\n",
        "            analyzer.plot_sample_size_analysis(\n",
        "                results, optimal_values, method_name, budget_percentages, len(groups)\n",
        "            )\n",
        "\n",
        "            # Print summary\n",
        "            print(f\"\\nSummary for {method_name}:\")\n",
        "            print(f\"Number of groups: {len(groups)}\")\n",
        "            print(\"Optimal values by budget:\")\n",
        "            for bp in budget_percentages:\n",
        "                print(f\"  {bp*100:.0f}%: {optimal_values[bp]:.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {method_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load NSW dataset\n",
        "    df_nsw = pd.read_stata('nsw.dta')\n",
        "\n",
        "    # Run sample size analysis with theoretical bounds\n",
        "    sample_sizes = [100, 250, 500, 750, 1000, 1200, 1500, 2000, 5000, 10000, 20000]\n",
        "    budget_percentages = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "    results = run_nsw_sample_size_analysis(\n",
        "        df_nsw,\n",
        "        sample_size_range=sample_sizes,\n",
        "        budget_percentages=budget_percentages,\n",
        "        n_trials=50\n",
        "    )"
      ]
    }
  ]
}