{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAVCKEVxh8yH"
      },
      "outputs": [],
      "source": [
        "# POSTOP CATE Analysis -- fixed epsilon\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import warnings\n",
        "import sys\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class TeeOutput:\n",
        "    \"\"\"Class to write output to both console and file simultaneously.\"\"\"\n",
        "    def __init__(self, filename):\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(filename, 'w')\n",
        "\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "        self.log.flush()  # Ensure immediate write to file\n",
        "\n",
        "    def flush(self):\n",
        "        self.terminal.flush()\n",
        "        self.log.flush()\n",
        "\n",
        "    def close(self):\n",
        "        self.log.close()\n",
        "\n",
        "class MedicalCATEAllocator:\n",
        "    \"\"\"Medical CATE allocation algorithm with fixed gamma=0.5 and updated heavy interval threshold.\"\"\"\n",
        "\n",
        "    def __init__(self, epsilon=0.1, gamma=0.5, delta=0.05, heavy_multiplier=1.6, random_seed=42):\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "        self.rho = gamma * np.sqrt(epsilon)\n",
        "        self.delta = delta\n",
        "        self.heavy_multiplier = heavy_multiplier  # New parameter for heavy interval threshold\n",
        "        self.random_seed = random_seed\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "        print(f\"Medical CATE Allocation Algorithm\")\n",
        "        print(f\"ε = {epsilon}\")\n",
        "        print(f\"√ε = {np.sqrt(epsilon):.6f}\")\n",
        "        print(f\"γ = {gamma}\")\n",
        "        print(f\"ρ = γ√ε = {self.rho:.6f}\")\n",
        "        print(f\"Heavy multiplier = {heavy_multiplier}x\")\n",
        "        print(f\"δ = {delta}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    def process_medical_data(self, df, outcome_col='postop4hour_throatpain', treatment_col='treat'):\n",
        "        \"\"\"Process medical dataset for analysis.\"\"\"\n",
        "        print(f\"Processing medical data with {len(df)} patients\")\n",
        "        print(f\"Available columns: {list(df.columns)}\")\n",
        "\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Check for required columns\n",
        "        if treatment_col not in df_processed.columns:\n",
        "            raise ValueError(f\"Missing required treatment column: {treatment_col}\")\n",
        "        if outcome_col not in df_processed.columns:\n",
        "            raise ValueError(f\"Missing required outcome column: {outcome_col}\")\n",
        "\n",
        "        # Set up treatment and outcome\n",
        "        df_processed['treatment'] = df_processed[treatment_col]\n",
        "        df_processed['outcome'] = df_processed[outcome_col]\n",
        "\n",
        "        # Create baseline risk using preoperative pain if available\n",
        "        if 'preop_pain' in df_processed.columns:\n",
        "            df_processed['baseline_risk'] = df_processed['preop_pain']\n",
        "        else:\n",
        "            df_processed['baseline_risk'] = 0  # Default if no baseline\n",
        "\n",
        "        # Clean data\n",
        "        initial_size = len(df_processed)\n",
        "        df_processed = df_processed.dropna(subset=['outcome', 'treatment'])\n",
        "        final_size = len(df_processed)\n",
        "\n",
        "        if initial_size != final_size:\n",
        "            print(f\"Dropped {initial_size - final_size} rows due to missing outcome/treatment\")\n",
        "\n",
        "        print(f\"Final dataset: {final_size} patients\")\n",
        "        print(f\"Treatment distribution: {df_processed['treatment'].value_counts().to_dict()}\")\n",
        "        print(f\"Outcome (4-hour throat pain) statistics: mean={df_processed['outcome'].mean():.2f}, std={df_processed['outcome'].std():.2f}\")\n",
        "\n",
        "        if 'baseline_risk' in df_processed.columns:\n",
        "            print(f\"Baseline (preop pain) stats: mean={df_processed['baseline_risk'].mean():.2f}, std={df_processed['baseline_risk'].std():.2f}\")\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def create_demographics_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups by key demographic characteristics in medical data.\"\"\"\n",
        "        print(f\"Creating medical demographics groups\")\n",
        "\n",
        "        # Key medical demographic variables\n",
        "        demo_features = ['preop_gender', 'preop_smoking', 'preop_asa']\n",
        "\n",
        "        # Check which features are available\n",
        "        available_features = [col for col in demo_features if col in df.columns]\n",
        "\n",
        "        if not available_features:\n",
        "            print(\"No demographic variables found\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Using demographic features: {available_features}\")\n",
        "\n",
        "        # Limit to top 3 features to avoid too many combinations\n",
        "        if len(available_features) > 3:\n",
        "            available_features = available_features[:3]\n",
        "\n",
        "        # Remove rows with missing values in these features\n",
        "        df_clean = df.dropna(subset=available_features)\n",
        "        print(f\"After removing missing values: {len(df_clean)}/{len(df)} patients\")\n",
        "\n",
        "        if len(df_clean) == 0:\n",
        "            return []\n",
        "\n",
        "        # Get unique combinations\n",
        "        groups = []\n",
        "        unique_combinations = df_clean[available_features].drop_duplicates()\n",
        "        print(f\"Found {len(unique_combinations)} unique demographic combinations\")\n",
        "\n",
        "        for combo_idx, (idx, combo) in enumerate(unique_combinations.iterrows()):\n",
        "            mask = pd.Series(True, index=df.index)\n",
        "            combo_description = []\n",
        "\n",
        "            for feature in available_features:\n",
        "                mask = mask & (df[feature] == combo[feature])\n",
        "                combo_description.append(f\"{feature}={combo[feature]}\")\n",
        "\n",
        "            indices = df[mask].index.tolist()\n",
        "            combo_id = \"_\".join(combo_description)\n",
        "\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': combo_id,\n",
        "                    'indices': indices,\n",
        "                    'type': 'demographics'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} demographic groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_age_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on age brackets.\"\"\"\n",
        "        print(f\"Creating age groups (target: {n_groups})\")\n",
        "\n",
        "        if 'preop_age' not in df.columns:\n",
        "            print(\"No age variable found\")\n",
        "            return []\n",
        "\n",
        "        # Create age-based groups\n",
        "        age = df['preop_age'].fillna(df['preop_age'].median())\n",
        "\n",
        "        # Create age brackets\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(age, percentiles)\n",
        "        bins = np.digitize(age, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'age_group_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'age'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} age groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_bmi_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on BMI categories.\"\"\"\n",
        "        print(f\"Creating BMI groups (target: {n_groups})\")\n",
        "\n",
        "        if 'preop_calcbmi' not in df.columns:\n",
        "            print(\"No BMI variable found\")\n",
        "            return []\n",
        "\n",
        "        # Create BMI-based groups\n",
        "        bmi = df['preop_calcbmi'].fillna(df['preop_calcbmi'].median())\n",
        "\n",
        "        # Create BMI brackets\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(bmi, percentiles)\n",
        "        bins = np.digitize(bmi, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'bmi_group_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'bmi'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} BMI groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_mallampati_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups based on Mallampati score (airway assessment).\"\"\"\n",
        "        print(f\"Creating Mallampati score groups\")\n",
        "\n",
        "        if 'preop_mallampati' not in df.columns:\n",
        "            print(\"No Mallampati score variable found\")\n",
        "            return []\n",
        "\n",
        "        groups = []\n",
        "        for mallampati_score in df['preop_mallampati'].unique():\n",
        "            if pd.isna(mallampati_score):\n",
        "                continue\n",
        "\n",
        "            indices = df[df['preop_mallampati'] == mallampati_score].index.tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'mallampati_class_{mallampati_score}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'mallampati'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} Mallampati groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_preop_pain_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups based on preoperative pain levels.\"\"\"\n",
        "        print(f\"Creating preoperative pain groups (target: {n_groups})\")\n",
        "\n",
        "        if 'preop_pain' not in df.columns:\n",
        "            print(\"No preoperative pain data available\")\n",
        "            return []\n",
        "\n",
        "        # Create pain-based groups\n",
        "        pain = df['preop_pain'].fillna(0)  # Fill NaN with 0 for no pain\n",
        "\n",
        "        # Create pain level brackets including zero pain\n",
        "        if (pain == 0).mean() > 0.3:  # If >30% have zero pain, create separate zero group\n",
        "            # Create one group for zero pain patients\n",
        "            zero_pain = df.index[pain == 0].tolist()\n",
        "            groups = []\n",
        "            if len(zero_pain) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': 'no_preop_pain',\n",
        "                    'indices': zero_pain,\n",
        "                    'type': 'preop_pain'\n",
        "                })\n",
        "\n",
        "            # Create groups for patients with pain\n",
        "            positive_pain = pain[pain > 0]\n",
        "            if len(positive_pain) > 0:\n",
        "                percentiles = np.linspace(0, 100, n_groups)\n",
        "                cuts = np.percentile(positive_pain, percentiles)\n",
        "\n",
        "                for i in range(len(cuts) - 1):\n",
        "                    mask = (pain > cuts[i]) & (pain <= cuts[i + 1])\n",
        "                    indices = df.index[mask].tolist()\n",
        "                    if len(indices) >= min_size:\n",
        "                        groups.append({\n",
        "                            'id': f'preop_pain_level_{i}',\n",
        "                            'indices': indices,\n",
        "                            'type': 'preop_pain'\n",
        "                        })\n",
        "        else:\n",
        "            # Standard percentile groups\n",
        "            percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "            cuts = np.percentile(pain, percentiles)\n",
        "            bins = np.digitize(pain, cuts) - 1\n",
        "\n",
        "            groups = []\n",
        "            for i in range(n_groups):\n",
        "                indices = df.index[bins == i].tolist()\n",
        "                if len(indices) >= min_size:\n",
        "                    groups.append({\n",
        "                        'id': f'preop_pain_level_{i}',\n",
        "                        'indices': indices,\n",
        "                        'type': 'preop_pain'\n",
        "                    })\n",
        "\n",
        "        print(f\"Created {len(groups)} preoperative pain groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_gender_smoking_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups based on gender and smoking status combinations.\"\"\"\n",
        "        print(f\"Creating gender-smoking groups\")\n",
        "\n",
        "        # Create gender-smoking categories\n",
        "        def get_gender_smoking(row):\n",
        "            gender = row.get('preop_gender', 'unknown')\n",
        "            smoking = row.get('preop_smoking', 'unknown')\n",
        "            return f\"gender_{gender}_smoking_{smoking}\"\n",
        "\n",
        "        if 'preop_gender' in df.columns and 'preop_smoking' in df.columns:\n",
        "            df['gender_smoking'] = df.apply(get_gender_smoking, axis=1)\n",
        "\n",
        "            groups = []\n",
        "            for category in df['gender_smoking'].unique():\n",
        "                indices = df[df['gender_smoking'] == category].index.tolist()\n",
        "                if len(indices) >= min_size:\n",
        "                    groups.append({\n",
        "                        'id': f'{category}',\n",
        "                        'indices': indices,\n",
        "                        'type': 'gender_smoking'\n",
        "                    })\n",
        "\n",
        "            print(f\"Created {len(groups)} gender-smoking groups\")\n",
        "            balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "            return balanced_groups\n",
        "        else:\n",
        "            print(\"No gender or smoking variables found\")\n",
        "            return []\n",
        "\n",
        "    def create_asa_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups based on ASA (American Society of Anesthesiologists) physical status.\"\"\"\n",
        "        print(f\"Creating ASA physical status groups\")\n",
        "\n",
        "        if 'preop_asa' not in df.columns:\n",
        "            print(\"No ASA physical status data\")\n",
        "            return []\n",
        "\n",
        "        groups = []\n",
        "        for asa_status in df['preop_asa'].unique():\n",
        "            if pd.isna(asa_status):\n",
        "                continue\n",
        "\n",
        "            indices = df[df['preop_asa'] == asa_status].index.tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'asa_class_{asa_status}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'asa_status'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} ASA status groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_surgery_size_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups based on surgery size.\"\"\"\n",
        "        print(f\"Creating surgery size groups\")\n",
        "\n",
        "        if 'intraop_surgerysize' not in df.columns:\n",
        "            print(\"No surgery size data available\")\n",
        "            return []\n",
        "\n",
        "        groups = []\n",
        "        for surgery_size in df['intraop_surgerysize'].unique():\n",
        "            if pd.isna(surgery_size):\n",
        "                continue\n",
        "\n",
        "            indices = df[df['intraop_surgerysize'] == surgery_size].index.tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'surgery_size_{surgery_size}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'surgery_size'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} surgery size groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_causal_forest_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create groups using clustering on baseline covariates only (no outcome data).\"\"\"\n",
        "        print(f\"Creating covariate-based forest groups (target: {n_groups})\")\n",
        "\n",
        "        feature_cols = ['preop_age', 'preop_calcbmi', 'preop_gender', 'preop_asa',\n",
        "                       'preop_mallampati', 'preop_smoking', 'preop_pain', 'intraop_surgerysize']\n",
        "        available_features = [col for col in feature_cols if col in df.columns]\n",
        "\n",
        "        if not available_features:\n",
        "            print(\"No features available for covariate clustering\")\n",
        "            return []\n",
        "\n",
        "        X = df[available_features].copy()\n",
        "\n",
        "        # Handle missing values and encode categorical variables\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object':\n",
        "                # Encode categorical variables\n",
        "                le = LabelEncoder()\n",
        "                X[col] = X[col].fillna('missing')\n",
        "                X[col] = le.fit_transform(X[col])\n",
        "            else:\n",
        "                if X[col].isna().any():\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "        # Cluster based ONLY on baseline covariates (no outcome-based predictions)\n",
        "        cluster_features = StandardScaler().fit_transform(X.values)\n",
        "        labels = KMeans(n_clusters=n_groups, random_state=self.random_seed).fit_predict(cluster_features)\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[labels == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'covariate_cluster_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'covariate_cluster'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} covariate-based groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_propensity_groups(self, df, n_groups=50, min_size=6):\n",
        "        \"\"\"Create groups based on propensity score strata.\"\"\"\n",
        "        print(f\"Creating propensity score groups (target: {n_groups})\")\n",
        "\n",
        "        # Use medical covariates\n",
        "        feature_cols = ['preop_age', 'preop_calcbmi', 'preop_gender', 'preop_asa',\n",
        "                       'preop_mallampati', 'preop_smoking', 'preop_pain', 'intraop_surgerysize']\n",
        "        available_features = [col for col in feature_cols if col in df.columns]\n",
        "\n",
        "        if not available_features:\n",
        "            print(\"No features available for propensity scoring\")\n",
        "            return []\n",
        "\n",
        "        X = df[available_features].copy()\n",
        "\n",
        "        # Handle missing values and encode categorical variables\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object':\n",
        "                # Encode categorical variables\n",
        "                le = LabelEncoder()\n",
        "                X[col] = X[col].fillna('missing')\n",
        "                X[col] = le.fit_transform(X[col])\n",
        "            else:\n",
        "                if X[col].isna().any():\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "        # Get propensity scores\n",
        "        try:\n",
        "            prop_scores = cross_val_predict(\n",
        "                LogisticRegression(random_state=self.random_seed, max_iter=1000),\n",
        "                X, df['treatment'], method='predict_proba', cv=5\n",
        "            )[:, 1]\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing propensity scores: {e}\")\n",
        "            return []\n",
        "\n",
        "        # Create strata\n",
        "        quantiles = np.linspace(0, 1, n_groups + 1)\n",
        "        bins = np.digitize(prop_scores, np.quantile(prop_scores, quantiles)) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'propensity_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'propensity'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} propensity groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def _ensure_balance_and_compute_cate(self, df, groups):\n",
        "        \"\"\"Ensure treatment balance and compute group CATE.\"\"\"\n",
        "        balanced_groups = []\n",
        "\n",
        "        for group in groups:\n",
        "            group_df = df.loc[group['indices']]\n",
        "\n",
        "            treatment_rate = group_df['treatment'].mean()\n",
        "            n_treated = group_df['treatment'].sum()\n",
        "            n_control = len(group_df) - n_treated\n",
        "\n",
        "            if not (0.15 <= treatment_rate <= 0.85 and n_treated >= 3 and n_control >= 3):\n",
        "                continue\n",
        "\n",
        "            treated_outcomes = group_df[group_df['treatment'] == 1]['outcome']\n",
        "            control_outcomes = group_df[group_df['treatment'] == 0]['outcome']\n",
        "            # For this dataset, negative CATE means treatment reduces throat pain (beneficial)\n",
        "            # We reverse sign to maintain consistency with plots (higher = more beneficial)\n",
        "            cate = -(treated_outcomes.mean() - control_outcomes.mean())  # Reversed sign\n",
        "\n",
        "            balanced_groups.append({\n",
        "                'id': group['id'],\n",
        "                'indices': group['indices'],\n",
        "                'size': len(group_df),\n",
        "                'treatment_rate': treatment_rate,\n",
        "                'n_treated': int(n_treated),\n",
        "                'n_control': int(n_control),\n",
        "                'cate': cate,\n",
        "                'type': group['type']\n",
        "            })\n",
        "\n",
        "        return balanced_groups\n",
        "\n",
        "    def normalize_cates(self, groups):\n",
        "        \"\"\"Normalize CATE values to [0,1].\"\"\"\n",
        "        cates = [g['cate'] for g in groups]\n",
        "        min_cate, max_cate = min(cates), max(cates)\n",
        "\n",
        "        if max_cate > min_cate:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = (group['cate'] - min_cate) / (max_cate - min_cate)\n",
        "        else:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = 0.5\n",
        "\n",
        "        print(f\"CATE normalization: [{min_cate:.3f}, {max_cate:.3f}] → [0, 1]\")\n",
        "        return groups\n",
        "\n",
        "    def plot_cate_distribution(self, groups, title_suffix=\"\"):\n",
        "        \"\"\"Plot CATE distribution.\"\"\"\n",
        "        original_cates = [g['cate'] for g in groups]\n",
        "        normalized_cates = [g['normalized_cate'] for g in groups]\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "        ax1.hist(original_cates, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        ax1.set_xlabel('Original CATE (pain reduction effect)')\n",
        "        ax1.set_ylabel('Frequency')\n",
        "        ax1.set_title(f'Original CATE Distribution{title_suffix}')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        ax2.hist(normalized_cates, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "        ax2.set_xlabel('Normalized CATE (τ)')\n",
        "        ax2.set_ylabel('Frequency')\n",
        "        ax2.set_title(f'Normalized CATE Distribution{title_suffix}')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def estimate_tau(self, true_tau, accuracy):\n",
        "        \"\"\"Estimate tau using Hoeffding's inequality with Bernoulli samples.\"\"\"\n",
        "        sample_size = int(np.ceil(np.log(2/self.delta) / (2 * accuracy**2)))\n",
        "        samples = np.random.binomial(1, true_tau, sample_size)\n",
        "        return np.mean(samples), sample_size\n",
        "\n",
        "    def run_single_trial(self, groups, epsilon_val, trial_seed):\n",
        "        \"\"\"Run allocation algorithm for single trial with fixed gamma.\"\"\"\n",
        "        np.random.seed(self.random_seed + trial_seed)\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "        rho = self.gamma * np.sqrt(epsilon_val)  # Use fixed gamma\n",
        "\n",
        "        # Estimate all tau values using rho accuracy\n",
        "        tau_estimates_rho = []\n",
        "        for tau in tau_true:\n",
        "            estimate, _ = self.estimate_tau(tau, rho)\n",
        "            tau_estimates_rho.append(estimate)\n",
        "        tau_estimates_rho = np.array(tau_estimates_rho)\n",
        "\n",
        "        # Also estimate using epsilon accuracy for comparison\n",
        "        tau_estimates_eps = []\n",
        "        for tau in tau_true:\n",
        "            estimate, _ = self.estimate_tau(tau, epsilon_val)\n",
        "            tau_estimates_eps.append(estimate)\n",
        "        tau_estimates_eps = np.array(tau_estimates_eps)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for K in range(1, n_groups):\n",
        "            optimal_indices = np.argsort(tau_true)[-K:]\n",
        "            optimal_value = np.sum(tau_true[optimal_indices])\n",
        "\n",
        "            rho_indices = np.argsort(tau_estimates_rho)[-K:]\n",
        "            rho_value = np.sum(tau_true[rho_indices])\n",
        "\n",
        "            eps_indices = np.argsort(tau_estimates_eps)[-K:]\n",
        "            eps_value = np.sum(tau_true[eps_indices])\n",
        "\n",
        "            rho_ratio = rho_value / optimal_value if optimal_value > 0 else 0\n",
        "            eps_ratio = eps_value / optimal_value if optimal_value > 0 else 0\n",
        "            rho_success = rho_ratio >= (1 - epsilon_val)\n",
        "            eps_success = eps_ratio >= (1 - epsilon_val)\n",
        "\n",
        "            tau_k_est = tau_estimates_rho[rho_indices[0]]\n",
        "            a2_lower = tau_k_est\n",
        "            a2_upper = tau_k_est + 2 * rho\n",
        "            units_in_a2 = np.sum((tau_estimates_rho >= a2_lower) & (tau_estimates_rho <= a2_upper))\n",
        "            expected_a2 = 2 * rho * n_groups\n",
        "            # Updated heavy interval detection with 1.6x multiplier\n",
        "            is_heavy = units_in_a2 > self.heavy_multiplier * expected_a2\n",
        "\n",
        "            results.append({\n",
        "                'K': K,\n",
        "                'optimal_value': optimal_value,\n",
        "                'rho_value': rho_value,\n",
        "                'eps_value': eps_value,\n",
        "                'rho_ratio': rho_ratio,\n",
        "                'eps_ratio': eps_ratio,\n",
        "                'rho_success': rho_success,\n",
        "                'eps_success': eps_success,\n",
        "                'is_heavy': is_heavy,\n",
        "                'tau_k_est': tau_k_est,\n",
        "                'units_in_a2': units_in_a2\n",
        "            })\n",
        "\n",
        "        return results, tau_estimates_rho\n",
        "\n",
        "    def find_recovery_units(self, K, tau_true, tau_estimates, epsilon_val):\n",
        "        \"\"\"Find minimum units needed to achieve 1-epsilon performance.\"\"\"\n",
        "        n_groups = len(tau_true)\n",
        "\n",
        "        # Original allocation (using rho estimates)\n",
        "        rho_indices = np.argsort(tau_estimates)[-K:]\n",
        "        optimal_value = np.sum(tau_true[np.argsort(tau_true)[-K:]])\n",
        "\n",
        "        # Remaining candidates (sorted by estimate, best first)\n",
        "        remaining_indices = np.argsort(tau_estimates)[:-K][::-1]\n",
        "\n",
        "        # Test adding 1 to 10 additional units\n",
        "        for extra in range(1, 11):\n",
        "            if extra > len(remaining_indices):\n",
        "                break\n",
        "\n",
        "            expanded_indices = np.concatenate([rho_indices, remaining_indices[:extra]])\n",
        "            expanded_value = np.sum(tau_true[expanded_indices])\n",
        "\n",
        "            if expanded_value / optimal_value >= (1 - epsilon_val):\n",
        "                return extra\n",
        "\n",
        "        return None  # Need more than 10 units\n",
        "\n",
        "    def find_closest_working_budget(self, failed_K, trial_results):\n",
        "        \"\"\"Find closest budget that works for a failed budget.\"\"\"\n",
        "        working_budgets = [r['K'] for r in trial_results if r['rho_success']]\n",
        "\n",
        "        if not working_budgets:\n",
        "            return None, None\n",
        "\n",
        "        # Distance to any working budget (either direction)\n",
        "        distances_any = [abs(K - failed_K) for K in working_budgets]\n",
        "        min_distance_any = min(distances_any)\n",
        "\n",
        "        # Distance to smaller working budget (underspending)\n",
        "        smaller_working = [K for K in working_budgets if K < failed_K]\n",
        "        if smaller_working:\n",
        "            min_distance_smaller = failed_K - max(smaller_working)\n",
        "        else:\n",
        "            min_distance_smaller = None\n",
        "\n",
        "        return min_distance_any, min_distance_smaller\n",
        "\n",
        "    def analyze_method(self, groups, epsilon_val, n_trials=30):\n",
        "        \"\"\"Analyze single method with fixed gamma and updated heavy threshold.\"\"\"\n",
        "        print(f\"\\nAnalyzing {len(groups)} groups with ε={epsilon_val}, γ={self.gamma}\")\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "\n",
        "        trial_data = []\n",
        "\n",
        "        for trial in range(n_trials):\n",
        "            print(f\"Trial {trial + 1}/{n_trials}...\")\n",
        "\n",
        "            # Run single trial\n",
        "            trial_results, tau_estimates = self.run_single_trial(groups, epsilon_val, trial)\n",
        "\n",
        "            # Analyze failures\n",
        "            failed_results = [r for r in trial_results if not r['rho_success']]\n",
        "            failed_budgets = [r['K'] for r in failed_results]\n",
        "\n",
        "            # Check which failed budgets are heavy with true tau values\n",
        "            failed_heavy_estimated = []\n",
        "            failed_heavy_true = []\n",
        "            rho = self.gamma * np.sqrt(epsilon_val)\n",
        "\n",
        "            for failed_result in failed_results:\n",
        "                K = failed_result['K']\n",
        "                # Heavy with estimated values (already computed)\n",
        "                failed_heavy_estimated.append(failed_result['is_heavy'])\n",
        "\n",
        "                # Check heavy with true tau values\n",
        "                tau_k_true = tau_true[np.argsort(tau_true)[-K:]][0]  # True smallest in top-K\n",
        "                a2_lower_true = tau_k_true\n",
        "                a2_upper_true = tau_k_true + 2 * rho\n",
        "                units_in_a2_true = np.sum((tau_true >= a2_lower_true) & (tau_true <= a2_upper_true))\n",
        "                expected_a2_true = 2 * rho * n_groups\n",
        "                is_heavy_true = units_in_a2_true > self.heavy_multiplier * expected_a2_true\n",
        "                failed_heavy_true.append(is_heavy_true)\n",
        "\n",
        "            # Print trial summary\n",
        "            print(f\"  Failed budgets: {failed_budgets}\")\n",
        "\n",
        "            # Print heavy vectors\n",
        "            if len(failed_budgets) > 0:\n",
        "                estimated_clean = [bool(x) for x in failed_heavy_estimated]\n",
        "                true_clean = [bool(x) for x in failed_heavy_true]\n",
        "                print(f\"  HEAVY INTERVALS - Estimated: {estimated_clean}\")\n",
        "                print(f\"  HEAVY INTERVALS - True τ_K:   {true_clean}\")\n",
        "\n",
        "            # Count total heavy intervals and failed budgets in heavy intervals\n",
        "            total_heavy = sum(r['is_heavy'] for r in trial_results)\n",
        "            failed_heavy = sum(r['is_heavy'] for r in failed_results)\n",
        "\n",
        "            # Recovery analysis\n",
        "            recovery_units = []\n",
        "            distances_to_working_any = []\n",
        "            distances_to_working_smaller = []\n",
        "\n",
        "            for failed_result in failed_results:\n",
        "                K = failed_result['K']\n",
        "\n",
        "                # Find recovery units needed\n",
        "                recovery = self.find_recovery_units(K, tau_true, tau_estimates, epsilon_val)\n",
        "                if recovery is not None:\n",
        "                    recovery_units.append(recovery)\n",
        "\n",
        "                # Find distances to closest working budgets\n",
        "                distance_any, distance_smaller = self.find_closest_working_budget(K, trial_results)\n",
        "                if distance_any is not None:\n",
        "                    distances_to_working_any.append(distance_any)\n",
        "                if distance_smaller is not None:\n",
        "                    distances_to_working_smaller.append(distance_smaller)\n",
        "\n",
        "            trial_info = {\n",
        "                'trial': trial,\n",
        "                'failed_budgets': failed_budgets,\n",
        "                'num_failures': len(failed_results),\n",
        "                'total_heavy': total_heavy,\n",
        "                'failed_heavy': failed_heavy,\n",
        "                'failed_heavy_estimated': failed_heavy_estimated,\n",
        "                'failed_heavy_true': failed_heavy_true,\n",
        "                'recovery_units': recovery_units,\n",
        "                'distances_to_working_any': distances_to_working_any,\n",
        "                'distances_to_working_smaller': distances_to_working_smaller\n",
        "            }\n",
        "\n",
        "            trial_data.append(trial_info)\n",
        "\n",
        "            print(f\"  Failures: {len(failed_results)}, Total heavy: {total_heavy}, Failed heavy: {failed_heavy}\")\n",
        "            if recovery_units:\n",
        "                print(f\"  Recovery units: μ={np.mean(recovery_units):.1f}, med={np.median(recovery_units):.0f}, max={np.max(recovery_units)}\")\n",
        "            if distances_to_working_any:\n",
        "                print(f\"  Distance any: μ={np.mean(distances_to_working_any):.1f}, med={np.median(distances_to_working_any):.0f}, max={np.max(distances_to_working_any)}\")\n",
        "            if distances_to_working_smaller:\n",
        "                print(f\"  Distance smaller: μ={np.mean(distances_to_working_smaller):.1f}, med={np.median(distances_to_working_smaller):.0f}, max={np.max(distances_to_working_smaller)}\")\n",
        "            else:\n",
        "                print(f\"  Distance smaller: No smaller working budgets found\")\n",
        "\n",
        "        return trial_data\n",
        "\n",
        "    def print_method_summary(self, method_name, trial_data, n_groups, epsilon_val):\n",
        "        \"\"\"Print summary statistics for a method.\"\"\"\n",
        "        budget_10pct_threshold = max(1, int(0.1 * n_groups))\n",
        "\n",
        "        print(f\"\\n{'='*100}\")\n",
        "        print(f\"SUMMARY - {method_name} - ε={epsilon_val} - {n_groups} GROUPS\")\n",
        "        print(\"=\"*100)\n",
        "        print(f\"{'Fail μ':<7} {'Fail σ':<7} {'FailR% μ':<9} {'FailR% σ':<9} {'TotHvy':<8} {'FailHvy':<9} {'Rec μ':<7} {'Rec med':<8} {'Rec max':<8} {'DAny μ':<8} {'DAny σ':<10} {'DAny max':<10} {'DSmall μ':<10} {'DSmall σ':<12} {'DSmall max':<12}\")\n",
        "        print(\"-\"*120)\n",
        "\n",
        "        # Aggregate statistics across all trials - ALL BUDGETS\n",
        "        all_failures = [t['num_failures'] for t in trial_data]\n",
        "        all_total_heavy = [t['total_heavy'] for t in trial_data]\n",
        "        all_failed_heavy = [t['failed_heavy'] for t in trial_data]\n",
        "        all_recovery = []\n",
        "        all_distances_any = []\n",
        "        all_distances_smaller = []\n",
        "\n",
        "        for t in trial_data:\n",
        "            all_recovery.extend(t['recovery_units'])\n",
        "            all_distances_any.extend(t['distances_to_working_any'])\n",
        "            all_distances_smaller.extend(t['distances_to_working_smaller'])\n",
        "\n",
        "        avg_failures = np.mean(all_failures)\n",
        "        std_failures = np.std(all_failures)\n",
        "        avg_failure_rate = avg_failures / (n_groups - 1) * 100\n",
        "        std_failure_rate = std_failures / (n_groups - 1) * 100\n",
        "        avg_total_heavy = np.mean(all_total_heavy)\n",
        "        avg_failed_heavy = np.mean(all_failed_heavy)\n",
        "\n",
        "        # Recovery statistics\n",
        "        if all_recovery:\n",
        "            recovery_mean = np.mean(all_recovery)\n",
        "            recovery_med = np.median(all_recovery)\n",
        "            recovery_max = np.max(all_recovery)\n",
        "        else:\n",
        "            recovery_mean = recovery_med = recovery_max = np.nan\n",
        "\n",
        "        # Distance statistics - any direction\n",
        "        if all_distances_any:\n",
        "            distance_any_mean = np.mean(all_distances_any)\n",
        "            distance_any_std = np.std(all_distances_any)\n",
        "            distance_any_max = np.max(all_distances_any)\n",
        "        else:\n",
        "            distance_any_mean = distance_any_std = distance_any_max = np.nan\n",
        "\n",
        "        # Distance statistics - smaller only\n",
        "        if all_distances_smaller:\n",
        "            distance_smaller_mean = np.mean(all_distances_smaller)\n",
        "            distance_smaller_std = np.std(all_distances_smaller)\n",
        "            distance_smaller_max = np.max(all_distances_smaller)\n",
        "        else:\n",
        "            distance_smaller_mean = distance_smaller_std = distance_smaller_max = np.nan\n",
        "\n",
        "        print(f\"{avg_failures:<7.1f} {std_failures:<7.1f} {avg_failure_rate:<9.1f} {std_failure_rate:<9.1f} {avg_total_heavy:<8.1f} {avg_failed_heavy:<9.1f} \"\n",
        "              f\"{recovery_mean:<7.1f} {recovery_med:<8.0f} {recovery_max:<8.0f} \"\n",
        "              f\"{distance_any_mean:<8.1f} {distance_any_std:<10.1f} {distance_any_max:<10.0f} \"\n",
        "              f\"{distance_smaller_mean:<10.1f} {distance_smaller_std:<12.1f} {distance_smaller_max:<12.0f}\")\n",
        "\n",
        "        return {\n",
        "            'avg_failures': avg_failures,\n",
        "            'failure_rate_pct': avg_failure_rate,\n",
        "            'avg_recovery': recovery_mean,\n",
        "            'n_groups': n_groups\n",
        "        }\n",
        "\n",
        "\n",
        "def run_comprehensive_medical_analysis(df_medical, epsilon_values=None, n_trials=30, log_file=None):\n",
        "    \"\"\"Run comprehensive medical analysis with all methods, fixed gamma=0.5, and 1.6x heavy threshold.\"\"\"\n",
        "\n",
        "    if epsilon_values is None:\n",
        "        epsilon_values = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2]\n",
        "\n",
        "    # Set up logging if requested\n",
        "    if log_file is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        log_file = f\"medical_comprehensive_analysis_gamma05_{timestamp}.txt\"\n",
        "\n",
        "    # Redirect output to both console and file\n",
        "    original_stdout = sys.stdout\n",
        "    tee = TeeOutput(log_file)\n",
        "    sys.stdout = tee\n",
        "\n",
        "    try:\n",
        "        print(\"COMPREHENSIVE MEDICAL ANALYSIS - ALL METHODS, FIXED γ=0.5, HEAVY THRESHOLD=1.6x\")\n",
        "        print(f\"Log file: {log_file}\")\n",
        "        print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        # Define all medical-specific grouping methods\n",
        "        methods = [\n",
        "            ('Demographics', lambda allocator, df: allocator.create_demographics_groups(df, min_size=6)),\n",
        "            ('Gender-Smoking', lambda allocator, df: allocator.create_gender_smoking_groups(df, min_size=6)),\n",
        "            ('Age Groups', lambda allocator, df: allocator.create_age_groups(df, n_groups=30, min_size=6)),\n",
        "            ('BMI Groups', lambda allocator, df: allocator.create_bmi_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Mallampati Score', lambda allocator, df: allocator.create_mallampati_groups(df, min_size=6)),\n",
        "            ('ASA Physical Status', lambda allocator, df: allocator.create_asa_groups(df, min_size=6)),\n",
        "            ('Preoperative Pain', lambda allocator, df: allocator.create_preop_pain_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Surgery Size', lambda allocator, df: allocator.create_surgery_size_groups(df, min_size=6)),\n",
        "            ('Covariate Forest 30', lambda allocator, df: allocator.create_causal_forest_groups(df, n_groups=30, min_size=6)),\n",
        "            ('Covariate Forest 50', lambda allocator, df: allocator.create_causal_forest_groups(df, n_groups=50, min_size=6)),\n",
        "            ('Propensity Score', lambda allocator, df: allocator.create_propensity_groups(df, n_groups=50, min_size=6))\n",
        "        ]\n",
        "\n",
        "        all_results = {}\n",
        "\n",
        "        for method_name, method_func in methods:\n",
        "            print(f\"\\n{'='*120}\")\n",
        "            print(f\"ANALYZING MEDICAL METHOD: {method_name}\")\n",
        "            print(\"=\"*120)\n",
        "\n",
        "            method_results = []\n",
        "\n",
        "            for eps in epsilon_values:\n",
        "                print(f\"\\n{'='*100}\")\n",
        "                print(f\"METHOD: {method_name} | EPSILON = {eps}\")\n",
        "                print(\"=\"*100)\n",
        "\n",
        "                # Initialize allocator with fixed gamma=0.5 and 1.6x heavy threshold\n",
        "                allocator = MedicalCATEAllocator(epsilon=eps, gamma=0.5, heavy_multiplier=1.6)\n",
        "                df_processed = allocator.process_medical_data(df_medical)\n",
        "\n",
        "                try:\n",
        "                    # Create groups using this method\n",
        "                    groups = method_func(allocator, df_processed)\n",
        "\n",
        "                    if len(groups) < 3:\n",
        "                        print(f\"Too few groups ({len(groups)}) for {method_name} with ε = {eps} - skipping\")\n",
        "                        continue\n",
        "\n",
        "                    groups = allocator.normalize_cates(groups)\n",
        "\n",
        "                    # Show CATE distribution\n",
        "                    allocator.plot_cate_distribution(groups, f\" ({method_name}, ε={eps})\")\n",
        "\n",
        "                    # Run analysis for this epsilon and method\n",
        "                    trial_data = allocator.analyze_method(groups, eps, n_trials)\n",
        "\n",
        "                    # Print method summary\n",
        "                    stats = allocator.print_method_summary(method_name, trial_data, len(groups), eps)\n",
        "\n",
        "                    epsilon_result = {\n",
        "                        'method': method_name,\n",
        "                        'epsilon': eps,\n",
        "                        'sqrt_epsilon': np.sqrt(eps),\n",
        "                        'gamma': 0.5,\n",
        "                        'rho': 0.5 * np.sqrt(eps),\n",
        "                        'groups': groups,\n",
        "                        'trial_data': trial_data,\n",
        "                        'stats': stats\n",
        "                    }\n",
        "\n",
        "                    method_results.append(epsilon_result)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error with {method_name} at ε = {eps}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            all_results[method_name] = method_results\n",
        "\n",
        "            # Add method-specific summary table after all epsilons for this method\n",
        "            if method_results:\n",
        "                print(f\"\\n{'='*120}\")\n",
        "                print(f\"METHOD SUMMARY - {method_name} - ALL EPSILON VALUES\")\n",
        "                print(\"=\"*120)\n",
        "                print(f\"{'ε':<8} {'√ε':<10} {'γ':<6} {'ρ':<10} {'Groups':<8} {'Fail μ':<8} {'FailR%':<8} {'Rec μ':<8}\")\n",
        "                print(\"-\" * 80)\n",
        "\n",
        "                for eps_result in method_results:\n",
        "                    eps = eps_result['epsilon']\n",
        "                    sqrt_eps = eps_result['sqrt_epsilon']\n",
        "                    gamma = eps_result['gamma']\n",
        "                    rho = eps_result['rho']\n",
        "                    n_groups = len(eps_result['groups'])\n",
        "                    stats = eps_result['stats']\n",
        "\n",
        "                    print(f\"{eps:<8} {sqrt_eps:<10.6f} {gamma:<6} {rho:<10.6f} \"\n",
        "                          f\"{n_groups:<8} {stats['avg_failures']:<8.1f} {stats['failure_rate_pct']:<8.1f} \"\n",
        "                          f\"{stats['avg_recovery']:<8.1f}\")\n",
        "                print(\"=\"*120)\n",
        "\n",
        "        # Create comprehensive summary across all methods and epsilon values\n",
        "        print(f\"\\n{'='*200}\")\n",
        "        print(\"COMPREHENSIVE SUMMARY - ALL MEDICAL METHODS AND EPSILON VALUES\")\n",
        "        print(\"=\"*200)\n",
        "\n",
        "        # Create summary table\n",
        "        summary_data = []\n",
        "\n",
        "        for method_name, method_results in all_results.items():\n",
        "            if not method_results:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n{'-'*100}\")\n",
        "            print(f\"MEDICAL METHOD: {method_name}\")\n",
        "            print(\"-\"*100)\n",
        "\n",
        "            for eps_result in method_results:\n",
        "                eps = eps_result['epsilon']\n",
        "                sqrt_eps = eps_result['sqrt_epsilon']\n",
        "                gamma = eps_result['gamma']\n",
        "                rho = eps_result['rho']\n",
        "                n_groups = len(eps_result['groups'])\n",
        "                stats = eps_result['stats']\n",
        "\n",
        "                summary_data.append({\n",
        "                    'method': method_name,\n",
        "                    'epsilon': eps,\n",
        "                    'sqrt_eps': sqrt_eps,\n",
        "                    'gamma': gamma,\n",
        "                    'rho': rho,\n",
        "                    'avg_failures': stats['avg_failures'],\n",
        "                    'failure_rate_pct': stats['failure_rate_pct'],\n",
        "                    'avg_recovery': stats['avg_recovery'],\n",
        "                    'n_groups': stats['n_groups']\n",
        "                })\n",
        "\n",
        "            # Print method-specific table\n",
        "            method_data = [d for d in summary_data if d['method'] == method_name]\n",
        "            if method_data:\n",
        "                print(f\"{'ε':<8} {'√ε':<10} {'γ':<6} {'ρ':<10} {'Groups':<8} {'Fail μ':<8} {'FailR%':<8} {'Rec μ':<8}\")\n",
        "                print(\"-\" * 80)\n",
        "\n",
        "                for data in method_data:\n",
        "                    print(f\"{data['epsilon']:<8} {data['sqrt_eps']:<10.6f} {data['gamma']:<6} {data['rho']:<10.6f} \"\n",
        "                          f\"{data['n_groups']:<8} {data['avg_failures']:<8.1f} {data['failure_rate_pct']:<8.1f} \"\n",
        "                          f\"{data['avg_recovery']:<8.1f}\")\n",
        "\n",
        "        # Overall summary table\n",
        "        print(f\"\\n{'='*200}\")\n",
        "        print(\"OVERALL SUMMARY TABLE - ALL MEDICAL METHODS COMBINED\")\n",
        "        print(\"=\"*200)\n",
        "        print(f\"{'Method':<18} {'ε':<8} {'√ε':<10} {'γ':<6} {'ρ':<10} {'Groups':<8} {'Fail μ':<8} {'FailR%':<8} {'Rec μ':<8}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        for data in summary_data:\n",
        "            print(f\"{data['method']:<18} {data['epsilon']:<8} {data['sqrt_eps']:<10.6f} {data['gamma']:<6} {data['rho']:<10.6f} \"\n",
        "                  f\"{data['n_groups']:<8} {data['avg_failures']:<8.1f} {data['failure_rate_pct']:<8.1f} \"\n",
        "                  f\"{data['avg_recovery']:<8.1f}\")\n",
        "\n",
        "        # Medical-specific analysis insights\n",
        "        print(f\"\\n{'='*100}\")\n",
        "        print(\"KEY INSIGHTS FOR MEDICAL DATASET\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        # Find best and worst performing methods for medical data\n",
        "        if summary_data:\n",
        "            # Average performance across all epsilon values per method\n",
        "            method_performance = {}\n",
        "            for method_name in all_results.keys():\n",
        "                method_data = [d for d in summary_data if d['method'] == method_name]\n",
        "                if method_data:\n",
        "                    avg_failure_rate = np.mean([d['failure_rate_pct'] for d in method_data])\n",
        "                    method_performance[method_name] = avg_failure_rate\n",
        "\n",
        "            if method_performance:\n",
        "                best_method = min(method_performance, key=method_performance.get)\n",
        "                worst_method = max(method_performance, key=method_performance.get)\n",
        "\n",
        "                print(f\"BEST PERFORMING MEDICAL METHOD: {best_method}\")\n",
        "                print(f\"  Average failure rate: {method_performance[best_method]:.1f}%\")\n",
        "\n",
        "                print(f\"\\nWORST PERFORMING MEDICAL METHOD: {worst_method}\")\n",
        "                print(f\"  Average failure rate: {method_performance[worst_method]:.1f}%\")\n",
        "\n",
        "                print(f\"\\nMEDICAL METHOD RANKING (by average failure rate):\")\n",
        "                sorted_methods = sorted(method_performance.items(), key=lambda x: x[1])\n",
        "                for i, (method, rate) in enumerate(sorted_methods, 1):\n",
        "                    print(f\"  {i}. {method}: {rate:.1f}%\")\n",
        "\n",
        "        # Effect of epsilon on medical data\n",
        "        print(f\"\\nEFFECT OF EPSILON ON MEDICAL DATA:\")\n",
        "        epsilon_performance = {}\n",
        "        for eps in epsilon_values:\n",
        "            eps_data = [d for d in summary_data if d['epsilon'] == eps]\n",
        "            if eps_data:\n",
        "                avg_failure_rate = np.mean([d['failure_rate_pct'] for d in eps_data])\n",
        "                epsilon_performance[eps] = avg_failure_rate\n",
        "\n",
        "        if epsilon_performance:\n",
        "            print(f\"{'Epsilon':<10} {'Avg Failure Rate':<15} {'ρ = 0.5√ε':<12}\")\n",
        "            print(\"-\" * 40)\n",
        "            for eps in sorted(epsilon_performance.keys()):\n",
        "                rho = 0.5 * np.sqrt(eps)\n",
        "                print(f\"{eps:<10} {epsilon_performance[eps]:<15.1f} {rho:<12.6f}\")\n",
        "\n",
        "        return all_results, summary_data\n",
        "\n",
        "    finally:\n",
        "        # Restore original stdout and close log file\n",
        "        sys.stdout = original_stdout\n",
        "        tee.close()\n",
        "        print(f\"Comprehensive medical analysis completed. Results saved to: {log_file}\")\n",
        "\n",
        "\n",
        "# Example usage for medical dataset\n",
        "if __name__ == \"__main__\":\n",
        "    # Load medical dataset and run the analysis\n",
        "    df_medical = pd.read_stata('Licorice Gargle.dta')\n",
        "\n",
        "    # Run comprehensive medical analysis with same parameters as NSW\n",
        "    epsilon_values = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2]\n",
        "\n",
        "    results, summary = run_comprehensive_medical_analysis(\n",
        "        df_medical,\n",
        "        epsilon_values=epsilon_values,\n",
        "        n_trials=30,\n",
        "        log_file=\"postop_comprehensive_analysis_gamma05_heavy16.txt\"\n",
        "    )"
      ]
    }
  ]
}
