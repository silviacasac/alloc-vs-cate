{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spPuIQP-RkTb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "STAR dataset, fixed epsilon method\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import warnings\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "class OutputLogger:\n",
        "    def __init__(self, filename):\n",
        "        \"\"\"\n",
        "        Initialize logger with output file.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        filename : str\n",
        "            Output file path\n",
        "        \"\"\"\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(filename, 'w')\n",
        "\n",
        "    def write(self, message):\n",
        "        \"\"\"Write message to both terminal and file.\"\"\"\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "        self.log.flush()\n",
        "\n",
        "    def flush(self):\n",
        "        \"\"\"Flush both output streams.\"\"\"\n",
        "        self.terminal.flush()\n",
        "        self.log.flush()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close file output stream.\"\"\"\n",
        "        self.log.close()\n",
        "\n",
        "\n",
        "class CATEAllocationAlgorithm:\n",
        "    \"\"\"\n",
        "    Implementation of the theoretical CATE allocation algorithm.\n",
        "\n",
        "    This class implements the allocation algorithm with fixed gamma parameter and\n",
        "    configurable heavy interval threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epsilon=0.1, gamma=0.5, delta=0.05, heavy_multiplier=1.6, random_seed=42):\n",
        "        \"\"\"\n",
        "        Initialize the allocation algorithm.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        epsilon : float, default=0.1\n",
        "            Approximation parameter for (1-ε)-optimal allocation\n",
        "        gamma : float, default=0.5\n",
        "            Fixed parameter controlling relationship between ε and ρ\n",
        "        delta : float, default=0.05\n",
        "            Confidence parameter for Hoeffding's inequality\n",
        "        heavy_multiplier : float, default=1.6\n",
        "            Threshold multiplier for heavy interval detection\n",
        "        random_seed : int, default=42\n",
        "            Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "        self.rho = gamma * np.sqrt(epsilon)\n",
        "        self.delta = delta\n",
        "        self.heavy_multiplier = heavy_multiplier\n",
        "        self.random_seed = random_seed\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "        print(f\"CATE Allocation Algorithm\")\n",
        "        print(f\"ε = {epsilon}\")\n",
        "        print(f\"√ε = {np.sqrt(epsilon):.6f}\")\n",
        "        print(f\"γ = {gamma}\")\n",
        "        print(f\"ρ = γ√ε = {self.rho:.6f}\")\n",
        "        print(f\"Heavy multiplier = {heavy_multiplier}x\")\n",
        "        print(f\"δ = {delta}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    def process_star_data(self, df, outcome_col=None):\n",
        "        \"\"\"\n",
        "        Process STAR dataset.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            Raw STAR dataset\n",
        "        outcome_col : str, optional\n",
        "            Custom outcome column name\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        pandas.DataFrame\n",
        "            Processed dataset ready for analysis\n",
        "        \"\"\"\n",
        "        print(f\"Processing STAR data with {len(df)} observations\")\n",
        "\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Validate required columns\n",
        "        required_cols = ['gkschid', 'gkclasstype']\n",
        "        missing = [col for col in required_cols if col not in df_processed.columns]\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "        # Filter treatment groups\n",
        "        print(f\"Original class type distribution: {df_processed['gkclasstype'].value_counts().to_dict()}\")\n",
        "        df_processed = df_processed[df_processed['gkclasstype'] != 'REGULAR + AIDE CLASS']\n",
        "        print(f\"After excluding aide classes: {len(df_processed)} observations\")\n",
        "\n",
        "        # Create binary treatment indicator\n",
        "        treatment_map = {'SMALL CLASS': 1, 'REGULAR CLASS': 0}\n",
        "        df_processed['treatment'] = df_processed['gkclasstype'].map(treatment_map)\n",
        "\n",
        "        # Construct composite outcome measure\n",
        "        test_components = ['gktreadss', 'gktmathss', 'gktlistss', 'gkwordskillss']\n",
        "        available_components = [col for col in test_components if col in df_processed.columns]\n",
        "\n",
        "        if not available_components:\n",
        "            raise ValueError(\"No test score components found\")\n",
        "\n",
        "        # Remove observations with missing outcomes\n",
        "        initial_size = len(df_processed)\n",
        "        df_processed = df_processed.dropna(subset=available_components)\n",
        "        print(f\"Dropped {initial_size - len(df_processed)} rows due to missing test scores\")\n",
        "\n",
        "        df_processed['total_score'] = df_processed[available_components].sum(axis=1)\n",
        "        df_processed['outcome'] = df_processed['total_score']\n",
        "\n",
        "        # Final data cleaning\n",
        "        initial_size = len(df_processed)\n",
        "        df_processed = df_processed.dropna(subset=['treatment', 'gkschid'])\n",
        "        final_size = len(df_processed)\n",
        "\n",
        "        if initial_size != final_size:\n",
        "            print(f\"Dropped {initial_size - final_size} rows due to missing treatment/school data\")\n",
        "\n",
        "        print(f\"Final dataset: {final_size} students\")\n",
        "        print(f\"Treatment distribution: {df_processed['treatment'].value_counts().to_dict()}\")\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def create_school_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups based on school identifiers.\"\"\"\n",
        "        print(f\"Creating school-based groups (min_size={min_size})\")\n",
        "\n",
        "        groups = []\n",
        "        for school_id in df['gkschid'].unique():\n",
        "            indices = df[df['gkschid'] == school_id].index.tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'school_{school_id}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'school'\n",
        "                })\n",
        "\n",
        "        print(f\"Raw groups created: {len(groups)}\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        print(f\"Balanced groups after filtering: {len(balanced_groups)}\")\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_ml_prediction_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"\n",
        "        Create groups using machine learning-based treatment effect prediction.\n",
        "\n",
        "        Uses separate Random Forest models for treated and control outcomes,\n",
        "        then clusters based on predicted treatment effects and covariates.\n",
        "        \"\"\"\n",
        "        print(f\"Creating ML prediction-based groups (target: {n_groups})\")\n",
        "\n",
        "        # Select baseline features\n",
        "        feature_cols = [col for col in df.columns\n",
        "                       if col not in ['treatment', 'outcome', 'total_score']\n",
        "                       and not col.startswith('gkt')]\n",
        "\n",
        "        X = df[feature_cols].copy()\n",
        "\n",
        "        # Preprocess features\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
        "                X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "            elif X[col].dtype == 'bool':\n",
        "                X[col] = X[col].astype(int)\n",
        "\n",
        "        # Handle missing values\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 0)\n",
        "\n",
        "        # Train separate outcome models\n",
        "        treated_mask = df['treatment'] == 1\n",
        "        control_mask = df['treatment'] == 0\n",
        "\n",
        "        if treated_mask.sum() == 0 or control_mask.sum() == 0:\n",
        "            print(\"Insufficient treated or control observations\")\n",
        "            return []\n",
        "\n",
        "        rf_treated = RandomForestRegressor(n_estimators=100, random_state=self.random_seed)\n",
        "        rf_control = RandomForestRegressor(n_estimators=100, random_state=self.random_seed)\n",
        "\n",
        "        rf_treated.fit(X[treated_mask], df.loc[treated_mask, 'outcome'])\n",
        "        rf_control.fit(X[control_mask], df.loc[control_mask, 'outcome'])\n",
        "\n",
        "        # Predict treatment effects and perform clustering\n",
        "        pred_cate = rf_treated.predict(X) - rf_control.predict(X)\n",
        "        cluster_features = np.column_stack([X.values, pred_cate.reshape(-1, 1)])\n",
        "        cluster_features = StandardScaler().fit_transform(cluster_features)\n",
        "\n",
        "        labels = KMeans(n_clusters=n_groups, random_state=self.random_seed).fit_predict(cluster_features)\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[labels == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'ml_prediction_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'ml_prediction'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} ML prediction-based groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_propensity_groups(self, df, n_groups=50, min_size=6):\n",
        "        \"\"\"Create groups based on propensity score stratification.\"\"\"\n",
        "        print(f\"Creating propensity score groups (target: {n_groups})\")\n",
        "\n",
        "        feature_cols = [col for col in df.columns\n",
        "                       if col not in ['treatment', 'outcome', 'total_score']]\n",
        "\n",
        "        X = df[feature_cols].copy()\n",
        "\n",
        "        # Preprocess features\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
        "                X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "            elif X[col].dtype == 'bool':\n",
        "                X[col] = X[col].astype(int)\n",
        "\n",
        "        # Handle missing values\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 0)\n",
        "\n",
        "        # Estimate propensity scores with cross-validation\n",
        "        prop_scores = cross_val_predict(\n",
        "            LogisticRegression(random_state=self.random_seed),\n",
        "            X, df['treatment'], method='predict_proba', cv=5\n",
        "        )[:, 1]\n",
        "\n",
        "        # Create quantile-based strata\n",
        "        quantiles = np.linspace(0, 1, n_groups + 1)\n",
        "        bins = np.digitize(prop_scores, np.quantile(prop_scores, quantiles)) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'propensity_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'propensity'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} propensity score groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_performance_groups(self, df, n_groups=50, min_size=6):\n",
        "        \"\"\"Create groups based on baseline academic performance.\"\"\"\n",
        "        print(f\"Creating performance groups (target: {n_groups})\")\n",
        "\n",
        "        # Identify baseline test score columns\n",
        "        score_cols = [col for col in df.columns if col.startswith('gkt') and 'ss' in col]\n",
        "        if not score_cols:\n",
        "            print(\"No baseline scores found\")\n",
        "            return []\n",
        "\n",
        "        baseline_score = df[score_cols].fillna(df[score_cols].mean()).mean(axis=1)\n",
        "\n",
        "        # Create percentile-based groups\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(baseline_score, percentiles)\n",
        "        bins = np.digitize(baseline_score, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'performance_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'performance'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} performance groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_demographics_groups(self, df, min_size=6, feature_cols=None):\n",
        "        \"\"\"Create groups based on demographic characteristic combinations.\"\"\"\n",
        "        print(f\"Creating demographics groups\")\n",
        "\n",
        "        if feature_cols is None:\n",
        "            potential_features = ['gkfreelunch', 'race', 'gender', 'birthyear']\n",
        "        else:\n",
        "            potential_features = feature_cols\n",
        "\n",
        "        # Identify available demographic features\n",
        "        available_features = []\n",
        "        for col in potential_features:\n",
        "            if col in df.columns and df[col].notna().sum() > 0:\n",
        "                available_features.append(col)\n",
        "\n",
        "        if len(available_features) == 0:\n",
        "            print(\"No demographic variables found, using school grouping\")\n",
        "            return self.create_school_groups(df, min_size)\n",
        "\n",
        "        print(f\"Using features: {available_features}\")\n",
        "\n",
        "        # Remove observations with missing demographic data\n",
        "        df_clean = df[available_features].dropna()\n",
        "        print(f\"After removing missing values: {len(df_clean)}/{len(df)} students\")\n",
        "\n",
        "        if len(df_clean) == 0:\n",
        "            return self.create_school_groups(df, min_size)\n",
        "\n",
        "        # Create groups based on unique characteristic combinations\n",
        "        unique_combinations = df_clean.drop_duplicates()\n",
        "        print(f\"Found {len(unique_combinations)} unique combinations\")\n",
        "\n",
        "        groups = []\n",
        "        for combo_idx, (idx, combo) in enumerate(unique_combinations.iterrows()):\n",
        "            mask = pd.Series(True, index=df.index)\n",
        "            combo_description = []\n",
        "\n",
        "            for feature in available_features:\n",
        "                mask = mask & (df[feature] == combo[feature])\n",
        "                combo_description.append(f\"{feature}={combo[feature]}\")\n",
        "\n",
        "            indices = df[mask].index.tolist()\n",
        "            combo_id = \"_\".join(combo_description)\n",
        "\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': combo_id,\n",
        "                    'indices': indices,\n",
        "                    'type': 'demographics'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} demographic groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def _ensure_balance_and_compute_cate(self, df, groups):\n",
        "        \"\"\"Apply treatment balance requirements and compute CATEs.\"\"\"\n",
        "        balanced_groups = []\n",
        "\n",
        "        for group in groups:\n",
        "            group_df = df.loc[group['indices']]\n",
        "\n",
        "            treatment_rate = group_df['treatment'].mean()\n",
        "            n_treated = group_df['treatment'].sum()\n",
        "            n_control = len(group_df) - n_treated\n",
        "\n",
        "            # Apply balance and minimum size constraints\n",
        "            if not (0.15 <= treatment_rate <= 0.85 and n_treated >= 3 and n_control >= 3):\n",
        "                continue\n",
        "\n",
        "            # Compute CATE as difference in means\n",
        "            treated_outcomes = group_df[group_df['treatment'] == 1]['outcome']\n",
        "            control_outcomes = group_df[group_df['treatment'] == 0]['outcome']\n",
        "            cate = treated_outcomes.mean() - control_outcomes.mean()\n",
        "\n",
        "            balanced_groups.append({\n",
        "                'id': group['id'],\n",
        "                'indices': group['indices'],\n",
        "                'size': len(group_df),\n",
        "                'treatment_rate': treatment_rate,\n",
        "                'n_treated': int(n_treated),\n",
        "                'n_control': int(n_control),\n",
        "                'cate': cate,\n",
        "                'type': group['type']\n",
        "            })\n",
        "\n",
        "        return balanced_groups\n",
        "\n",
        "    def normalize_cates(self, groups):\n",
        "        \"\"\"Normalize CATE values to [0,1] interval.\"\"\"\n",
        "        cates = [g['cate'] for g in groups]\n",
        "        min_cate, max_cate = min(cates), max(cates)\n",
        "\n",
        "        if max_cate > min_cate:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = (group['cate'] - min_cate) / (max_cate - min_cate)\n",
        "        else:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = 0.5\n",
        "\n",
        "        print(f\"CATE normalization: [{min_cate:.3f}, {max_cate:.3f}] → [0, 1]\")\n",
        "        return groups\n",
        "\n",
        "    def plot_cate_distribution(self, groups, title_suffix=\"\"):\n",
        "        \"\"\"Visualize CATE distribution before and after normalization.\"\"\"\n",
        "        original_cates = [g['cate'] for g in groups]\n",
        "        normalized_cates = [g['normalized_cate'] for g in groups]\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "        ax1.hist(original_cates, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        ax1.set_xlabel('Original CATE')\n",
        "        ax1.set_ylabel('Frequency')\n",
        "        ax1.set_title(f'Original CATE Distribution{title_suffix}')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        ax2.hist(normalized_cates, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "        ax2.set_xlabel('Normalized CATE (τ)')\n",
        "        ax2.set_ylabel('Frequency')\n",
        "        ax2.set_title(f'Normalized CATE Distribution{title_suffix}')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def estimate_tau(self, true_tau, accuracy):\n",
        "        \"\"\"\n",
        "        Estimate tau using Hoeffding's inequality with Bernoulli samples.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        true_tau : float\n",
        "            True normalized CATE value\n",
        "        accuracy : float\n",
        "            Desired estimation accuracy\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            (estimate, sample_size) pair\n",
        "        \"\"\"\n",
        "        sample_size = int(np.ceil(np.log(2/self.delta) / (2 * accuracy**2)))\n",
        "        samples = np.random.binomial(1, true_tau, sample_size)\n",
        "        return np.mean(samples), sample_size\n",
        "\n",
        "    def run_single_trial(self, groups, epsilon_val, trial_seed):\n",
        "        \"\"\"\n",
        "        Execute one trial of the allocation algorithm.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        groups : list\n",
        "            List of groups with normalized CATEs\n",
        "        epsilon_val : float\n",
        "            Current epsilon value for analysis\n",
        "        trial_seed : int\n",
        "            Trial-specific random seed\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            (trial_results, tau_estimates) containing performance metrics\n",
        "        \"\"\"\n",
        "        np.random.seed(self.random_seed + trial_seed)\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "        rho = self.gamma * np.sqrt(epsilon_val)\n",
        "\n",
        "        # Estimate all tau values using rho-level accuracy\n",
        "        tau_estimates_rho = []\n",
        "        for tau in tau_true:\n",
        "            estimate, _ = self.estimate_tau(tau, rho)\n",
        "            tau_estimates_rho.append(estimate)\n",
        "        tau_estimates_rho = np.array(tau_estimates_rho)\n",
        "\n",
        "        # Also estimate using epsilon-level accuracy for comparison\n",
        "        tau_estimates_eps = []\n",
        "        for tau in tau_true:\n",
        "            estimate, _ = self.estimate_tau(tau, epsilon_val)\n",
        "            tau_estimates_eps.append(estimate)\n",
        "        tau_estimates_eps = np.array(tau_estimates_eps)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # Evaluate all possible budget levels\n",
        "        for K in range(1, n_groups):\n",
        "            # Compute optimal allocation\n",
        "            optimal_indices = np.argsort(tau_true)[-K:]\n",
        "            optimal_value = np.sum(tau_true[optimal_indices])\n",
        "\n",
        "            # Compute algorithm allocations\n",
        "            rho_indices = np.argsort(tau_estimates_rho)[-K:]\n",
        "            rho_value = np.sum(tau_true[rho_indices])\n",
        "\n",
        "            eps_indices = np.argsort(tau_estimates_eps)[-K:]\n",
        "            eps_value = np.sum(tau_true[eps_indices])\n",
        "\n",
        "            # Calculate performance ratios\n",
        "            rho_ratio = rho_value / optimal_value if optimal_value > 0 else 0\n",
        "            eps_ratio = eps_value / optimal_value if optimal_value > 0 else 0\n",
        "            rho_success = rho_ratio >= (1 - epsilon_val)\n",
        "            eps_success = eps_ratio >= (1 - epsilon_val)\n",
        "\n",
        "            # Detect heavy intervals\n",
        "            tau_k_est = tau_estimates_rho[rho_indices[0]]\n",
        "            a2_lower = tau_k_est\n",
        "            a2_upper = tau_k_est + 2 * rho\n",
        "            units_in_a2 = np.sum((tau_estimates_rho >= a2_lower) & (tau_estimates_rho <= a2_upper))\n",
        "            expected_a2 = 2 * rho * n_groups\n",
        "            is_heavy = units_in_a2 > self.heavy_multiplier * expected_a2\n",
        "\n",
        "            results.append({\n",
        "                'K': K,\n",
        "                'optimal_value': optimal_value,\n",
        "                'rho_value': rho_value,\n",
        "                'eps_value': eps_value,\n",
        "                'rho_ratio': rho_ratio,\n",
        "                'eps_ratio': eps_ratio,\n",
        "                'rho_success': rho_success,\n",
        "                'eps_success': eps_success,\n",
        "                'is_heavy': is_heavy,\n",
        "                'tau_k_est': tau_k_est,\n",
        "                'units_in_a2': units_in_a2\n",
        "            })\n",
        "\n",
        "        return results, tau_estimates_rho\n",
        "\n",
        "    def find_recovery_units(self, K, tau_true, tau_estimates, epsilon_val):\n",
        "        \"\"\"Determine additional units needed to achieve target performance.\"\"\"\n",
        "        n_groups = len(tau_true)\n",
        "\n",
        "        # Current allocation using rho estimates\n",
        "        rho_indices = np.argsort(tau_estimates)[-K:]\n",
        "        optimal_value = np.sum(tau_true[np.argsort(tau_true)[-K:]])\n",
        "\n",
        "        # Remaining candidates sorted by estimated quality\n",
        "        remaining_indices = np.argsort(tau_estimates)[:-K][::-1]\n",
        "\n",
        "        # Test incremental expansion\n",
        "        for extra in range(1, 11):\n",
        "            if extra > len(remaining_indices):\n",
        "                break\n",
        "\n",
        "            expanded_indices = np.concatenate([rho_indices, remaining_indices[:extra]])\n",
        "            expanded_value = np.sum(tau_true[expanded_indices])\n",
        "\n",
        "            if expanded_value / optimal_value >= (1 - epsilon_val):\n",
        "                return extra\n",
        "\n",
        "        return None\n",
        "\n",
        "    def find_closest_working_budget(self, failed_K, trial_results):\n",
        "        \"\"\"Find proximity to successful budget levels.\"\"\"\n",
        "        working_budgets = [r['K'] for r in trial_results if r['rho_success']]\n",
        "\n",
        "        if not working_budgets:\n",
        "            return None, None\n",
        "\n",
        "        # Distance to any working budget\n",
        "        distances_any = [abs(K - failed_K) for K in working_budgets]\n",
        "        min_distance_any = min(distances_any)\n",
        "\n",
        "        # Distance to smaller working budget\n",
        "        smaller_working = [K for K in working_budgets if K < failed_K]\n",
        "        if smaller_working:\n",
        "            min_distance_smaller = failed_K - max(smaller_working)\n",
        "        else:\n",
        "            min_distance_smaller = None\n",
        "\n",
        "        return min_distance_any, min_distance_smaller\n",
        "\n",
        "    def analyze_method(self, groups, epsilon_val, n_trials=30):\n",
        "        \"\"\"\n",
        "        Analyze algorithm performance for a single grouping method.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        groups : list\n",
        "            List of groups with normalized CATEs\n",
        "        epsilon_val : float\n",
        "            Current epsilon value\n",
        "        n_trials : int, default=30\n",
        "            Number of trials to run\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        list\n",
        "            Trial data with performance metrics\n",
        "        \"\"\"\n",
        "        print(f\"\\nAnalyzing {len(groups)} groups with ε={epsilon_val}, γ={self.gamma}\")\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "\n",
        "        trial_data = []\n",
        "\n",
        "        for trial in range(n_trials):\n",
        "            print(f\"Trial {trial + 1}/{n_trials}...\")\n",
        "\n",
        "            # Execute single trial\n",
        "            trial_results, tau_estimates = self.run_single_trial(groups, epsilon_val, trial)\n",
        "\n",
        "            # Analyze failure patterns\n",
        "            failed_results = [r for r in trial_results if not r['rho_success']]\n",
        "            failed_budgets = [r['K'] for r in failed_results]\n",
        "\n",
        "            # Heavy interval analysis with both estimated and true values\n",
        "            failed_heavy_estimated = []\n",
        "            failed_heavy_true = []\n",
        "            rho = self.gamma * np.sqrt(epsilon_val)\n",
        "\n",
        "            for failed_result in failed_results:\n",
        "                K = failed_result['K']\n",
        "                failed_heavy_estimated.append(failed_result['is_heavy'])\n",
        "\n",
        "                # Check heavy intervals using true tau values\n",
        "                tau_k_true = tau_true[np.argsort(tau_true)[-K:]][0]\n",
        "                a2_lower_true = tau_k_true\n",
        "                a2_upper_true = tau_k_true + 2 * rho\n",
        "                units_in_a2_true = np.sum((tau_true >= a2_lower_true) & (tau_true <= a2_upper_true))\n",
        "                expected_a2_true = 2 * rho * n_groups\n",
        "                is_heavy_true = units_in_a2_true > self.heavy_multiplier * expected_a2_true\n",
        "                failed_heavy_true.append(is_heavy_true)\n",
        "\n",
        "            # Report trial summary\n",
        "            print(f\"  Failed budgets: {failed_budgets}\")\n",
        "\n",
        "            if len(failed_budgets) > 0:\n",
        "                estimated_clean = [bool(x) for x in failed_heavy_estimated]\n",
        "                true_clean = [bool(x) for x in failed_heavy_true]\n",
        "                print(f\"  Heavy intervals - Estimated: {estimated_clean}\")\n",
        "                print(f\"  Heavy intervals - True τ_K:   {true_clean}\")\n",
        "\n",
        "            # Compute aggregate statistics\n",
        "            total_heavy = sum(r['is_heavy'] for r in trial_results)\n",
        "            failed_heavy = sum(r['is_heavy'] for r in failed_results)\n",
        "\n",
        "            # Recovery analysis\n",
        "            recovery_units = []\n",
        "            distances_to_working_any = []\n",
        "            distances_to_working_smaller = []\n",
        "\n",
        "            for failed_result in failed_results:\n",
        "                K = failed_result['K']\n",
        "\n",
        "                # Compute recovery requirements\n",
        "                recovery = self.find_recovery_units(K, tau_true, tau_estimates, epsilon_val)\n",
        "                if recovery is not None:\n",
        "                    recovery_units.append(recovery)\n",
        "\n",
        "                # Compute distances to working budgets\n",
        "                distance_any, distance_smaller = self.find_closest_working_budget(K, trial_results)\n",
        "                if distance_any is not None:\n",
        "                    distances_to_working_any.append(distance_any)\n",
        "                if distance_smaller is not None:\n",
        "                    distances_to_working_smaller.append(distance_smaller)\n",
        "\n",
        "            trial_info = {\n",
        "                'trial': trial,\n",
        "                'failed_budgets': failed_budgets,\n",
        "                'num_failures': len(failed_results),\n",
        "                'total_heavy': total_heavy,\n",
        "                'failed_heavy': failed_heavy,\n",
        "                'failed_heavy_estimated': failed_heavy_estimated,\n",
        "                'failed_heavy_true': failed_heavy_true,\n",
        "                'recovery_units': recovery_units,\n",
        "                'distances_to_working_any': distances_to_working_any,\n",
        "                'distances_to_working_smaller': distances_to_working_smaller\n",
        "            }\n",
        "\n",
        "            trial_data.append(trial_info)\n",
        "\n",
        "            # Print trial metrics\n",
        "            print(f\"  Failures: {len(failed_results)}, Total heavy: {total_heavy}, Failed heavy: {failed_heavy}\")\n",
        "            if recovery_units:\n",
        "                print(f\"  Recovery units: μ={np.mean(recovery_units):.1f}, med={np.median(recovery_units):.0f}, max={np.max(recovery_units)}\")\n",
        "            if distances_to_working_any:\n",
        "                print(f\"  Distance any: μ={np.mean(distances_to_working_any):.1f}, med={np.median(distances_to_working_any):.0f}, max={np.max(distances_to_working_any)}\")\n",
        "            if distances_to_working_smaller:\n",
        "                print(f\"  Distance smaller: μ={np.mean(distances_to_working_smaller):.1f}, med={np.median(distances_to_working_smaller):.0f}, max={np.max(distances_to_working_smaller)}\")\n",
        "            else:\n",
        "                print(f\"  Distance smaller: No smaller working budgets found\")\n",
        "\n",
        "        return trial_data\n",
        "\n",
        "    def print_method_summary(self, method_name, trial_data, n_groups):\n",
        "        \"\"\"Generate comprehensive summary statistics for a method.\"\"\"\n",
        "        print(f\"\\n{'='*100}\")\n",
        "        print(f\"SUMMARY - {method_name} - ALL BUDGETS\")\n",
        "        print(\"=\"*100)\n",
        "        print(f\"{'Fail μ':<7} {'Fail σ':<7} {'FailR% μ':<9} {'FailR% σ':<9} {'TotHvy':<8} {'FailHvy':<9} {'Rec μ':<7} {'Rec med':<8} {'Rec max':<8} {'DAny μ':<8} {'DAny σ':<10} {'DAny max':<10} {'DSmall μ':<10} {'DSmall σ':<12} {'DSmall max':<12}\")\n",
        "        print(\"-\"*120)\n",
        "\n",
        "        # Aggregate statistics across trials\n",
        "        all_failures = [t['num_failures'] for t in trial_data]\n",
        "        all_total_heavy = [t['total_heavy'] for t in trial_data]\n",
        "        all_failed_heavy = [t['failed_heavy'] for t in trial_data]\n",
        "        all_recovery = []\n",
        "        all_distances_any = []\n",
        "        all_distances_smaller = []\n",
        "\n",
        "        for t in trial_data:\n",
        "            all_recovery.extend(t['recovery_units'])\n",
        "            all_distances_any.extend(t['distances_to_working_any'])\n",
        "            all_distances_smaller.extend(t['distances_to_working_smaller'])\n",
        "\n",
        "        # Compute summary statistics\n",
        "        avg_failures = np.mean(all_failures)\n",
        "        std_failures = np.std(all_failures)\n",
        "        avg_failure_rate = avg_failures / (n_groups - 1) * 100\n",
        "        std_failure_rate = std_failures / (n_groups - 1) * 100\n",
        "        avg_total_heavy = np.mean(all_total_heavy)\n",
        "        avg_failed_heavy = np.mean(all_failed_heavy)\n",
        "\n",
        "        # Recovery statistics\n",
        "        if all_recovery:\n",
        "            recovery_mean = np.mean(all_recovery)\n",
        "            recovery_med = np.median(all_recovery)\n",
        "            recovery_max = np.max(all_recovery)\n",
        "        else:\n",
        "            recovery_mean = recovery_med = recovery_max = np.nan\n",
        "\n",
        "        # Distance statistics - any direction\n",
        "        if all_distances_any:\n",
        "            distance_any_mean = np.mean(all_distances_any)\n",
        "            distance_any_std = np.std(all_distances_any)\n",
        "            distance_any_max = np.max(all_distances_any)\n",
        "        else:\n",
        "            distance_any_mean = distance_any_std = distance_any_max = np.nan\n",
        "\n",
        "        # Distance statistics - smaller budgets only\n",
        "        if all_distances_smaller:\n",
        "            distance_smaller_mean = np.mean(all_distances_smaller)\n",
        "            distance_smaller_std = np.std(all_distances_smaller)\n",
        "            distance_smaller_max = np.max(all_distances_smaller)\n",
        "        else:\n",
        "            distance_smaller_mean = distance_smaller_std = distance_smaller_max = np.nan\n",
        "\n",
        "        print(f\"{avg_failures:<7.1f} {std_failures:<7.1f} {avg_failure_rate:<9.1f} {std_failure_rate:<9.1f} {avg_total_heavy:<8.1f} {avg_failed_heavy:<9.1f} \"\n",
        "              f\"{recovery_mean:<7.1f} {recovery_med:<8.0f} {recovery_max:<8.0f} \"\n",
        "              f\"{distance_any_mean:<8.1f} {distance_any_std:<10.1f} {distance_any_max:<10.0f} \"\n",
        "              f\"{distance_smaller_mean:<10.1f} {distance_smaller_std:<12.1f} {distance_smaller_max:<12.0f}\")\n",
        "\n",
        "        return {\n",
        "            'avg_failures': avg_failures,\n",
        "            'failure_rate_pct': avg_failure_rate,\n",
        "            'avg_recovery': recovery_mean,\n",
        "            'n_groups': n_groups\n",
        "        }\n",
        "\n",
        "\n",
        "def run_comprehensive_analysis(df_star, epsilon_values=None, n_trials=30, log_file=None):\n",
        "    \"\"\"\n",
        "    Execute comprehensive analysis across all grouping methods.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df_star : pandas.DataFrame\n",
        "        STAR dataset\n",
        "    epsilon_values : list, optional\n",
        "        Epsilon values to test\n",
        "    n_trials : int, default=30\n",
        "        Number of trials per configuration\n",
        "    log_file : str, optional\n",
        "        Output log file path\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (all_results, summary_data) containing comprehensive analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    if epsilon_values is None:\n",
        "        epsilon_values = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2]\n",
        "\n",
        "    # Configure logging\n",
        "    if log_file is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        log_file = f\"comprehensive_analysis_gamma05_{timestamp}.txt\"\n",
        "\n",
        "    # Redirect output to both console and file\n",
        "    original_stdout = sys.stdout\n",
        "    logger = OutputLogger(log_file)\n",
        "    sys.stdout = logger\n",
        "\n",
        "    try:\n",
        "        print(\"COMPREHENSIVE ALGORITHMIC ANALYSIS\")\n",
        "        print(f\"Configuration: γ=0.5, Heavy threshold=1.6x\")\n",
        "        print(f\"Log file: {log_file}\")\n",
        "        print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        # Define grouping methods for analysis\n",
        "        methods = [\n",
        "            ('School', lambda allocator, df: allocator.create_school_groups(df, min_size=6)),\n",
        "            ('Demographics', lambda allocator, df: allocator.create_demographics_groups(df,\n",
        "                                                   feature_cols=['gkfreelunch', 'race', 'gender'], min_size=6)),\n",
        "            ('ML Prediction 30', lambda allocator, df: allocator.create_ml_prediction_groups(df, n_groups=30, min_size=6)),\n",
        "            ('ML Prediction 50', lambda allocator, df: allocator.create_ml_prediction_groups(df, n_groups=50, min_size=6)),\n",
        "            ('Propensity Score', lambda allocator, df: allocator.create_propensity_groups(df, n_groups=50, min_size=6)),\n",
        "            ('Performance', lambda allocator, df: allocator.create_performance_groups(df, n_groups=50, min_size=6))\n",
        "        ]\n",
        "\n",
        "        all_results = {}\n",
        "\n",
        "        for method_name, method_func in methods:\n",
        "            print(f\"\\n{'='*120}\")\n",
        "            print(f\"ANALYZING METHOD: {method_name}\")\n",
        "            print(\"=\"*120)\n",
        "\n",
        "            method_results = []\n",
        "\n",
        "            for eps in epsilon_values:\n",
        "                print(f\"\\n{'='*100}\")\n",
        "                print(f\"METHOD: {method_name} | EPSILON = {eps}\")\n",
        "                print(\"=\"*100)\n",
        "\n",
        "                # Initialize algorithm with current parameters\n",
        "                allocator = CATEAllocationAlgorithm(epsilon=eps, gamma=0.5, heavy_multiplier=1.6)\n",
        "                df_processed = allocator.process_star_data(df_star)\n",
        "\n",
        "                try:\n",
        "                    # Create groups using current method\n",
        "                    groups = method_func(allocator, df_processed)\n",
        "\n",
        "                    if len(groups) < 3:\n",
        "                        print(f\"Insufficient groups ({len(groups)}) for {method_name} with ε = {eps} - skipping\")\n",
        "                        continue\n",
        "\n",
        "                    groups = allocator.normalize_cates(groups)\n",
        "\n",
        "                    # Display CATE distribution\n",
        "                    allocator.plot_cate_distribution(groups, f\" ({method_name}, ε={eps})\")\n",
        "\n",
        "                    # Execute algorithmic analysis\n",
        "                    trial_data = allocator.analyze_method(groups, eps, n_trials)\n",
        "\n",
        "                    # Generate method summary\n",
        "                    stats = allocator.print_method_summary(method_name, trial_data, len(groups))\n",
        "\n",
        "                    epsilon_result = {\n",
        "                        'method': method_name,\n",
        "                        'epsilon': eps,\n",
        "                        'sqrt_epsilon': np.sqrt(eps),\n",
        "                        'gamma': 0.5,\n",
        "                        'rho': 0.5 * np.sqrt(eps),\n",
        "                        'groups': groups,\n",
        "                        'trial_data': trial_data,\n",
        "                        'stats': stats\n",
        "                    }\n",
        "\n",
        "                    method_results.append(epsilon_result)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error with {method_name} at ε = {eps}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            all_results[method_name] = method_results\n",
        "\n",
        "        # Generate comprehensive summary\n",
        "        print(f\"\\n{'='*200}\")\n",
        "        print(\"COMPREHENSIVE SUMMARY - ALL METHODS AND EPSILON VALUES\")\n",
        "        print(\"=\"*200)\n",
        "\n",
        "        summary_data = []\n",
        "\n",
        "        for method_name, method_results in all_results.items():\n",
        "            if not method_results:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n{'-'*100}\")\n",
        "            print(f\"METHOD: {method_name}\")\n",
        "            print(\"-\"*100)\n",
        "\n",
        "            for eps_result in method_results:\n",
        "                eps = eps_result['epsilon']\n",
        "                sqrt_eps = eps_result['sqrt_epsilon']\n",
        "                gamma = eps_result['gamma']\n",
        "                rho = eps_result['rho']\n",
        "                n_groups = len(eps_result['groups'])\n",
        "                stats = eps_result['stats']\n",
        "\n",
        "                summary_data.append({\n",
        "                    'method': method_name,\n",
        "                    'epsilon': eps,\n",
        "                    'sqrt_eps': sqrt_eps,\n",
        "                    'gamma': gamma,\n",
        "                    'rho': rho,\n",
        "                    'avg_failures': stats['avg_failures'],\n",
        "                    'failure_rate_pct': stats['failure_rate_pct'],\n",
        "                    'avg_recovery': stats['avg_recovery'],\n",
        "                    'n_groups': stats['n_groups']\n",
        "                })\n",
        "\n",
        "            # Print method-specific table\n",
        "            method_data = [d for d in summary_data if d['method'] == method_name]\n",
        "            if method_data:\n",
        "                print(f\"{'ε':<8} {'√ε':<10} {'γ':<6} {'ρ':<10} {'Groups':<8} {'Fail μ':<8} {'FailR%':<8} {'Rec μ':<8}\")\n",
        "                print(\"-\" * 80)\n",
        "\n",
        "                for data in method_data:\n",
        "                    print(f\"{data['epsilon']:<8} {data['sqrt_eps']:<10.6f} {data['gamma']:<6} {data['rho']:<10.6f} \"\n",
        "                          f\"{data['n_groups']:<8} {data['avg_failures']:<8.1f} {data['failure_rate_pct']:<8.1f} \"\n",
        "                          f\"{data['avg_recovery']:<8.1f}\")\n",
        "\n",
        "        # Overall summary table\n",
        "        print(f\"\\n{'='*200}\")\n",
        "        print(\"OVERALL SUMMARY TABLE - ALL METHODS COMBINED\")\n",
        "        print(\"=\"*200)\n",
        "        print(f\"{'Method':<18} {'ε':<8} {'√ε':<10} {'γ':<6} {'ρ':<10} {'Groups':<8} {'Fail μ':<8} {'FailR%':<8} {'Rec μ':<8}\")\n",
        "        print(\"-\" * 120)\n",
        "\n",
        "        for data in summary_data:\n",
        "            print(f\"{data['method']:<18} {data['epsilon']:<8} {data['sqrt_eps']:<10.6f} {data['gamma']:<6} {data['rho']:<10.6f} \"\n",
        "                  f\"{data['n_groups']:<8} {data['avg_failures']:<8.1f} {data['failure_rate_pct']:<8.1f} \"\n",
        "                  f\"{data['avg_recovery']:<8.1f}\")\n",
        "\n",
        "        # Generate key insights\n",
        "        print(f\"\\n{'='*100}\")\n",
        "        print(\"KEY INSIGHTS\")\n",
        "        print(\"=\"*100)\n",
        "\n",
        "        if summary_data:\n",
        "            # Method performance ranking\n",
        "            method_performance = {}\n",
        "            for method_name in all_results.keys():\n",
        "                method_data = [d for d in summary_data if d['method'] == method_name]\n",
        "                if method_data:\n",
        "                    avg_failure_rate = np.mean([d['failure_rate_pct'] for d in method_data])\n",
        "                    method_performance[method_name] = avg_failure_rate\n",
        "\n",
        "            if method_performance:\n",
        "                best_method = min(method_performance, key=method_performance.get)\n",
        "                worst_method = max(method_performance, key=method_performance.get)\n",
        "\n",
        "                print(f\"Best performing method: {best_method}\")\n",
        "                print(f\"  Average failure rate: {method_performance[best_method]:.1f}%\")\n",
        "\n",
        "                print(f\"\\nWorst performing method: {worst_method}\")\n",
        "                print(f\"  Average failure rate: {method_performance[worst_method]:.1f}%\")\n",
        "\n",
        "                print(f\"\\nMethod ranking (by average failure rate):\")\n",
        "                sorted_methods = sorted(method_performance.items(), key=lambda x: x[1])\n",
        "                for i, (method, rate) in enumerate(sorted_methods, 1):\n",
        "                    print(f\"  {i}. {method}: {rate:.1f}%\")\n",
        "\n",
        "        # Epsilon effect analysis\n",
        "        print(f\"\\nEffect of epsilon parameter:\")\n",
        "        epsilon_performance = {}\n",
        "        for eps in epsilon_values:\n",
        "            eps_data = [d for d in summary_data if d['epsilon'] == eps]\n",
        "            if eps_data:\n",
        "                avg_failure_rate = np.mean([d['failure_rate_pct'] for d in eps_data])\n",
        "                epsilon_performance[eps] = avg_failure_rate\n",
        "\n",
        "        if epsilon_performance:\n",
        "            print(f\"{'Epsilon':<10} {'Avg Failure Rate':<15} {'ρ = 0.5√ε':<12}\")\n",
        "            print(\"-\" * 40)\n",
        "            for eps in sorted(epsilon_performance.keys()):\n",
        "                rho = 0.5 * np.sqrt(eps)\n",
        "                print(f\"{eps:<10} {epsilon_performance[eps]:<15.1f} {rho:<12.6f}\")\n",
        "\n",
        "        print(f\"\\nAnalysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"Results saved to: {log_file}\")\n",
        "        print(f\"\\nConfiguration summary:\")\n",
        "        print(f\"- Fixed γ = 0.5\")\n",
        "        print(f\"- Heavy interval threshold = 1.6× uniform expectation\")\n",
        "        print(f\"- Trials per configuration = {n_trials}\")\n",
        "        print(f\"- Methods tested = {len(methods)}\")\n",
        "        print(f\"- Epsilon values tested = {len(epsilon_values)}\")\n",
        "\n",
        "        return all_results, summary_data\n",
        "\n",
        "    finally:\n",
        "        # Restore original stdout and close log\n",
        "        sys.stdout = original_stdout\n",
        "        logger.close()\n",
        "        print(f\"Comprehensive analysis completed. Results saved to: {log_file}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load STAR dataset\n",
        "    df_star = pd.read_spss('STAR_Students.sav')\n",
        "\n",
        "    # Configure analysis parameters\n",
        "    epsilon_values = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2]\n",
        "\n",
        "    # Execute comprehensive analysis\n",
        "    results, summary = run_comprehensive_analysis(\n",
        "        df_star,\n",
        "        epsilon_values=epsilon_values,\n",
        "        n_trials=30,\n",
        "        log_file=\"algorithmic_analysis_gamma05_heavy16.txt\"\n",
        "    )\n",
        "\n",
        "    print(f\"Analyzed {len(results)} grouping methods across {len(epsilon_values)} epsilon values.\")\n",
        "    print(f\"Total configurations: {sum(len(method_results) for method_results in results.values())}\")"
      ]
    }
  ]
}
