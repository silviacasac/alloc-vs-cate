{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQdw8FhDhet5"
      },
      "outputs": [],
      "source": [
        "# POSTOP CATE ANALYSIS -- theoretical bound\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import warnings\n",
        "import sys\n",
        "from datetime import datetime\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def theoretical_bound(m, beta, N, delta, OPT):\n",
        "    \"\"\"Compute theoretical bound (1 - (N*ln(2N/delta)/m)^beta) * OPT\"\"\"\n",
        "    term = N * np.log(2 * N / delta) / m\n",
        "    if term >= 1:\n",
        "        return 0  # Bound becomes meaningless\n",
        "    return (1 - term**beta) * OPT\n",
        "\n",
        "class TeeOutput:\n",
        "    \"\"\"Class to write output to both console and file simultaneously.\"\"\"\n",
        "    def __init__(self, filename):\n",
        "        self.terminal = sys.stdout\n",
        "        self.log = open(filename, 'w')\n",
        "\n",
        "    def write(self, message):\n",
        "        self.terminal.write(message)\n",
        "        self.log.write(message)\n",
        "        self.log.flush()\n",
        "\n",
        "    def flush(self):\n",
        "        self.terminal.flush()\n",
        "        self.log.flush()\n",
        "\n",
        "    def close(self):\n",
        "        self.log.close()\n",
        "\n",
        "class MedicalSampleSizeAnalyzer:\n",
        "    \"\"\"Medical CATE allocation with sample size analysis and theoretical bounds.\"\"\"\n",
        "\n",
        "    def __init__(self, random_seed=42):\n",
        "        self.random_seed = random_seed\n",
        "        np.random.seed(random_seed)\n",
        "        print(f\"Medical Sample Size Analyzer initialized with seed {random_seed}\")\n",
        "\n",
        "    def process_medical_data(self, df, outcome_col='postop4hour_throatpain', treatment_col='treat'):\n",
        "        \"\"\"Process medical dataset\"\"\"\n",
        "        print(f\"Processing medical data with {len(df)} patients\")\n",
        "        print(f\"Available columns: {list(df.columns)}\")\n",
        "\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Check for required columns\n",
        "        if treatment_col not in df_processed.columns:\n",
        "            raise ValueError(f\"Missing required treatment column: {treatment_col}\")\n",
        "        if outcome_col not in df_processed.columns:\n",
        "            raise ValueError(f\"Missing required outcome column: {outcome_col}\")\n",
        "\n",
        "        # Set up treatment and outcome\n",
        "        df_processed['treatment'] = df_processed[treatment_col]\n",
        "        df_processed['outcome'] = df_processed[outcome_col]\n",
        "\n",
        "        # Create baseline risk using preoperative pain if available\n",
        "        if 'preop_pain' in df_processed.columns:\n",
        "            df_processed['baseline_risk'] = df_processed['preop_pain']\n",
        "        else:\n",
        "            df_processed['baseline_risk'] = 0\n",
        "\n",
        "        # Clean data\n",
        "        initial_size = len(df_processed)\n",
        "        df_processed = df_processed.dropna(subset=['outcome', 'treatment'])\n",
        "        final_size = len(df_processed)\n",
        "\n",
        "        if initial_size != final_size:\n",
        "            print(f\"Dropped {initial_size - final_size} rows due to missing outcome/treatment\")\n",
        "\n",
        "        print(f\"Final dataset: {final_size} patients\")\n",
        "        print(f\"Treatment distribution: {df_processed['treatment'].value_counts().to_dict()}\")\n",
        "        print(f\"Outcome (4-hour throat pain) statistics: mean={df_processed['outcome'].mean():.2f}, std={df_processed['outcome'].std():.2f}\")\n",
        "\n",
        "        if 'baseline_risk' in df_processed.columns:\n",
        "            print(f\"Baseline (preop pain) stats: mean={df_processed['baseline_risk'].mean():.2f}, std={df_processed['baseline_risk'].std():.2f}\")\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def create_demographics_groups(self, df, min_size=6):\n",
        "        \"\"\"Create groups by demographics\"\"\"\n",
        "        print(f\"Creating medical demographics groups\")\n",
        "\n",
        "        demo_features = ['preop_gender', 'preop_smoking', 'preop_asa']\n",
        "        available_features = [col for col in demo_features if col in df.columns]\n",
        "\n",
        "        if not available_features:\n",
        "            print(\"No demographic variables found\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Using demographic features: {available_features}\")\n",
        "        if len(available_features) > 3:\n",
        "            available_features = available_features[:3]\n",
        "\n",
        "        df_clean = df.dropna(subset=available_features)\n",
        "        print(f\"After removing missing values: {len(df_clean)}/{len(df)} patients\")\n",
        "\n",
        "        if len(df_clean) == 0:\n",
        "            return []\n",
        "\n",
        "        groups = []\n",
        "        unique_combinations = df_clean[available_features].drop_duplicates()\n",
        "        print(f\"Found {len(unique_combinations)} unique demographic combinations\")\n",
        "\n",
        "        for combo_idx, (idx, combo) in enumerate(unique_combinations.iterrows()):\n",
        "            mask = pd.Series(True, index=df.index)\n",
        "            combo_description = []\n",
        "\n",
        "            for feature in available_features:\n",
        "                mask = mask & (df[feature] == combo[feature])\n",
        "                combo_description.append(f\"{feature}={combo[feature]}\")\n",
        "\n",
        "            indices = df[mask].index.tolist()\n",
        "            combo_id = \"_\".join(combo_description)\n",
        "\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': combo_id,\n",
        "                    'indices': indices,\n",
        "                    'type': 'demographics'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} demographic groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_age_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create age groups\"\"\"\n",
        "        print(f\"Creating age groups (target: {n_groups})\")\n",
        "\n",
        "        if 'preop_age' not in df.columns:\n",
        "            print(\"No age variable found\")\n",
        "            return []\n",
        "\n",
        "        age = df['preop_age'].fillna(df['preop_age'].median())\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(age, percentiles)\n",
        "        bins = np.digitize(age, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'age_group_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'age'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} age groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_bmi_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create BMI groups\"\"\"\n",
        "        print(f\"Creating BMI groups (target: {n_groups})\")\n",
        "\n",
        "        if 'preop_calcbmi' not in df.columns:\n",
        "            print(\"No BMI variable found\")\n",
        "            return []\n",
        "\n",
        "        bmi = df['preop_calcbmi'].fillna(df['preop_calcbmi'].median())\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(bmi, percentiles)\n",
        "        bins = np.digitize(bmi, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'bmi_group_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'bmi'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} BMI groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_preop_pain_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create preop pain groups\"\"\"\n",
        "        print(f\"Creating preoperative pain groups (target: {n_groups})\")\n",
        "\n",
        "        if 'preop_pain' not in df.columns:\n",
        "            print(\"No preoperative pain data available\")\n",
        "            return []\n",
        "\n",
        "        pain = df['preop_pain'].fillna(0)\n",
        "\n",
        "        if (pain == 0).mean() > 0.3:\n",
        "            zero_pain = df.index[pain == 0].tolist()\n",
        "            groups = []\n",
        "            if len(zero_pain) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': 'no_preop_pain',\n",
        "                    'indices': zero_pain,\n",
        "                    'type': 'preop_pain'\n",
        "                })\n",
        "\n",
        "            positive_pain = pain[pain > 0]\n",
        "            if len(positive_pain) > 0:\n",
        "                percentiles = np.linspace(0, 100, n_groups)\n",
        "                cuts = np.percentile(positive_pain, percentiles)\n",
        "\n",
        "                for i in range(len(cuts) - 1):\n",
        "                    mask = (pain > cuts[i]) & (pain <= cuts[i + 1])\n",
        "                    indices = df.index[mask].tolist()\n",
        "                    if len(indices) >= min_size:\n",
        "                        groups.append({\n",
        "                            'id': f'preop_pain_level_{i}',\n",
        "                            'indices': indices,\n",
        "                            'type': 'preop_pain'\n",
        "                        })\n",
        "        else:\n",
        "            percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "            cuts = np.percentile(pain, percentiles)\n",
        "            bins = np.digitize(pain, cuts) - 1\n",
        "\n",
        "            groups = []\n",
        "            for i in range(n_groups):\n",
        "                indices = df.index[bins == i].tolist()\n",
        "                if len(indices) >= min_size:\n",
        "                    groups.append({\n",
        "                        'id': f'preop_pain_level_{i}',\n",
        "                        'indices': indices,\n",
        "                        'type': 'preop_pain'\n",
        "                    })\n",
        "\n",
        "        print(f\"Created {len(groups)} preoperative pain groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_asa_groups(self, df, min_size=6):\n",
        "        \"\"\"Create ASA groups\"\"\"\n",
        "        print(f\"Creating ASA physical status groups\")\n",
        "\n",
        "        if 'preop_asa' not in df.columns:\n",
        "            print(\"No ASA physical status data\")\n",
        "            return []\n",
        "\n",
        "        groups = []\n",
        "        for asa_status in df['preop_asa'].unique():\n",
        "            if pd.isna(asa_status):\n",
        "                continue\n",
        "\n",
        "            indices = df[df['preop_asa'] == asa_status].index.tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'asa_class_{asa_status}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'asa_status'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} ASA status groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_covariate_forest_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"Create covariate forest groups\"\"\"\n",
        "        print(f\"Creating covariate-based forest groups (target: {n_groups})\")\n",
        "\n",
        "        feature_cols = ['preop_age', 'preop_calcbmi', 'preop_gender', 'preop_asa',\n",
        "                       'preop_mallampati', 'preop_smoking', 'preop_pain', 'intraop_surgerysize']\n",
        "        available_features = [col for col in feature_cols if col in df.columns]\n",
        "\n",
        "        if not available_features:\n",
        "            print(\"No features available for covariate clustering\")\n",
        "            return []\n",
        "\n",
        "        X = df[available_features].copy()\n",
        "\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object':\n",
        "                le = LabelEncoder()\n",
        "                X[col] = X[col].fillna('missing')\n",
        "                X[col] = le.fit_transform(X[col])\n",
        "            else:\n",
        "                if X[col].isna().any():\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "        cluster_features = StandardScaler().fit_transform(X.values)\n",
        "        labels = KMeans(n_clusters=n_groups, random_state=self.random_seed).fit_predict(cluster_features)\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[labels == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'covariate_cluster_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'covariate_cluster'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} covariate-based groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def _ensure_balance_and_compute_cate(self, df, groups):\n",
        "        \"\"\"Ensure balance and compute CATE\"\"\"\n",
        "        balanced_groups = []\n",
        "\n",
        "        for group in groups:\n",
        "            group_df = df.loc[group['indices']]\n",
        "\n",
        "            treatment_rate = group_df['treatment'].mean()\n",
        "            n_treated = group_df['treatment'].sum()\n",
        "            n_control = len(group_df) - n_treated\n",
        "\n",
        "            if not (0.15 <= treatment_rate <= 0.85 and n_treated >= 3 and n_control >= 3):\n",
        "                continue\n",
        "\n",
        "            treated_outcomes = group_df[group_df['treatment'] == 1]['outcome']\n",
        "            control_outcomes = group_df[group_df['treatment'] == 0]['outcome']\n",
        "            # Reverse sign: negative CATE means treatment reduces pain (beneficial)\n",
        "            cate = -(treated_outcomes.mean() - control_outcomes.mean())\n",
        "\n",
        "            balanced_groups.append({\n",
        "                'id': group['id'],\n",
        "                'indices': group['indices'],\n",
        "                'size': len(group_df),\n",
        "                'treatment_rate': treatment_rate,\n",
        "                'n_treated': int(n_treated),\n",
        "                'n_control': int(n_control),\n",
        "                'cate': cate,\n",
        "                'type': group['type']\n",
        "            })\n",
        "\n",
        "        return balanced_groups\n",
        "\n",
        "    def normalize_cates(self, groups):\n",
        "        \"\"\"Normalize CATE values to [0,1]\"\"\"\n",
        "        cates = [g['cate'] for g in groups]\n",
        "        min_cate, max_cate = min(cates), max(cates)\n",
        "\n",
        "        if max_cate > min_cate:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = (group['cate'] - min_cate) / (max_cate - min_cate)\n",
        "        else:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = 0.5\n",
        "\n",
        "        print(f\"CATE normalization: [{min_cate:.3f}, {max_cate:.3f}] → [0, 1]\")\n",
        "        return groups\n",
        "\n",
        "    def simulate_sampling_trial(self, groups, sample_size, trial_seed):\n",
        "        \"\"\"Simulate sampling trial.\"\"\"\n",
        "        np.random.seed(self.random_seed + trial_seed)\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "\n",
        "        # Initialize tau estimates\n",
        "        tau_estimates = np.zeros(n_groups)\n",
        "        sample_counts = np.zeros(n_groups)\n",
        "\n",
        "        # Perform sampling: choose group uniformly, sample Bernoulli(tau(u))\n",
        "        for _ in range(sample_size):\n",
        "            group_idx = np.random.randint(n_groups)\n",
        "            sample = np.random.binomial(1, tau_true[group_idx])\n",
        "\n",
        "            sample_counts[group_idx] += 1\n",
        "            if sample_counts[group_idx] == 1:\n",
        "                tau_estimates[group_idx] = sample\n",
        "            else:\n",
        "                tau_estimates[group_idx] = ((sample_counts[group_idx] - 1) * tau_estimates[group_idx] + sample) / sample_counts[group_idx]\n",
        "\n",
        "        # Groups with no samples get estimate 0\n",
        "        tau_estimates[sample_counts == 0] = 0\n",
        "\n",
        "        return tau_estimates, sample_counts\n",
        "\n",
        "    def analyze_sample_size_performance(self, groups, sample_sizes, budget_percentages, n_trials=50):\n",
        "        \"\"\"Analyze performance vs sample size.\"\"\"\n",
        "        print(f\"Analyzing sample size performance with {len(groups)} groups\")\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "\n",
        "        # Calculate budgets\n",
        "        budgets = [max(1, int(p * n_groups)) for p in budget_percentages]\n",
        "        print(f\"Budget percentages {budget_percentages} → K values {budgets}\")\n",
        "\n",
        "        # Calculate optimal values\n",
        "        optimal_values = {}\n",
        "        for i, K in enumerate(budgets):\n",
        "            optimal_indices = np.argsort(tau_true)[-K:]\n",
        "            optimal_values[budget_percentages[i]] = np.sum(tau_true[optimal_indices])\n",
        "\n",
        "        # Run trials\n",
        "        results = {bp: {'sample_sizes': [], 'values': [], 'stds': []} for bp in budget_percentages}\n",
        "\n",
        "        for sample_size in sample_sizes:\n",
        "            print(f\"  Sample size {sample_size}...\")\n",
        "\n",
        "            budget_trial_values = {bp: [] for bp in budget_percentages}\n",
        "\n",
        "            for trial in range(n_trials):\n",
        "                tau_estimates, sample_counts = self.simulate_sampling_trial(groups, sample_size, trial)\n",
        "\n",
        "                for i, K in enumerate(budgets):\n",
        "                    bp = budget_percentages[i]\n",
        "\n",
        "                    # Select top K based on estimates\n",
        "                    selected_indices = np.argsort(tau_estimates)[-K:]\n",
        "\n",
        "                    # Compute realized value with true tau\n",
        "                    realized_value = np.sum(tau_true[selected_indices])\n",
        "                    budget_trial_values[bp].append(realized_value)\n",
        "\n",
        "            # Store results\n",
        "            for bp in budget_percentages:\n",
        "                results[bp]['sample_sizes'].append(sample_size)\n",
        "                results[bp]['values'].append(np.mean(budget_trial_values[bp]))\n",
        "                results[bp]['stds'].append(np.std(budget_trial_values[bp]))\n",
        "\n",
        "        return results, optimal_values\n",
        "\n",
        "    def plot_sample_size_analysis(self, results, optimal_values, method_name, budget_percentages, n_groups):\n",
        "        \"\"\"Create 6 plots (one per budget) for sample size analysis with theoretical bounds\"\"\"\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        # Calculate parameters for theoretical bounds\n",
        "        delta = 0.05\n",
        "\n",
        "        print(f\"\\nPlotting {method_name} (M={n_groups})\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for i, bp in enumerate([0.2, 0.3, 0.5, 0.7]):\n",
        "            ax = axes[i]\n",
        "\n",
        "            # Get data for this budget\n",
        "            sample_sizes = results[bp]['sample_sizes']\n",
        "            values = results[bp]['values']\n",
        "            stds = results[bp]['stds']\n",
        "            optimal_val = optimal_values[bp]\n",
        "\n",
        "            # Normalize all values by optimal value\n",
        "            values_norm = np.array(values) / optimal_val\n",
        "            stds_norm = np.array(stds) / optimal_val\n",
        "\n",
        "            # Plot empirical performance curve\n",
        "            ax.errorbar(sample_sizes, values_norm, yerr=stds_norm,\n",
        "                      marker='o', capsize=5, capthick=3, linewidth=6, markersize=8,\n",
        "                      label='Empirical data', color='blue', alpha=0.8)\n",
        "\n",
        "            # Plot optimal value (normalized to 1)\n",
        "            ax.axhline(y=1.0, color='black', linestyle=':', linewidth=2,\n",
        "                      label='Optimal (1.0)', alpha=0.8)\n",
        "\n",
        "            # Create smooth curves for plotting theoretical bounds\n",
        "            m_smooth = np.linspace(min(sample_sizes), max(sample_sizes), 200)\n",
        "\n",
        "            # Plot reference curves\n",
        "            ref_curve_05 = [theoretical_bound(m, 0.5, n_groups, delta, optimal_val) / optimal_val for m in m_smooth]\n",
        "            ref_curve_10 = [theoretical_bound(m, 1.0, n_groups, delta, optimal_val) / optimal_val for m in m_smooth]\n",
        "\n",
        "            ax.plot(m_smooth, ref_curve_05, 'red', linestyle=(0, (3, 2)), linewidth=6,\n",
        "                  label='FullCATE', alpha=0.8)\n",
        "            ax.plot(m_smooth, ref_curve_10, 'green', linestyle=(0, (3, 1, 1, 1)), linewidth=6,\n",
        "                  label='ALLOC', alpha=0.8)\n",
        "\n",
        "            # Set labels\n",
        "            ax.set_xlabel('Sample size', fontsize=23)\n",
        "            ax.set_ylabel('Normalized allocation value', fontsize=23)\n",
        "            ax.set_title(f'Budget = {bp*100:.0f}% (K={max(1, int(bp * n_groups))})', fontsize=24, fontweight='bold')\n",
        "\n",
        "            # Legend\n",
        "            ax.legend(fontsize=21, framealpha=0.9)\n",
        "            ax.grid(True, alpha=0.4, linewidth=1)\n",
        "\n",
        "            ax.tick_params(axis='both', which='major', labelsize=16, width=1.5, length=5)\n",
        "\n",
        "            # Set y-axis limits starting at 0.3\n",
        "            y_min = 0.2\n",
        "            y_max = 1.05  # Slightly above optimal\n",
        "            ax.set_ylim(y_min, y_max)\n",
        "\n",
        "            # Keep axes normal weight\n",
        "            for spine in ax.spines.values():\n",
        "                spine.set_linewidth(1.5)\n",
        "\n",
        "        plt.suptitle(f'{method_name} (M={n_groups})', fontsize=24, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        clean_name = method_name.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')\n",
        "        pdf_filename = f\"{clean_name}_M{n_groups}_sample_size_analysis.pdf\"\n",
        "        plt.savefig(pdf_filename, format='pdf', dpi=300, bbox_inches='tight')\n",
        "        print(f\"Saved plot as: {pdf_filename}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Plot complete for {method_name}\")\n",
        "\n",
        "\n",
        "def run_medical_sample_size_analysis(df_medical, sample_size_range=None, budget_percentages=None, n_trials=50,\n",
        "                                   outcome_col='postop4hour_throatpain', treatment_col='treat'):\n",
        "    \"\"\"Run sample size analysis on Medical dataset with theoretical bounds.\"\"\"\n",
        "\n",
        "    if sample_size_range is None:\n",
        "        sample_size_range = [100, 250, 500, 750, 1000, 1200, 1500, 2000, 5000, 10000, 20000]\n",
        "\n",
        "    if budget_percentages is None:\n",
        "        budget_percentages = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "    print(\"MEDICAL SAMPLE SIZE ANALYSIS - EMPIRICAL VS THEORETICAL BOUNDS\")\n",
        "    print(f\"Sample sizes: {sample_size_range}\")\n",
        "    print(f\"Budget percentages: {budget_percentages}\")\n",
        "    print(f\"Trials per sample size: {n_trials}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Define Medical grouping methods\n",
        "    methods = [\n",
        "        ('Demographics', lambda analyzer, df: analyzer.create_demographics_groups(df, min_size=6)),\n",
        "        ('Age Groups', lambda analyzer, df: analyzer.create_age_groups(df, n_groups=30, min_size=6)),\n",
        "        ('BMI Groups', lambda analyzer, df: analyzer.create_bmi_groups(df, n_groups=30, min_size=6)),\n",
        "        ('Preoperative Pain', lambda analyzer, df: analyzer.create_preop_pain_groups(df, n_groups=30, min_size=6)),\n",
        "        ('ASA Physical Status', lambda analyzer, df: analyzer.create_asa_groups(df, min_size=6)),\n",
        "        ('Covariate Forest', lambda analyzer, df: analyzer.create_covariate_forest_groups(df, n_groups=30, min_size=6))\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    for method_name, method_func in methods:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ANALYZING MEDICAL METHOD: {method_name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        try:\n",
        "            analyzer = MedicalSampleSizeAnalyzer()\n",
        "            df_processed = analyzer.process_medical_data(df_medical, outcome_col=outcome_col, treatment_col=treatment_col)\n",
        "\n",
        "            groups = method_func(analyzer, df_processed)\n",
        "\n",
        "            if len(groups) < 10:\n",
        "                print(f\"Too few groups ({len(groups)}) for {method_name} - skipping\")\n",
        "                continue\n",
        "\n",
        "            groups = analyzer.normalize_cates(groups)\n",
        "\n",
        "            # Run sample size analysis\n",
        "            results, optimal_values = analyzer.analyze_sample_size_performance(\n",
        "                groups, sample_size_range, budget_percentages, n_trials\n",
        "            )\n",
        "\n",
        "            all_results[method_name] = {\n",
        "                'results': results,\n",
        "                'optimal_values': optimal_values,\n",
        "                'n_groups': len(groups)\n",
        "            }\n",
        "\n",
        "            # Create plots with theoretical bounds\n",
        "            print(f\"Creating plots for {method_name}...\")\n",
        "            analyzer.plot_sample_size_analysis(\n",
        "                results, optimal_values, method_name, budget_percentages, len(groups)\n",
        "            )\n",
        "\n",
        "            # Print summary\n",
        "            print(f\"\\nSummary for {method_name}:\")\n",
        "            print(f\"Number of groups: {len(groups)}\")\n",
        "            print(\"Optimal values by budget:\")\n",
        "            for bp in budget_percentages:\n",
        "                print(f\"  {bp*100:.0f}%: {optimal_values[bp]:.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {method_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load medical dataset\n",
        "    df_medical = pd.read_stata('Licorice Gargle.dta')\n",
        "\n",
        "    # Run sample size analysis with theoretical bounds\n",
        "    sample_sizes = [100, 250, 500, 750, 1000, 1200, 1500, 2000, 5000, 10000, 20000]\n",
        "    budget_percentages = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "    results = run_medical_sample_size_analysis(\n",
        "        df_medical,\n",
        "        sample_size_range=sample_sizes,\n",
        "        budget_percentages=budget_percentages,\n",
        "        n_trials=50\n",
        "    )"
      ]
    }
  ]
}