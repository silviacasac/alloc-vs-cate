{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JrNwpkBPeo8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "STAR dataset, no fixed epsilon method\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def theoretical_bound(m, beta, N, delta, OPT):\n",
        "    \"\"\"\n",
        "    Compute theoretical performance bound for allocation algorithms.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    m : int\n",
        "        Sample size\n",
        "    beta : float\n",
        "        Bound parameter (0.5 for FullCATE, 1.0 for ALLOC)\n",
        "    N : int\n",
        "        Number of groups\n",
        "    delta : float\n",
        "        Confidence parameter\n",
        "    OPT : float\n",
        "        Optimal allocation value\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float\n",
        "        Theoretical lower bound on performance\n",
        "    \"\"\"\n",
        "    term = N * np.log(2 * N / delta) / m\n",
        "    if term >= 1:\n",
        "        return 0\n",
        "    return (1 - term**beta) * OPT\n",
        "\n",
        "\n",
        "class SampleSizeAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyzer for sample size requirements in CATE allocation problems.\n",
        "\n",
        "    This class implements methods to evaluate how allocation performance\n",
        "    varies with sample size across different group construction methods.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, random_seed=42):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        random_seed : int, default=42\n",
        "            Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.random_seed = random_seed\n",
        "        np.random.seed(random_seed)\n",
        "        print(f\"Sample Size Analyzer initialized with seed {random_seed}\")\n",
        "\n",
        "    def process_star_data(self, df, outcome_col=None):\n",
        "        \"\"\"\n",
        "        Process STAR dataset for analysis.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            Raw STAR dataset\n",
        "        outcome_col : str, optional\n",
        "            Outcome column name (if different from default)\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        pandas.DataFrame\n",
        "            Processed dataset ready for analysis\n",
        "        \"\"\"\n",
        "        print(f\"Processing STAR data with {len(df)} observations\")\n",
        "\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Validate required columns\n",
        "        required_cols = ['gkschid', 'gkclasstype']\n",
        "        missing = [col for col in required_cols if col not in df_processed.columns]\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "        # Filter class types\n",
        "        print(f\"Original class type distribution: {df_processed['gkclasstype'].value_counts().to_dict()}\")\n",
        "        df_processed = df_processed[df_processed['gkclasstype'] != 'REGULAR + AIDE CLASS']\n",
        "        print(f\"After excluding aide classes: {len(df_processed)} observations\")\n",
        "\n",
        "        # Create treatment variable\n",
        "        treatment_map = {'SMALL CLASS': 1, 'REGULAR CLASS': 0}\n",
        "        df_processed['treatment'] = df_processed['gkclasstype'].map(treatment_map)\n",
        "\n",
        "        # Create composite outcome\n",
        "        test_components = ['gktreadss', 'gktmathss', 'gktlistss', 'gkwordskillss']\n",
        "        available_components = [col for col in test_components if col in df_processed.columns]\n",
        "\n",
        "        if not available_components:\n",
        "            raise ValueError(\"No test score components found\")\n",
        "\n",
        "        # Remove observations with missing test scores\n",
        "        initial_size = len(df_processed)\n",
        "        df_processed = df_processed.dropna(subset=available_components)\n",
        "        print(f\"Dropped {initial_size - len(df_processed)} rows due to missing test scores\")\n",
        "\n",
        "        df_processed['total_score'] = df_processed[available_components].sum(axis=1)\n",
        "        df_processed['outcome'] = df_processed['total_score']\n",
        "\n",
        "        # Final data cleaning\n",
        "        initial_size = len(df_processed)\n",
        "        df_processed = df_processed.dropna(subset=['treatment', 'gkschid'])\n",
        "        final_size = len(df_processed)\n",
        "\n",
        "        if initial_size != final_size:\n",
        "            print(f\"Dropped {initial_size - final_size} rows due to missing treatment/school data\")\n",
        "\n",
        "        print(f\"Final dataset: {final_size} students\")\n",
        "        print(f\"Treatment distribution: {df_processed['treatment'].value_counts().to_dict()}\")\n",
        "\n",
        "        return df_processed\n",
        "\n",
        "    def create_school_groups(self, df, min_size=6):\n",
        "        \"\"\"\n",
        "        Create groups based on school identifiers.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            Processed dataset\n",
        "        min_size : int, default=6\n",
        "            Minimum group size threshold\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        list\n",
        "            List of group dictionaries with balanced treatment assignment\n",
        "        \"\"\"\n",
        "        print(f\"Creating school-based groups (min_size={min_size})\")\n",
        "\n",
        "        groups = []\n",
        "        for school_id in df['gkschid'].unique():\n",
        "            indices = df[df['gkschid'] == school_id].index.tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'school_{school_id}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'school'\n",
        "                })\n",
        "\n",
        "        print(f\"Raw groups created: {len(groups)}\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        print(f\"Balanced groups after filtering: {len(balanced_groups)}\")\n",
        "\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_causal_forest_groups(self, df, n_groups=30, min_size=6):\n",
        "        \"\"\"\n",
        "        Create groups using RF-based treatment effect prediction.\n",
        "\n",
        "        Uses separate Random Forest models for treated and control outcomes,\n",
        "        then clusters based on predicted treatment effects and covariates.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            Processed dataset\n",
        "        n_groups : int, default=30\n",
        "            Target number of groups\n",
        "        min_size : int, default=6\n",
        "            Minimum group size threshold\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        list\n",
        "            List of group dictionaries\n",
        "        \"\"\"\n",
        "        print(f\"Creating RF-based groups (target: {n_groups})\")\n",
        "\n",
        "        # Select features (exclude treatment, outcome, and test scores)\n",
        "        feature_cols = [col for col in df.columns\n",
        "                       if col not in ['treatment', 'outcome', 'total_score']\n",
        "                       and not col.startswith('gkt')]\n",
        "\n",
        "        X = df[feature_cols].copy()\n",
        "\n",
        "        # Data preprocessing\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
        "                X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "            elif X[col].dtype == 'bool':\n",
        "                X[col] = X[col].astype(int)\n",
        "\n",
        "        # Handle missing values\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 0)\n",
        "\n",
        "        # Train separate outcome models\n",
        "        treated_mask = df['treatment'] == 1\n",
        "        control_mask = df['treatment'] == 0\n",
        "\n",
        "        if treated_mask.sum() == 0 or control_mask.sum() == 0:\n",
        "            print(\"Insufficient treated or control observations\")\n",
        "            return []\n",
        "\n",
        "        rf_treated = RandomForestRegressor(n_estimators=100, random_state=self.random_seed)\n",
        "        rf_control = RandomForestRegressor(n_estimators=100, random_state=self.random_seed)\n",
        "\n",
        "        rf_treated.fit(X[treated_mask], df.loc[treated_mask, 'outcome'])\n",
        "        rf_control.fit(X[control_mask], df.loc[control_mask, 'outcome'])\n",
        "\n",
        "        # Predict treatment effects and cluster\n",
        "        pred_cate = rf_treated.predict(X) - rf_control.predict(X)\n",
        "        cluster_features = np.column_stack([X.values, pred_cate.reshape(-1, 1)])\n",
        "        cluster_features = StandardScaler().fit_transform(cluster_features)\n",
        "\n",
        "        labels = KMeans(n_clusters=n_groups, random_state=self.random_seed).fit_predict(cluster_features)\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[labels == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'rf_cluster_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'rf_cluster'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} RF-based groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_propensity_groups(self, df, n_groups=50, min_size=6):\n",
        "        \"\"\"\n",
        "        Create groups based on propensity score stratification.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            Processed dataset\n",
        "        n_groups : int, default=50\n",
        "            Target number of strata\n",
        "        min_size : int, default=6\n",
        "            Minimum group size threshold\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        list\n",
        "            List of group dictionaries\n",
        "        \"\"\"\n",
        "        print(f\"Creating propensity score groups (target: {n_groups})\")\n",
        "\n",
        "        feature_cols = [col for col in df.columns\n",
        "                       if col not in ['treatment', 'outcome', 'total_score']]\n",
        "\n",
        "        X = df[feature_cols].copy()\n",
        "\n",
        "        # Data preprocessing\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
        "                X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n",
        "            elif X[col].dtype == 'bool':\n",
        "                X[col] = X[col].astype(int)\n",
        "\n",
        "        # Handle missing values\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                if X[col].dtype in ['int64', 'float64']:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 0)\n",
        "\n",
        "        # Estimate propensity scores using cross-validation\n",
        "        prop_scores = cross_val_predict(\n",
        "            LogisticRegression(random_state=self.random_seed),\n",
        "            X, df['treatment'], method='predict_proba', cv=5\n",
        "        )[:, 1]\n",
        "\n",
        "        # Create quantile-based strata\n",
        "        quantiles = np.linspace(0, 1, n_groups + 1)\n",
        "        bins = np.digitize(prop_scores, np.quantile(prop_scores, quantiles)) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'propensity_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'propensity'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} propensity score groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_performance_groups(self, df, n_groups=50, min_size=6):\n",
        "        \"\"\"\n",
        "        Create groups based on baseline academic performance percentiles.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            Processed dataset\n",
        "        n_groups : int, default=50\n",
        "            Target number of groups\n",
        "        min_size : int, default=6\n",
        "            Minimum group size threshold\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        list\n",
        "            List of group dictionaries\n",
        "        \"\"\"\n",
        "        print(f\"Creating performance groups (target: {n_groups})\")\n",
        "\n",
        "        # Identify baseline score columns\n",
        "        score_cols = [col for col in df.columns if col.startswith('gkt') and 'ss' in col]\n",
        "        if not score_cols:\n",
        "            print(\"No baseline scores found\")\n",
        "            return []\n",
        "\n",
        "        baseline_score = df[score_cols].fillna(df[score_cols].mean()).mean(axis=1)\n",
        "\n",
        "        # Create percentile-based groups\n",
        "        percentiles = np.linspace(0, 100, n_groups + 1)\n",
        "        cuts = np.percentile(baseline_score, percentiles)\n",
        "        bins = np.digitize(baseline_score, cuts) - 1\n",
        "\n",
        "        groups = []\n",
        "        for i in range(n_groups):\n",
        "            indices = df.index[bins == i].tolist()\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': f'performance_{i}',\n",
        "                    'indices': indices,\n",
        "                    'type': 'performance'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} performance groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def create_demographics_groups(self, df, min_size=6, feature_cols=None):\n",
        "        \"\"\"\n",
        "        Create groups based on demographic characteristic combinations.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            Processed dataset\n",
        "        min_size : int, default=6\n",
        "            Minimum group size threshold\n",
        "        feature_cols : list, optional\n",
        "            List of demographic features to use\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        list\n",
        "            List of group dictionaries\n",
        "        \"\"\"\n",
        "        print(f\"Creating demographics groups\")\n",
        "\n",
        "        if feature_cols is None:\n",
        "            potential_features = ['gkfreelunch', 'race', 'gender', 'birthyear']\n",
        "        else:\n",
        "            potential_features = feature_cols\n",
        "\n",
        "        # Identify available features\n",
        "        available_features = []\n",
        "        for col in potential_features:\n",
        "            if col in df.columns and df[col].notna().sum() > 0:\n",
        "                available_features.append(col)\n",
        "\n",
        "        if len(available_features) == 0:\n",
        "            print(\"No demographic variables found, using school grouping\")\n",
        "            return self.create_school_groups(df, min_size)\n",
        "\n",
        "        print(f\"Using features: {available_features}\")\n",
        "\n",
        "        # Remove observations with missing demographic data\n",
        "        df_clean = df[available_features].dropna()\n",
        "        print(f\"After removing missing values: {len(df_clean)}/{len(df)} students\")\n",
        "\n",
        "        if len(df_clean) == 0:\n",
        "            return self.create_school_groups(df, min_size)\n",
        "\n",
        "        # Create groups based on unique combinations\n",
        "        unique_combinations = df_clean.drop_duplicates()\n",
        "        print(f\"Found {len(unique_combinations)} unique combinations\")\n",
        "\n",
        "        groups = []\n",
        "        for combo_idx, (idx, combo) in enumerate(unique_combinations.iterrows()):\n",
        "            mask = pd.Series(True, index=df.index)\n",
        "            combo_description = []\n",
        "\n",
        "            for feature in available_features:\n",
        "                mask = mask & (df[feature] == combo[feature])\n",
        "                combo_description.append(f\"{feature}={combo[feature]}\")\n",
        "\n",
        "            indices = df[mask].index.tolist()\n",
        "            combo_id = \"_\".join(combo_description)\n",
        "\n",
        "            if len(indices) >= min_size:\n",
        "                groups.append({\n",
        "                    'id': combo_id,\n",
        "                    'indices': indices,\n",
        "                    'type': 'demographics'\n",
        "                })\n",
        "\n",
        "        print(f\"Created {len(groups)} demographic groups\")\n",
        "        balanced_groups = self._ensure_balance_and_compute_cate(df, groups)\n",
        "        return balanced_groups\n",
        "\n",
        "    def _ensure_balance_and_compute_cate(self, df, groups):\n",
        "        \"\"\"\n",
        "        Filter groups for treatment balance and compute conditional average treatment effects.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            Dataset\n",
        "        groups : list\n",
        "            List of candidate groups\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        list\n",
        "            List of balanced groups with CATE estimates\n",
        "        \"\"\"\n",
        "        balanced_groups = []\n",
        "\n",
        "        for group in groups:\n",
        "            group_df = df.loc[group['indices']]\n",
        "\n",
        "            treatment_rate = group_df['treatment'].mean()\n",
        "            n_treated = group_df['treatment'].sum()\n",
        "            n_control = len(group_df) - n_treated\n",
        "\n",
        "            # Apply balance and minimum size requirements\n",
        "            if not (0.15 <= treatment_rate <= 0.85 and n_treated >= 3 and n_control >= 3):\n",
        "                continue\n",
        "\n",
        "            # Compute CATE as difference in means\n",
        "            treated_outcomes = group_df[group_df['treatment'] == 1]['outcome']\n",
        "            control_outcomes = group_df[group_df['treatment'] == 0]['outcome']\n",
        "            cate = treated_outcomes.mean() - control_outcomes.mean()\n",
        "\n",
        "            balanced_groups.append({\n",
        "                'id': group['id'],\n",
        "                'indices': group['indices'],\n",
        "                'size': len(group_df),\n",
        "                'treatment_rate': treatment_rate,\n",
        "                'n_treated': int(n_treated),\n",
        "                'n_control': int(n_control),\n",
        "                'cate': cate,\n",
        "                'type': group['type']\n",
        "            })\n",
        "\n",
        "        return balanced_groups\n",
        "\n",
        "    def normalize_cates(self, groups):\n",
        "        \"\"\"\n",
        "        Normalize CATE values to [0,1] interval.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        groups : list\n",
        "            List of groups with CATE estimates\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        list\n",
        "            Groups with normalized CATE values\n",
        "        \"\"\"\n",
        "        cates = [g['cate'] for g in groups]\n",
        "        min_cate, max_cate = min(cates), max(cates)\n",
        "\n",
        "        if max_cate > min_cate:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = (group['cate'] - min_cate) / (max_cate - min_cate)\n",
        "        else:\n",
        "            for group in groups:\n",
        "                group['normalized_cate'] = 0.5\n",
        "\n",
        "        print(f\"CATE normalization: [{min_cate:.3f}, {max_cate:.3f}] → [0, 1]\")\n",
        "        return groups\n",
        "\n",
        "    def simulate_sampling_trial(self, groups, sample_size, trial_seed):\n",
        "        \"\"\"\n",
        "        Simulate one trial of the bandit sampling process.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        groups : list\n",
        "            List of groups with normalized CATEs\n",
        "        sample_size : int\n",
        "            Total sample size for the trial\n",
        "        trial_seed : int\n",
        "            Trial-specific random seed\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            (tau_estimates, sample_counts) arrays\n",
        "        \"\"\"\n",
        "        np.random.seed(self.random_seed + trial_seed)\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "\n",
        "        # Initialize estimates\n",
        "        tau_estimates = np.zeros(n_groups)\n",
        "        sample_counts = np.zeros(n_groups)\n",
        "\n",
        "        # Perform sampling\n",
        "        for _ in range(sample_size):\n",
        "            # Choose group uniformly at random\n",
        "            group_idx = np.random.randint(n_groups)\n",
        "\n",
        "            # Sample from Bernoulli distribution with success probability tau_true\n",
        "            sample = np.random.binomial(1, tau_true[group_idx])\n",
        "\n",
        "            # Update running average\n",
        "            sample_counts[group_idx] += 1\n",
        "            if sample_counts[group_idx] == 1:\n",
        "                tau_estimates[group_idx] = sample\n",
        "            else:\n",
        "                tau_estimates[group_idx] = ((sample_counts[group_idx] - 1) * tau_estimates[group_idx] + sample) / sample_counts[group_idx]\n",
        "\n",
        "        # Set unsampled groups to zero estimate\n",
        "        tau_estimates[sample_counts == 0] = 0\n",
        "\n",
        "        return tau_estimates, sample_counts\n",
        "\n",
        "    def analyze_sample_size_performance(self, groups, sample_sizes, budget_percentages, n_trials=50):\n",
        "        \"\"\"\n",
        "        Analyze allocation performance as a function of sample size.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        groups : list\n",
        "            List of groups with normalized CATEs\n",
        "        sample_sizes : list\n",
        "            Sample sizes to evaluate\n",
        "        budget_percentages : list\n",
        "            Budget constraints as fractions of total groups\n",
        "        n_trials : int, default=50\n",
        "            Number of trials per sample size\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            (results, optimal_values) dictionaries\n",
        "        \"\"\"\n",
        "        print(f\"Analyzing sample size performance with {len(groups)} groups\")\n",
        "\n",
        "        n_groups = len(groups)\n",
        "        tau_true = np.array([g['normalized_cate'] for g in groups])\n",
        "\n",
        "        # Convert budget percentages to group counts\n",
        "        budgets = [max(1, int(p * n_groups)) for p in budget_percentages]\n",
        "        print(f\"Budget percentages {budget_percentages} → K values {budgets}\")\n",
        "\n",
        "        # Calculate optimal values for each budget\n",
        "        optimal_values = {}\n",
        "        for i, K in enumerate(budgets):\n",
        "            optimal_indices = np.argsort(tau_true)[-K:]\n",
        "            optimal_values[budget_percentages[i]] = np.sum(tau_true[optimal_indices])\n",
        "\n",
        "        # Initialize results storage\n",
        "        results = {bp: {'sample_sizes': [], 'values': [], 'stds': []} for bp in budget_percentages}\n",
        "\n",
        "        for sample_size in sample_sizes:\n",
        "            print(f\"  Sample size {sample_size}...\")\n",
        "\n",
        "            # Store trial results for each budget\n",
        "            budget_trial_values = {bp: [] for bp in budget_percentages}\n",
        "\n",
        "            for trial in range(n_trials):\n",
        "                tau_estimates, sample_counts = self.simulate_sampling_trial(groups, sample_size, trial)\n",
        "\n",
        "                # Evaluate each budget level\n",
        "                for i, K in enumerate(budgets):\n",
        "                    bp = budget_percentages[i]\n",
        "\n",
        "                    # Select top K groups based on estimates\n",
        "                    selected_indices = np.argsort(tau_estimates)[-K:]\n",
        "\n",
        "                    # Compute realized value using true CATE values\n",
        "                    realized_value = np.sum(tau_true[selected_indices])\n",
        "                    budget_trial_values[bp].append(realized_value)\n",
        "\n",
        "            # Store summary statistics\n",
        "            for bp in budget_percentages:\n",
        "                results[bp]['sample_sizes'].append(sample_size)\n",
        "                results[bp]['values'].append(np.mean(budget_trial_values[bp]))\n",
        "                results[bp]['stds'].append(np.std(budget_trial_values[bp]))\n",
        "\n",
        "        return results, optimal_values\n",
        "\n",
        "    def plot_sample_size_analysis(self, results, optimal_values, method_name, budget_percentages, n_groups):\n",
        "        \"\"\"\n",
        "        Create visualization of sample size analysis with theoretical bounds.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        results : dict\n",
        "            Results from sample size analysis\n",
        "        optimal_values : dict\n",
        "            Optimal values for each budget\n",
        "        method_name : str\n",
        "            Name of the grouping method\n",
        "        budget_percentages : list\n",
        "            Budget levels analyzed\n",
        "        n_groups : int\n",
        "            Total number of groups\n",
        "        \"\"\"\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        delta = 0.05  # Confidence parameter for theoretical bounds\n",
        "\n",
        "        print(f\"\\nPlotting {method_name} (={n_groups})\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for i, bp in enumerate([0.2, 0.3, 0.5, 0.7]):\n",
        "            ax = axes[i]\n",
        "\n",
        "            # Extract data for this budget level\n",
        "            sample_sizes = results[bp]['sample_sizes']\n",
        "            values = results[bp]['values']\n",
        "            stds = results[bp]['stds']\n",
        "            optimal_val = optimal_values[bp]\n",
        "\n",
        "            # Normalize by optimal value\n",
        "            values_norm = np.array(values) / optimal_val\n",
        "            stds_norm = np.array(stds) / optimal_val\n",
        "\n",
        "            # Plot empirical performance\n",
        "            ax.errorbar(sample_sizes, values_norm, yerr=stds_norm,\n",
        "                      marker='o', capsize=5, capthick=3, linewidth=6, markersize=8,\n",
        "                      label='Empirical data', color='blue', alpha=0.8)\n",
        "\n",
        "            # Plot optimal performance line\n",
        "            ax.axhline(y=1.0, color='black', linestyle=':', linewidth=2,\n",
        "                      label='Optimal (1.0)', alpha=0.8)\n",
        "\n",
        "            # Create theoretical bound curves\n",
        "            m_smooth = np.linspace(min(sample_sizes), max(sample_sizes), 200)\n",
        "\n",
        "            # Compute and plot theoretical bounds\n",
        "            ref_curve_05 = [theoretical_bound(m, 0.5, n_groups, delta, optimal_val) / optimal_val for m in m_smooth]\n",
        "            ref_curve_10 = [theoretical_bound(m, 1.0, n_groups, delta, optimal_val) / optimal_val for m in m_smooth]\n",
        "\n",
        "            ax.plot(m_smooth, ref_curve_05, 'red', linestyle=(0, (3, 2)), linewidth=6,\n",
        "                  label='FullCATE (β=0.5)', alpha=0.8)\n",
        "            ax.plot(m_smooth, ref_curve_10, 'green', linestyle=(0, (3, 1, 1, 1)), linewidth=6,\n",
        "                  label='ALLOC (β=1.0)', alpha=0.8)\n",
        "\n",
        "            # Formatting\n",
        "            ax.set_xlabel('Sample size', fontsize=23)\n",
        "            ax.set_ylabel('Normalized allocation value', fontsize=23)\n",
        "            ax.set_title(f'Budget = {bp*100:.0f}% (K={max(1, int(bp * n_groups))})',\n",
        "                        fontsize=24, fontweight='bold')\n",
        "\n",
        "            ax.legend(fontsize=21, framealpha=0.9)\n",
        "            ax.grid(True, alpha=0.4, linewidth=1)\n",
        "            ax.tick_params(axis='both', which='major', labelsize=16, width=1.5, length=5)\n",
        "\n",
        "            # Set axis limits\n",
        "            y_min = 0.2\n",
        "            y_max = 1.05\n",
        "            ax.set_ylim(y_min, y_max)\n",
        "\n",
        "            for spine in ax.spines.values():\n",
        "                spine.set_linewidth(1.5)\n",
        "\n",
        "        plt.suptitle(f'{method_name} (M={n_groups})', fontsize=24, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        clean_name = method_name.replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')\n",
        "        pdf_filename = f\"{clean_name}_M{n_groups}_sample_size_analysis.pdf\"\n",
        "        plt.savefig(pdf_filename, format='pdf', dpi=300, bbox_inches='tight')\n",
        "        print(f\"Saved plot as: {pdf_filename}\")\n",
        "\n",
        "        plt.show()\n",
        "        print(f\"Plot complete for {method_name}\")\n",
        "\n",
        "\n",
        "def run_sample_size_analysis(df_star, sample_size_range=None, budget_percentages=None, n_trials=50):\n",
        "    \"\"\"\n",
        "    Execute comprehensive sample size analysis across all grouping methods.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df_star : pandas.DataFrame\n",
        "        STAR dataset\n",
        "    sample_size_range : list, optional\n",
        "        Sample sizes to evaluate\n",
        "    budget_percentages : list, optional\n",
        "        Budget constraints to analyze\n",
        "    n_trials : int, default=50\n",
        "        Number of trials per configuration\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Comprehensive results across all methods\n",
        "    \"\"\"\n",
        "\n",
        "    if sample_size_range is None:\n",
        "        sample_size_range = [100, 250, 500, 750, 1000, 1200, 1500, 2000, 5000, 10000, 20000]\n",
        "\n",
        "    if budget_percentages is None:\n",
        "        budget_percentages = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "    print(\"SAMPLE SIZE ANALYSIS - EMPIRICAL VS THEORETICAL BOUNDS\")\n",
        "    print(f\"Sample sizes: {sample_size_range}\")\n",
        "    print(f\"Budget percentages: {budget_percentages}\")\n",
        "    print(f\"Trials per sample size: {n_trials}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Define grouping methods for analysis\n",
        "    methods = [\n",
        "        ('School Groups', lambda analyzer, df: analyzer.create_school_groups(df, min_size=6)),\n",
        "        ('Demographics', lambda analyzer, df: analyzer.create_demographics_groups(df,\n",
        "                                               feature_cols=['gkfreelunch', 'race', 'gender'], min_size=6)),\n",
        "        ('Causal Forest (30)', lambda analyzer, df: analyzer.create_causal_forest_groups(df, n_groups=30, min_size=6)),\n",
        "        ('Causal Forest (50)', lambda analyzer, df: analyzer.create_causal_forest_groups(df, n_groups=50, min_size=6)),\n",
        "        ('Propensity Score', lambda analyzer, df: analyzer.create_propensity_groups(df, n_groups=50, min_size=6)),\n",
        "        ('Performance Groups', lambda analyzer, df: analyzer.create_performance_groups(df, n_groups=50, min_size=6))\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    for method_name, method_func in methods:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ANALYZING METHOD: {method_name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        try:\n",
        "            # Initialize analyzer and process data\n",
        "            analyzer = SampleSizeAnalyzer()\n",
        "            df_processed = analyzer.process_star_data(df_star)\n",
        "\n",
        "            # Create groups using current method\n",
        "            groups = method_func(analyzer, df_processed)\n",
        "\n",
        "            if len(groups) < 10:\n",
        "                print(f\"Insufficient groups ({len(groups)}) for {method_name} - skipping\")\n",
        "                continue\n",
        "\n",
        "            groups = analyzer.normalize_cates(groups)\n",
        "\n",
        "            # Perform sample size analysis\n",
        "            results, optimal_values = analyzer.analyze_sample_size_performance(\n",
        "                groups, sample_size_range, budget_percentages, n_trials\n",
        "            )\n",
        "\n",
        "            # Store results\n",
        "            all_results[method_name] = {\n",
        "                'results': results,\n",
        "                'optimal_values': optimal_values,\n",
        "                'n_groups': len(groups)\n",
        "            }\n",
        "\n",
        "            # Generate visualization\n",
        "            print(f\"Creating plots for {method_name}...\")\n",
        "            analyzer.plot_sample_size_analysis(\n",
        "                results, optimal_values, method_name, budget_percentages, len(groups)\n",
        "            )\n",
        "\n",
        "            # Print summary statistics\n",
        "            print(f\"\\nSummary for {method_name}:\")\n",
        "            print(f\"Number of groups: {len(groups)}\")\n",
        "            print(\"Optimal values by budget:\")\n",
        "            for bp in budget_percentages:\n",
        "                print(f\"  {bp*100:.0f}%: {optimal_values[bp]:.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {method_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load STAR dataset\n",
        "    df_star = pd.read_spss('STAR_Students.sav')\n",
        "\n",
        "    # Configure analysis parameters\n",
        "    sample_sizes = [100, 250, 500, 750, 1000, 1200, 1500, 2000, 5000, 10000, 20000]\n",
        "    budget_percentages = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "    # Execute comprehensive sample size analysis\n",
        "    results = run_sample_size_analysis(\n",
        "        df_star,\n",
        "        sample_size_range=sample_sizes,\n",
        "        budget_percentages=budget_percentages,\n",
        "        n_trials=50\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SAMPLE SIZE ANALYSIS COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Print overall summary\n",
        "    print(f\"Analysis completed for {len(results)} grouping methods:\")\n",
        "    for method_name, method_data in results.items():\n",
        "        n_groups = method_data['n_groups']\n",
        "        print(f\"  {method_name}: {n_groups} groups\")\n",
        "\n",
        "    print(f\"\\nAnalysis parameters:\")\n",
        "    print(f\"  Sample sizes: {len(sample_sizes)} levels\")\n",
        "    print(f\"  Budget constraints: {len(budget_percentages)} levels\")\n",
        "    print(f\"  Trials: 50 per configuration\")\n",
        "    print(f\"  Total configurations analyzed: {len(sample_sizes) * len(budget_percentages) * len(results)}\")"
      ]
    }
  ]
}